{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATH7016 Deep Learning\n",
    "### Coursework 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.layers import Activation\n",
    "from keras.utils import get_custom_objects\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ask_price1</th>\n",
       "      <th>ask_size1</th>\n",
       "      <th>bid_price1</th>\n",
       "      <th>bid_size1</th>\n",
       "      <th>ask_price2</th>\n",
       "      <th>ask_size2</th>\n",
       "      <th>bid_price2</th>\n",
       "      <th>bid_size2</th>\n",
       "      <th>ask_price3</th>\n",
       "      <th>ask_size3</th>\n",
       "      <th>...</th>\n",
       "      <th>bid_size3</th>\n",
       "      <th>ask_price4</th>\n",
       "      <th>ask_size4</th>\n",
       "      <th>bid_price4</th>\n",
       "      <th>bid_size4</th>\n",
       "      <th>midprice_change1</th>\n",
       "      <th>midprice_change2</th>\n",
       "      <th>midprice_change3</th>\n",
       "      <th>midprice_change4</th>\n",
       "      <th>midprice_change5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>696400.0</td>\n",
       "      <td>16</td>\n",
       "      <td>696000.0</td>\n",
       "      <td>12</td>\n",
       "      <td>696500.0</td>\n",
       "      <td>57</td>\n",
       "      <td>695900.0</td>\n",
       "      <td>118</td>\n",
       "      <td>696600.0</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>262</td>\n",
       "      <td>696700.0</td>\n",
       "      <td>150</td>\n",
       "      <td>695700.0</td>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>740800.0</td>\n",
       "      <td>2</td>\n",
       "      <td>740400.0</td>\n",
       "      <td>20</td>\n",
       "      <td>741000.0</td>\n",
       "      <td>60</td>\n",
       "      <td>740200.0</td>\n",
       "      <td>27</td>\n",
       "      <td>741200.0</td>\n",
       "      <td>156</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>741300.0</td>\n",
       "      <td>200</td>\n",
       "      <td>740000.0</td>\n",
       "      <td>170</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>730900.0</td>\n",
       "      <td>1</td>\n",
       "      <td>730200.0</td>\n",
       "      <td>230</td>\n",
       "      <td>731000.0</td>\n",
       "      <td>111</td>\n",
       "      <td>730100.0</td>\n",
       "      <td>86</td>\n",
       "      <td>731100.0</td>\n",
       "      <td>42</td>\n",
       "      <td>...</td>\n",
       "      <td>136</td>\n",
       "      <td>731200.0</td>\n",
       "      <td>100</td>\n",
       "      <td>729900.0</td>\n",
       "      <td>132</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>630600.0</td>\n",
       "      <td>100</td>\n",
       "      <td>630300.0</td>\n",
       "      <td>69</td>\n",
       "      <td>630700.0</td>\n",
       "      <td>110</td>\n",
       "      <td>630200.0</td>\n",
       "      <td>2</td>\n",
       "      <td>630800.0</td>\n",
       "      <td>219</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>630900.0</td>\n",
       "      <td>101</td>\n",
       "      <td>630000.0</td>\n",
       "      <td>104</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>851100.0</td>\n",
       "      <td>579</td>\n",
       "      <td>850300.0</td>\n",
       "      <td>25</td>\n",
       "      <td>851200.0</td>\n",
       "      <td>17</td>\n",
       "      <td>850100.0</td>\n",
       "      <td>287</td>\n",
       "      <td>851400.0</td>\n",
       "      <td>307</td>\n",
       "      <td>...</td>\n",
       "      <td>270</td>\n",
       "      <td>851500.0</td>\n",
       "      <td>223</td>\n",
       "      <td>849900.0</td>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ask_price1  ask_size1  bid_price1  bid_size1  ask_price2  ask_size2  \\\n",
       "0    696400.0         16    696000.0         12    696500.0         57   \n",
       "1    740800.0          2    740400.0         20    741000.0         60   \n",
       "2    730900.0          1    730200.0        230    731000.0        111   \n",
       "3    630600.0        100    630300.0         69    630700.0        110   \n",
       "4    851100.0        579    850300.0         25    851200.0         17   \n",
       "\n",
       "   bid_price2  bid_size2  ask_price3  ask_size3  ...  bid_size3  ask_price4  \\\n",
       "0    695900.0        118    696600.0        100  ...        262    696700.0   \n",
       "1    740200.0         27    741200.0        156  ...         31    741300.0   \n",
       "2    730100.0         86    731100.0         42  ...        136    731200.0   \n",
       "3    630200.0          2    630800.0        219  ...          1    630900.0   \n",
       "4    850100.0        287    851400.0        307  ...        270    851500.0   \n",
       "\n",
       "   ask_size4  bid_price4  bid_size4  midprice_change1  midprice_change2  \\\n",
       "0        150    695700.0        104                 1                 0   \n",
       "1        200    740000.0        170                 0                 1   \n",
       "2        100    729900.0        132                 1                 1   \n",
       "3        101    630000.0        104                 0                 1   \n",
       "4        223    849900.0         72                 1                 0   \n",
       "\n",
       "   midprice_change3  midprice_change4  midprice_change5  \n",
       "0                 1                 0                 1  \n",
       "1                 0                 0                 1  \n",
       "2                 0                 0                 1  \n",
       "3                 0                 0                 0  \n",
       "4                 1                 0                 1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read training and test datasets\n",
    "train_df = pd.read_csv('DL-2025-CW-data/Data_A.csv')\n",
    "test_df = pd.read_csv('DL-2025-CW-data/Data_B_nolabels.csv')\n",
    "\n",
    "# Define column names\n",
    "columns_names = ['target']\n",
    "level_cols=[]\n",
    "for i in range(4):\n",
    "    level_cols +=[f'ask_price{i+1}',f'ask_size{i+1}',f'bid_price{i+1}',f'bid_size{i+1}']\n",
    "change_cols =  []\n",
    "for i in range(5):\n",
    "    change_cols+=[f'midprice_change{i+1}']\n",
    "columns_names+=level_cols+change_cols\n",
    "train_df.columns = columns_names\n",
    "test_df.columns = columns_names[1:]\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mid_price(df,  bid_p, ask_p):\n",
    "    return (df[bid_p] + df[ask_p])/2\n",
    "\n",
    "def calc_wap(df, bid_p, ask_p, bid_s, ask_s):\n",
    "    return (df[bid_p] * df[ask_s] + df[ask_p] * df[bid_s]) / (df[bid_s] + df[ask_s])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(df):\n",
    "    # Step 1: Calculate price & volume features\n",
    "    for i in range(1,5):\n",
    "        df[f\"wap{i}\"] = calc_wap(df, f\"bid_price{i}\", f\"ask_price{i}\", f\"bid_size{i}\", f\"ask_size{i}\")\n",
    "        #df[f\"wmp{i}\"] = calc_wmp(df, f\"bid_price{i}\", f\"ask_price{i}\", f\"bid_size{i}\", f\"ask_size{i}\")\n",
    "        df[f\"price_spread{i}\"] = (df[f\"ask_price{i}\"] - df[f\"bid_price{i}\"]) / ((df[f\"ask_price{i}\"] + df[f\"bid_price{i}\"]))\n",
    "        df[f'midprice{i}']=mid_price(df,f'bid_price{i}',f'ask_price{i}')\n",
    " \n",
    "        df[f\"order_imbalance{i}\"] = (df[f\"bid_size{i}\"] - df[f\"ask_size{i}\"]) / (df[f\"bid_size{i}\"] + df[f\"ask_size{i}\"])\n",
    "        df[f'bid_ask_ratio{i}'] = df[f'bid_size{i}'] / (df[f'bid_size{i}'] + df[f'ask_size{i}'])\n",
    "        \n",
    "    # Price features\n",
    "    df['bid_depth_ratio'] = df['bid_size1'] / (df[['bid_size1','bid_size2','bid_size3','bid_size4']].sum(axis=1))\n",
    "    df['ask_depth_ratio'] = df['ask_size1'] / (df[['ask_size1','ask_size2','ask_size3','ask_size4']].sum(axis=1))\n",
    "\n",
    "    # Volume features\n",
    "    df[\"total_volume\"] = df[[\"ask_size1\", \"ask_size2\", \"bid_size1\", \"bid_size2\"]].sum(axis=1)\n",
    "    df[\"volume_imbalance\"] = (\n",
    "        (df[\"ask_size1\"] + df[\"ask_size2\"]) - (df[\"bid_size1\"] + df[\"bid_size2\"])\n",
    "    ).abs()\n",
    "\n",
    "    # Order imbalances \n",
    "    df['bid_vol_ratio'] = df[['bid_size1','bid_size2']].sum(axis=1) / df[['bid_size1','bid_size2','ask_size1','ask_size2']].sum(axis=1)\n",
    "    df['ask_vol_ratio'] = 1 - df['bid_vol_ratio']\n",
    "    \n",
    "    #df = df.drop(columns=level_cols)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nn = preprocessor(train_df)\n",
    "test_nn = preprocessor(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ask_price1</th>\n",
       "      <th>ask_size1</th>\n",
       "      <th>bid_price1</th>\n",
       "      <th>bid_size1</th>\n",
       "      <th>ask_price2</th>\n",
       "      <th>ask_size2</th>\n",
       "      <th>bid_price2</th>\n",
       "      <th>bid_size2</th>\n",
       "      <th>ask_price3</th>\n",
       "      <th>...</th>\n",
       "      <th>price_spread4</th>\n",
       "      <th>midprice4</th>\n",
       "      <th>order_imbalance4</th>\n",
       "      <th>bid_ask_ratio4</th>\n",
       "      <th>bid_depth_ratio</th>\n",
       "      <th>ask_depth_ratio</th>\n",
       "      <th>total_volume</th>\n",
       "      <th>volume_imbalance</th>\n",
       "      <th>bid_vol_ratio</th>\n",
       "      <th>ask_vol_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>650400.0</td>\n",
       "      <td>501</td>\n",
       "      <td>650200.0</td>\n",
       "      <td>106</td>\n",
       "      <td>650500.0</td>\n",
       "      <td>245</td>\n",
       "      <td>650100.0</td>\n",
       "      <td>259</td>\n",
       "      <td>650600.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>650300.0</td>\n",
       "      <td>0.325359</td>\n",
       "      <td>0.662679</td>\n",
       "      <td>0.109278</td>\n",
       "      <td>0.484058</td>\n",
       "      <td>1111</td>\n",
       "      <td>381</td>\n",
       "      <td>0.328533</td>\n",
       "      <td>0.671467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>636200.0</td>\n",
       "      <td>153</td>\n",
       "      <td>635800.0</td>\n",
       "      <td>150</td>\n",
       "      <td>636300.0</td>\n",
       "      <td>100</td>\n",
       "      <td>635700.0</td>\n",
       "      <td>15</td>\n",
       "      <td>636400.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>636000.0</td>\n",
       "      <td>0.313725</td>\n",
       "      <td>0.656863</td>\n",
       "      <td>0.315126</td>\n",
       "      <td>0.301181</td>\n",
       "      <td>418</td>\n",
       "      <td>88</td>\n",
       "      <td>0.394737</td>\n",
       "      <td>0.605263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>724800.0</td>\n",
       "      <td>4</td>\n",
       "      <td>724500.0</td>\n",
       "      <td>14</td>\n",
       "      <td>724900.0</td>\n",
       "      <td>50</td>\n",
       "      <td>724300.0</td>\n",
       "      <td>312</td>\n",
       "      <td>725100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>724650.0</td>\n",
       "      <td>-0.742529</td>\n",
       "      <td>0.128736</td>\n",
       "      <td>0.029046</td>\n",
       "      <td>0.006861</td>\n",
       "      <td>380</td>\n",
       "      <td>272</td>\n",
       "      <td>0.857895</td>\n",
       "      <td>0.142105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>622900.0</td>\n",
       "      <td>110</td>\n",
       "      <td>622700.0</td>\n",
       "      <td>100</td>\n",
       "      <td>623000.0</td>\n",
       "      <td>523</td>\n",
       "      <td>622600.0</td>\n",
       "      <td>300</td>\n",
       "      <td>623100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000642</td>\n",
       "      <td>622800.0</td>\n",
       "      <td>-0.503106</td>\n",
       "      <td>0.248447</td>\n",
       "      <td>0.117786</td>\n",
       "      <td>0.065632</td>\n",
       "      <td>1033</td>\n",
       "      <td>233</td>\n",
       "      <td>0.387222</td>\n",
       "      <td>0.612778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>620100.0</td>\n",
       "      <td>374</td>\n",
       "      <td>619900.0</td>\n",
       "      <td>110</td>\n",
       "      <td>620200.0</td>\n",
       "      <td>495</td>\n",
       "      <td>619800.0</td>\n",
       "      <td>210</td>\n",
       "      <td>620300.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000645</td>\n",
       "      <td>620000.0</td>\n",
       "      <td>-0.512195</td>\n",
       "      <td>0.243902</td>\n",
       "      <td>0.107738</td>\n",
       "      <td>0.269258</td>\n",
       "      <td>1189</td>\n",
       "      <td>549</td>\n",
       "      <td>0.269134</td>\n",
       "      <td>0.730866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199994</th>\n",
       "      <td>0</td>\n",
       "      <td>429000.0</td>\n",
       "      <td>400</td>\n",
       "      <td>428800.0</td>\n",
       "      <td>500</td>\n",
       "      <td>429100.0</td>\n",
       "      <td>300</td>\n",
       "      <td>428700.0</td>\n",
       "      <td>700</td>\n",
       "      <td>429200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>428900.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.221729</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>1900</td>\n",
       "      <td>500</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.368421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>1</td>\n",
       "      <td>413600.0</td>\n",
       "      <td>100</td>\n",
       "      <td>413500.0</td>\n",
       "      <td>500</td>\n",
       "      <td>413700.0</td>\n",
       "      <td>611</td>\n",
       "      <td>413400.0</td>\n",
       "      <td>457</td>\n",
       "      <td>413800.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000846</td>\n",
       "      <td>413550.0</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.231803</td>\n",
       "      <td>0.070872</td>\n",
       "      <td>1668</td>\n",
       "      <td>246</td>\n",
       "      <td>0.573741</td>\n",
       "      <td>0.426259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>0</td>\n",
       "      <td>381900.0</td>\n",
       "      <td>401</td>\n",
       "      <td>381800.0</td>\n",
       "      <td>100</td>\n",
       "      <td>382000.0</td>\n",
       "      <td>705</td>\n",
       "      <td>381700.0</td>\n",
       "      <td>800</td>\n",
       "      <td>382100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>381850.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.179821</td>\n",
       "      <td>2006</td>\n",
       "      <td>206</td>\n",
       "      <td>0.448654</td>\n",
       "      <td>0.551346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>1</td>\n",
       "      <td>443000.0</td>\n",
       "      <td>100</td>\n",
       "      <td>442900.0</td>\n",
       "      <td>29</td>\n",
       "      <td>443100.0</td>\n",
       "      <td>300</td>\n",
       "      <td>442800.0</td>\n",
       "      <td>100</td>\n",
       "      <td>443200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000790</td>\n",
       "      <td>442950.0</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.039781</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>529</td>\n",
       "      <td>271</td>\n",
       "      <td>0.243856</td>\n",
       "      <td>0.756144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>0</td>\n",
       "      <td>528200.0</td>\n",
       "      <td>202</td>\n",
       "      <td>528000.0</td>\n",
       "      <td>13</td>\n",
       "      <td>528500.0</td>\n",
       "      <td>100</td>\n",
       "      <td>527900.0</td>\n",
       "      <td>706</td>\n",
       "      <td>528600.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001041</td>\n",
       "      <td>528150.0</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.015931</td>\n",
       "      <td>0.495098</td>\n",
       "      <td>1021</td>\n",
       "      <td>417</td>\n",
       "      <td>0.704212</td>\n",
       "      <td>0.295788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199999 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        target  ask_price1  ask_size1  bid_price1  bid_size1  ask_price2  \\\n",
       "0            0    650400.0        501    650200.0        106    650500.0   \n",
       "1            1    636200.0        153    635800.0        150    636300.0   \n",
       "2            0    724800.0          4    724500.0         14    724900.0   \n",
       "3            0    622900.0        110    622700.0        100    623000.0   \n",
       "4            1    620100.0        374    619900.0        110    620200.0   \n",
       "...        ...         ...        ...         ...        ...         ...   \n",
       "199994       0    429000.0        400    428800.0        500    429100.0   \n",
       "199995       1    413600.0        100    413500.0        500    413700.0   \n",
       "199996       0    381900.0        401    381800.0        100    382000.0   \n",
       "199997       1    443000.0        100    442900.0         29    443100.0   \n",
       "199998       0    528200.0        202    528000.0         13    528500.0   \n",
       "\n",
       "        ask_size2  bid_price2  bid_size2  ask_price3  ...  price_spread4  \\\n",
       "0             245    650100.0        259    650600.0  ...       0.000615   \n",
       "1             100    635700.0         15    636400.0  ...       0.000786   \n",
       "2              50    724300.0        312    725100.0  ...       0.000759   \n",
       "3             523    622600.0        300    623100.0  ...       0.000642   \n",
       "4             495    619800.0        210    620300.0  ...       0.000645   \n",
       "...           ...         ...        ...         ...  ...            ...   \n",
       "199994        300    428700.0        700    429200.0  ...       0.000933   \n",
       "199995        611    413400.0        457    413800.0  ...       0.000846   \n",
       "199996        705    381700.0        800    382100.0  ...       0.000917   \n",
       "199997        300    442800.0        100    443200.0  ...       0.000790   \n",
       "199998        100    527900.0        706    528600.0  ...       0.001041   \n",
       "\n",
       "        midprice4  order_imbalance4  bid_ask_ratio4  bid_depth_ratio  \\\n",
       "0        650300.0          0.325359        0.662679         0.109278   \n",
       "1        636000.0          0.313725        0.656863         0.315126   \n",
       "2        724650.0         -0.742529        0.128736         0.029046   \n",
       "3        622800.0         -0.503106        0.248447         0.117786   \n",
       "4        620000.0         -0.512195        0.243902         0.107738   \n",
       "...           ...               ...             ...              ...   \n",
       "199994   428900.0          0.142857        0.571429         0.221729   \n",
       "199995   413550.0          0.454545        0.727273         0.231803   \n",
       "199996   381850.0          0.000000        0.500000         0.058824   \n",
       "199997   442950.0         -0.600000        0.200000         0.039781   \n",
       "199998   528150.0         -0.111111        0.444444         0.015931   \n",
       "\n",
       "        ask_depth_ratio  total_volume  volume_imbalance  bid_vol_ratio  \\\n",
       "0              0.484058          1111               381       0.328533   \n",
       "1              0.301181           418                88       0.394737   \n",
       "2              0.006861           380               272       0.857895   \n",
       "3              0.065632          1033               233       0.387222   \n",
       "4              0.269258          1189               549       0.269134   \n",
       "...                 ...           ...               ...            ...   \n",
       "199994         0.210526          1900               500       0.631579   \n",
       "199995         0.070872          1668               246       0.573741   \n",
       "199996         0.179821          2006               206       0.448654   \n",
       "199997         0.066667           529               271       0.243856   \n",
       "199998         0.495098          1021               417       0.704212   \n",
       "\n",
       "        ask_vol_ratio  \n",
       "0            0.671467  \n",
       "1            0.605263  \n",
       "2            0.142105  \n",
       "3            0.612778  \n",
       "4            0.730866  \n",
       "...               ...  \n",
       "199994       0.368421  \n",
       "199995       0.426259  \n",
       "199996       0.551346  \n",
       "199997       0.756144  \n",
       "199998       0.295788  \n",
       "\n",
       "[199999 rows x 48 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swish(x, beta = 1):\n",
    "    return (x * K.sigmoid(beta * x))\n",
    "\n",
    "get_custom_objects().update({'swish': Activation(swish)})\n",
    "\n",
    "hidden_units = (128,64,32)\n",
    "feature_cols = [c for c in train_nn.columns if c != 'target']\n",
    "num_features = len(feature_cols)\n",
    "\n",
    "def base_model():\n",
    "  \n",
    "    num_input = keras.Input(shape=(num_features,), name='num_data')\n",
    "    x = keras.layers.Concatenate()([num_input])\n",
    "   \n",
    "    for n_hidden in hidden_units:\n",
    "        x = keras.layers.Dense(n_hidden, activation='swish')(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "        x = keras.layers.Dropout(0.3)(x)\n",
    "    out = keras.layers.Dense(1, activation='sigmoid', name='prediction')(x)\n",
    "\n",
    "    model = keras.Model(\n",
    "        inputs=[num_input],\n",
    "        outputs=out\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "313/313 [==============================] - 3s 6ms/step - loss: 0.5876 - accuracy: 0.6803 - val_loss: 0.5540 - val_accuracy: 0.6984 - lr: 0.0050\n",
      "Epoch 2/200\n",
      "313/313 [==============================] - 1s 5ms/step - loss: 0.5618 - accuracy: 0.6913 - val_loss: 0.5451 - val_accuracy: 0.6982 - lr: 0.0050\n",
      "Epoch 3/200\n",
      "313/313 [==============================] - 1s 5ms/step - loss: 0.5537 - accuracy: 0.6941 - val_loss: 0.5368 - val_accuracy: 0.7042 - lr: 0.0050\n",
      "Epoch 4/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5481 - accuracy: 0.6986 - val_loss: 0.5360 - val_accuracy: 0.7075 - lr: 0.0050\n",
      "Epoch 5/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5450 - accuracy: 0.7009 - val_loss: 0.5315 - val_accuracy: 0.7063 - lr: 0.0050\n",
      "Epoch 6/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5426 - accuracy: 0.7034 - val_loss: 0.5284 - val_accuracy: 0.7097 - lr: 0.0050\n",
      "Epoch 7/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5400 - accuracy: 0.7045 - val_loss: 0.5289 - val_accuracy: 0.7112 - lr: 0.0050\n",
      "Epoch 8/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5392 - accuracy: 0.7058 - val_loss: 0.5246 - val_accuracy: 0.7146 - lr: 0.0050\n",
      "Epoch 9/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5377 - accuracy: 0.7081 - val_loss: 0.5249 - val_accuracy: 0.7131 - lr: 0.0050\n",
      "Epoch 10/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5371 - accuracy: 0.7074 - val_loss: 0.5226 - val_accuracy: 0.7147 - lr: 0.0050\n",
      "Epoch 11/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5356 - accuracy: 0.7092 - val_loss: 0.5230 - val_accuracy: 0.7156 - lr: 0.0050\n",
      "Epoch 12/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5351 - accuracy: 0.7085 - val_loss: 0.5230 - val_accuracy: 0.7151 - lr: 0.0050\n",
      "Epoch 13/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5344 - accuracy: 0.7093 - val_loss: 0.5227 - val_accuracy: 0.7166 - lr: 0.0050\n",
      "Epoch 14/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5334 - accuracy: 0.7101 - val_loss: 0.5209 - val_accuracy: 0.7190 - lr: 0.0050\n",
      "Epoch 15/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5330 - accuracy: 0.7108 - val_loss: 0.5210 - val_accuracy: 0.7177 - lr: 0.0050\n",
      "Epoch 16/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5324 - accuracy: 0.7105 - val_loss: 0.5214 - val_accuracy: 0.7179 - lr: 0.0050\n",
      "Epoch 17/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5321 - accuracy: 0.7110 - val_loss: 0.5204 - val_accuracy: 0.7189 - lr: 0.0050\n",
      "Epoch 18/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5316 - accuracy: 0.7108 - val_loss: 0.5196 - val_accuracy: 0.7194 - lr: 0.0050\n",
      "Epoch 19/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5310 - accuracy: 0.7111 - val_loss: 0.5196 - val_accuracy: 0.7203 - lr: 0.0050\n",
      "Epoch 20/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5309 - accuracy: 0.7109 - val_loss: 0.5185 - val_accuracy: 0.7191 - lr: 0.0050\n",
      "Epoch 21/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5306 - accuracy: 0.7136 - val_loss: 0.5180 - val_accuracy: 0.7187 - lr: 0.0050\n",
      "Epoch 22/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5304 - accuracy: 0.7116 - val_loss: 0.5193 - val_accuracy: 0.7209 - lr: 0.0050\n",
      "Epoch 23/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5299 - accuracy: 0.7132 - val_loss: 0.5189 - val_accuracy: 0.7181 - lr: 0.0050\n",
      "Epoch 24/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5293 - accuracy: 0.7131 - val_loss: 0.5180 - val_accuracy: 0.7214 - lr: 0.0050\n",
      "Epoch 25/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5282 - accuracy: 0.7139 - val_loss: 0.5174 - val_accuracy: 0.7204 - lr: 0.0050\n",
      "Epoch 26/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5290 - accuracy: 0.7134 - val_loss: 0.5200 - val_accuracy: 0.7194 - lr: 0.0050\n",
      "Epoch 27/200\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.5284 - accuracy: 0.7141 - val_loss: 0.5183 - val_accuracy: 0.7206 - lr: 0.0050\n",
      "Epoch 28/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5283 - accuracy: 0.7145 - val_loss: 0.5176 - val_accuracy: 0.7219 - lr: 0.0050\n",
      "Epoch 29/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5275 - accuracy: 0.7139 - val_loss: 0.5172 - val_accuracy: 0.7218 - lr: 0.0050\n",
      "Epoch 30/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5272 - accuracy: 0.7158 - val_loss: 0.5169 - val_accuracy: 0.7220 - lr: 0.0050\n",
      "Epoch 31/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5278 - accuracy: 0.7139 - val_loss: 0.5183 - val_accuracy: 0.7187 - lr: 0.0050\n",
      "Epoch 32/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5281 - accuracy: 0.7153 - val_loss: 0.5201 - val_accuracy: 0.7188 - lr: 0.0050\n",
      "Epoch 33/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5264 - accuracy: 0.7156 - val_loss: 0.5177 - val_accuracy: 0.7200 - lr: 0.0050\n",
      "Epoch 34/200\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.5268 - accuracy: 0.7157 - val_loss: 0.5172 - val_accuracy: 0.7208 - lr: 0.0050\n",
      "Epoch 35/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5262 - accuracy: 0.7163 - val_loss: 0.5170 - val_accuracy: 0.7208 - lr: 0.0050\n",
      "Epoch 36/200\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.5262 - accuracy: 0.7167 - val_loss: 0.5178 - val_accuracy: 0.7223 - lr: 0.0050\n",
      "Epoch 37/200\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.5264 - accuracy: 0.7166 - val_loss: 0.5159 - val_accuracy: 0.7229 - lr: 0.0050\n",
      "Epoch 38/200\n",
      "313/313 [==============================] - 1s 5ms/step - loss: 0.5257 - accuracy: 0.7162 - val_loss: 0.5187 - val_accuracy: 0.7178 - lr: 0.0050\n",
      "Epoch 39/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5262 - accuracy: 0.7158 - val_loss: 0.5169 - val_accuracy: 0.7234 - lr: 0.0050\n",
      "Epoch 40/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5258 - accuracy: 0.7159 - val_loss: 0.5161 - val_accuracy: 0.7226 - lr: 0.0050\n",
      "Epoch 41/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5260 - accuracy: 0.7162 - val_loss: 0.5164 - val_accuracy: 0.7206 - lr: 0.0050\n",
      "Epoch 42/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5256 - accuracy: 0.7169 - val_loss: 0.5157 - val_accuracy: 0.7225 - lr: 0.0050\n",
      "Epoch 43/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5248 - accuracy: 0.7167 - val_loss: 0.5199 - val_accuracy: 0.7174 - lr: 0.0050\n",
      "Epoch 44/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5247 - accuracy: 0.7166 - val_loss: 0.5176 - val_accuracy: 0.7197 - lr: 0.0050\n",
      "Epoch 45/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5251 - accuracy: 0.7159 - val_loss: 0.5167 - val_accuracy: 0.7218 - lr: 0.0050\n",
      "Epoch 46/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5247 - accuracy: 0.7167 - val_loss: 0.5171 - val_accuracy: 0.7200 - lr: 0.0050\n",
      "Epoch 47/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5245 - accuracy: 0.7167 - val_loss: 0.5155 - val_accuracy: 0.7217 - lr: 0.0050\n",
      "Epoch 48/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5249 - accuracy: 0.7172 - val_loss: 0.5179 - val_accuracy: 0.7220 - lr: 0.0050\n",
      "Epoch 49/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5239 - accuracy: 0.7172 - val_loss: 0.5157 - val_accuracy: 0.7225 - lr: 0.0050\n",
      "Epoch 50/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5237 - accuracy: 0.7173 - val_loss: 0.5164 - val_accuracy: 0.7228 - lr: 0.0050\n",
      "Epoch 51/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5241 - accuracy: 0.7173 - val_loss: 0.5150 - val_accuracy: 0.7234 - lr: 0.0050\n",
      "Epoch 52/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5239 - accuracy: 0.7186 - val_loss: 0.5150 - val_accuracy: 0.7228 - lr: 0.0050\n",
      "Epoch 53/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5237 - accuracy: 0.7180 - val_loss: 0.5151 - val_accuracy: 0.7218 - lr: 0.0050\n",
      "Epoch 54/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5239 - accuracy: 0.7184 - val_loss: 0.5149 - val_accuracy: 0.7228 - lr: 0.0050\n",
      "Epoch 55/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5244 - accuracy: 0.7175 - val_loss: 0.5161 - val_accuracy: 0.7213 - lr: 0.0050\n",
      "Epoch 56/200\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.5233 - accuracy: 0.7189 - val_loss: 0.5166 - val_accuracy: 0.7214 - lr: 0.0050\n",
      "Epoch 57/200\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.5230 - accuracy: 0.7180 - val_loss: 0.5155 - val_accuracy: 0.7230 - lr: 0.0050\n",
      "Epoch 58/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5235 - accuracy: 0.7180 - val_loss: 0.5155 - val_accuracy: 0.7204 - lr: 0.0050\n",
      "Epoch 59/200\n",
      "313/313 [==============================] - 1s 5ms/step - loss: 0.5211 - accuracy: 0.7194 - val_loss: 0.5132 - val_accuracy: 0.7246 - lr: 1.0000e-03\n",
      "Epoch 60/200\n",
      "313/313 [==============================] - 1s 5ms/step - loss: 0.5199 - accuracy: 0.7203 - val_loss: 0.5136 - val_accuracy: 0.7240 - lr: 1.0000e-03\n",
      "Epoch 61/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5194 - accuracy: 0.7212 - val_loss: 0.5132 - val_accuracy: 0.7240 - lr: 1.0000e-03\n",
      "Epoch 62/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5199 - accuracy: 0.7205 - val_loss: 0.5130 - val_accuracy: 0.7225 - lr: 1.0000e-03\n",
      "Epoch 63/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5191 - accuracy: 0.7214 - val_loss: 0.5127 - val_accuracy: 0.7252 - lr: 1.0000e-03\n",
      "Epoch 64/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5191 - accuracy: 0.7230 - val_loss: 0.5131 - val_accuracy: 0.7236 - lr: 1.0000e-03\n",
      "Epoch 65/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5186 - accuracy: 0.7218 - val_loss: 0.5127 - val_accuracy: 0.7248 - lr: 1.0000e-03\n",
      "Epoch 66/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5186 - accuracy: 0.7221 - val_loss: 0.5130 - val_accuracy: 0.7246 - lr: 1.0000e-03\n",
      "Epoch 67/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5180 - accuracy: 0.7217 - val_loss: 0.5127 - val_accuracy: 0.7234 - lr: 1.0000e-03\n",
      "Epoch 68/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5194 - accuracy: 0.7206 - val_loss: 0.5130 - val_accuracy: 0.7235 - lr: 1.0000e-03\n",
      "Epoch 69/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5183 - accuracy: 0.7212 - val_loss: 0.5130 - val_accuracy: 0.7243 - lr: 1.0000e-03\n",
      "Epoch 70/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5189 - accuracy: 0.7222 - val_loss: 0.5127 - val_accuracy: 0.7236 - lr: 1.0000e-03\n",
      "Epoch 71/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5184 - accuracy: 0.7221 - val_loss: 0.5125 - val_accuracy: 0.7247 - lr: 2.0000e-04\n",
      "Epoch 72/200\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.5187 - accuracy: 0.7220 - val_loss: 0.5126 - val_accuracy: 0.7251 - lr: 2.0000e-04\n",
      "Epoch 73/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5180 - accuracy: 0.7226 - val_loss: 0.5126 - val_accuracy: 0.7248 - lr: 2.0000e-04\n",
      "Epoch 74/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5174 - accuracy: 0.7228 - val_loss: 0.5125 - val_accuracy: 0.7245 - lr: 2.0000e-04\n",
      "Epoch 75/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5173 - accuracy: 0.7227 - val_loss: 0.5124 - val_accuracy: 0.7247 - lr: 2.0000e-04\n",
      "Epoch 76/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5174 - accuracy: 0.7238 - val_loss: 0.5124 - val_accuracy: 0.7251 - lr: 2.0000e-04\n",
      "Epoch 77/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5175 - accuracy: 0.7231 - val_loss: 0.5124 - val_accuracy: 0.7253 - lr: 2.0000e-04\n",
      "Epoch 78/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5180 - accuracy: 0.7228 - val_loss: 0.5123 - val_accuracy: 0.7254 - lr: 2.0000e-04\n",
      "Epoch 79/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5174 - accuracy: 0.7228 - val_loss: 0.5125 - val_accuracy: 0.7254 - lr: 2.0000e-04\n",
      "Epoch 80/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5172 - accuracy: 0.7224 - val_loss: 0.5123 - val_accuracy: 0.7254 - lr: 2.0000e-04\n",
      "Epoch 81/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5181 - accuracy: 0.7231 - val_loss: 0.5124 - val_accuracy: 0.7247 - lr: 2.0000e-04\n",
      "Epoch 82/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5171 - accuracy: 0.7227 - val_loss: 0.5126 - val_accuracy: 0.7251 - lr: 2.0000e-04\n",
      "Epoch 83/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5167 - accuracy: 0.7234 - val_loss: 0.5125 - val_accuracy: 0.7251 - lr: 4.0000e-05\n",
      "Epoch 84/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5174 - accuracy: 0.7215 - val_loss: 0.5124 - val_accuracy: 0.7250 - lr: 4.0000e-05\n",
      "Epoch 85/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5172 - accuracy: 0.7232 - val_loss: 0.5125 - val_accuracy: 0.7246 - lr: 4.0000e-05\n",
      "Epoch 86/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5171 - accuracy: 0.7230 - val_loss: 0.5124 - val_accuracy: 0.7249 - lr: 4.0000e-05\n",
      "Epoch 87/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5173 - accuracy: 0.7233 - val_loss: 0.5123 - val_accuracy: 0.7250 - lr: 4.0000e-05\n",
      "Epoch 88/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5178 - accuracy: 0.7237 - val_loss: 0.5124 - val_accuracy: 0.7250 - lr: 4.0000e-05\n",
      "Epoch 89/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5172 - accuracy: 0.7230 - val_loss: 0.5124 - val_accuracy: 0.7250 - lr: 4.0000e-05\n",
      "Epoch 90/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5174 - accuracy: 0.7229 - val_loss: 0.5124 - val_accuracy: 0.7247 - lr: 8.0000e-06\n",
      "Epoch 91/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5166 - accuracy: 0.7236 - val_loss: 0.5123 - val_accuracy: 0.7247 - lr: 8.0000e-06\n",
      "Epoch 92/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5173 - accuracy: 0.7224 - val_loss: 0.5123 - val_accuracy: 0.7250 - lr: 8.0000e-06\n",
      "Epoch 93/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5175 - accuracy: 0.7226 - val_loss: 0.5123 - val_accuracy: 0.7250 - lr: 8.0000e-06\n",
      "Epoch 94/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5167 - accuracy: 0.7236 - val_loss: 0.5123 - val_accuracy: 0.7250 - lr: 8.0000e-06\n",
      "Epoch 95/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5176 - accuracy: 0.7225 - val_loss: 0.5123 - val_accuracy: 0.7252 - lr: 8.0000e-06\n",
      "Epoch 96/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5175 - accuracy: 0.7220 - val_loss: 0.5124 - val_accuracy: 0.7250 - lr: 8.0000e-06\n",
      "Epoch 97/200\n",
      "211/313 [===================>..........] - ETA: 0s - loss: 0.5167 - accuracy: 0.7239"
     ]
    }
   ],
   "source": [
    "train_nn = train_nn.fillna(0)\n",
    "test_nn = test_nn.fillna(0)\n",
    "\n",
    "X = train_nn[feature_cols]\n",
    "y = train_nn['target']\n",
    "\n",
    "test_predictions_nn = np.zeros(test_nn.shape[0])\n",
    "scores_folds = {'NN_model': []}\n",
    "\n",
    "# KFold \n",
    "kfolds = 5\n",
    "kf = KFold(n_splits=kfolds, shuffle=True, random_state=42)\n",
    "\n",
    "counter = 1\n",
    "\n",
    "for train_index, val_index in kf.split(X):\n",
    "\n",
    "    X_train = X.iloc[train_index]\n",
    "    y_train = y.iloc[train_index]\n",
    "    X_val = X.iloc[val_index]\n",
    "    y_val = y.iloc[val_index]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "    model = base_model()\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.005),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # ---- callbacks ----\n",
    "    # Early stopping callback\n",
    "    es = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=20, verbose=0,\n",
    "        mode='min', restore_best_weights=True)\n",
    "\n",
    "    # Learning rate reduction callback \n",
    "    plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.2, patience=7, verbose=0,\n",
    "        mode='min')\n",
    "    model.fit(\n",
    "        [X_train_scaled],\n",
    "        y_train,\n",
    "        batch_size=512,\n",
    "        epochs=200,\n",
    "        validation_data=([X_val_scaled], y_val),\n",
    "        callbacks=[es, plateau],\n",
    "        shuffle=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    preds = model.predict([X_val_scaled]).reshape(-1)\n",
    "    score = np.mean((preds.round() != y_val.values).astype(float)) \n",
    "    print(f'Fold {counter}: Accuracy = {1 - score:.5f}')\n",
    "    scores_folds['NN_model'].append(1 - score)\n",
    "    \n",
    "    X_test = test_nn[feature_cols]\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    test_predictions_nn += model.predict([X_test_scaled]).reshape(-1) / kfolds\n",
    "   \n",
    "    counter += 1\n",
    "print(\"CV folds scores:\", scores_folds['NN_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = (test_predictions_nn > 0.5).astype(int)\n",
    "np.savetxt(\"02205857_Wang.txt\", final_pred, fmt=\"%d\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_evn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
