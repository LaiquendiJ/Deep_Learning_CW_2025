{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATH7016 Deep Learning\n",
    "### Coursework 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.layers import Activation\n",
    "from keras.utils import get_custom_objects\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ask_price1</th>\n",
       "      <th>ask_size1</th>\n",
       "      <th>bid_price1</th>\n",
       "      <th>bid_size1</th>\n",
       "      <th>ask_price2</th>\n",
       "      <th>ask_size2</th>\n",
       "      <th>bid_price2</th>\n",
       "      <th>bid_size2</th>\n",
       "      <th>ask_price3</th>\n",
       "      <th>ask_size3</th>\n",
       "      <th>...</th>\n",
       "      <th>bid_size3</th>\n",
       "      <th>ask_price4</th>\n",
       "      <th>ask_size4</th>\n",
       "      <th>bid_price4</th>\n",
       "      <th>bid_size4</th>\n",
       "      <th>midprice_change1</th>\n",
       "      <th>midprice_change2</th>\n",
       "      <th>midprice_change3</th>\n",
       "      <th>midprice_change4</th>\n",
       "      <th>midprice_change5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>696400.0</td>\n",
       "      <td>16</td>\n",
       "      <td>696000.0</td>\n",
       "      <td>12</td>\n",
       "      <td>696500.0</td>\n",
       "      <td>57</td>\n",
       "      <td>695900.0</td>\n",
       "      <td>118</td>\n",
       "      <td>696600.0</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>262</td>\n",
       "      <td>696700.0</td>\n",
       "      <td>150</td>\n",
       "      <td>695700.0</td>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>740800.0</td>\n",
       "      <td>2</td>\n",
       "      <td>740400.0</td>\n",
       "      <td>20</td>\n",
       "      <td>741000.0</td>\n",
       "      <td>60</td>\n",
       "      <td>740200.0</td>\n",
       "      <td>27</td>\n",
       "      <td>741200.0</td>\n",
       "      <td>156</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>741300.0</td>\n",
       "      <td>200</td>\n",
       "      <td>740000.0</td>\n",
       "      <td>170</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>730900.0</td>\n",
       "      <td>1</td>\n",
       "      <td>730200.0</td>\n",
       "      <td>230</td>\n",
       "      <td>731000.0</td>\n",
       "      <td>111</td>\n",
       "      <td>730100.0</td>\n",
       "      <td>86</td>\n",
       "      <td>731100.0</td>\n",
       "      <td>42</td>\n",
       "      <td>...</td>\n",
       "      <td>136</td>\n",
       "      <td>731200.0</td>\n",
       "      <td>100</td>\n",
       "      <td>729900.0</td>\n",
       "      <td>132</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>630600.0</td>\n",
       "      <td>100</td>\n",
       "      <td>630300.0</td>\n",
       "      <td>69</td>\n",
       "      <td>630700.0</td>\n",
       "      <td>110</td>\n",
       "      <td>630200.0</td>\n",
       "      <td>2</td>\n",
       "      <td>630800.0</td>\n",
       "      <td>219</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>630900.0</td>\n",
       "      <td>101</td>\n",
       "      <td>630000.0</td>\n",
       "      <td>104</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>851100.0</td>\n",
       "      <td>579</td>\n",
       "      <td>850300.0</td>\n",
       "      <td>25</td>\n",
       "      <td>851200.0</td>\n",
       "      <td>17</td>\n",
       "      <td>850100.0</td>\n",
       "      <td>287</td>\n",
       "      <td>851400.0</td>\n",
       "      <td>307</td>\n",
       "      <td>...</td>\n",
       "      <td>270</td>\n",
       "      <td>851500.0</td>\n",
       "      <td>223</td>\n",
       "      <td>849900.0</td>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ask_price1  ask_size1  bid_price1  bid_size1  ask_price2  ask_size2  \\\n",
       "0    696400.0         16    696000.0         12    696500.0         57   \n",
       "1    740800.0          2    740400.0         20    741000.0         60   \n",
       "2    730900.0          1    730200.0        230    731000.0        111   \n",
       "3    630600.0        100    630300.0         69    630700.0        110   \n",
       "4    851100.0        579    850300.0         25    851200.0         17   \n",
       "\n",
       "   bid_price2  bid_size2  ask_price3  ask_size3  ...  bid_size3  ask_price4  \\\n",
       "0    695900.0        118    696600.0        100  ...        262    696700.0   \n",
       "1    740200.0         27    741200.0        156  ...         31    741300.0   \n",
       "2    730100.0         86    731100.0         42  ...        136    731200.0   \n",
       "3    630200.0          2    630800.0        219  ...          1    630900.0   \n",
       "4    850100.0        287    851400.0        307  ...        270    851500.0   \n",
       "\n",
       "   ask_size4  bid_price4  bid_size4  midprice_change1  midprice_change2  \\\n",
       "0        150    695700.0        104                 1                 0   \n",
       "1        200    740000.0        170                 0                 1   \n",
       "2        100    729900.0        132                 1                 1   \n",
       "3        101    630000.0        104                 0                 1   \n",
       "4        223    849900.0         72                 1                 0   \n",
       "\n",
       "   midprice_change3  midprice_change4  midprice_change5  \n",
       "0                 1                 0                 1  \n",
       "1                 0                 0                 1  \n",
       "2                 0                 0                 1  \n",
       "3                 0                 0                 0  \n",
       "4                 1                 0                 1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('DL-2025-CW-data/Data_A.csv')\n",
    "test_df = pd.read_csv('DL-2025-CW-data/Data_B_nolabels.csv')\n",
    "\n",
    "columns_names = ['target']\n",
    "level_cols=[]\n",
    "for i in range(4):\n",
    "    level_cols +=[f'ask_price{i+1}',f'ask_size{i+1}',f'bid_price{i+1}',f'bid_size{i+1}']\n",
    "change_cols =  []\n",
    "for i in range(5):\n",
    "    change_cols+=[f'midprice_change{i+1}']\n",
    "columns_names+=level_cols+change_cols\n",
    "train_df.columns = columns_names\n",
    "test_df.columns = columns_names[1:]\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ask_price1</th>\n",
       "      <th>ask_size1</th>\n",
       "      <th>bid_price1</th>\n",
       "      <th>bid_size1</th>\n",
       "      <th>ask_price2</th>\n",
       "      <th>ask_size2</th>\n",
       "      <th>bid_price2</th>\n",
       "      <th>bid_size2</th>\n",
       "      <th>ask_price3</th>\n",
       "      <th>...</th>\n",
       "      <th>bid_size3</th>\n",
       "      <th>ask_price4</th>\n",
       "      <th>ask_size4</th>\n",
       "      <th>bid_price4</th>\n",
       "      <th>bid_size4</th>\n",
       "      <th>midprice_change1</th>\n",
       "      <th>midprice_change2</th>\n",
       "      <th>midprice_change3</th>\n",
       "      <th>midprice_change4</th>\n",
       "      <th>midprice_change5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>650400.0</td>\n",
       "      <td>501</td>\n",
       "      <td>650200.0</td>\n",
       "      <td>106</td>\n",
       "      <td>650500.0</td>\n",
       "      <td>245</td>\n",
       "      <td>650100.0</td>\n",
       "      <td>259</td>\n",
       "      <td>650600.0</td>\n",
       "      <td>...</td>\n",
       "      <td>328</td>\n",
       "      <td>650700.0</td>\n",
       "      <td>141</td>\n",
       "      <td>649900.0</td>\n",
       "      <td>277</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>636200.0</td>\n",
       "      <td>153</td>\n",
       "      <td>635800.0</td>\n",
       "      <td>150</td>\n",
       "      <td>636300.0</td>\n",
       "      <td>100</td>\n",
       "      <td>635700.0</td>\n",
       "      <td>15</td>\n",
       "      <td>636400.0</td>\n",
       "      <td>...</td>\n",
       "      <td>110</td>\n",
       "      <td>636500.0</td>\n",
       "      <td>105</td>\n",
       "      <td>635500.0</td>\n",
       "      <td>201</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>724800.0</td>\n",
       "      <td>4</td>\n",
       "      <td>724500.0</td>\n",
       "      <td>14</td>\n",
       "      <td>724900.0</td>\n",
       "      <td>50</td>\n",
       "      <td>724300.0</td>\n",
       "      <td>312</td>\n",
       "      <td>725100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>100</td>\n",
       "      <td>725200.0</td>\n",
       "      <td>379</td>\n",
       "      <td>724100.0</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>622900.0</td>\n",
       "      <td>110</td>\n",
       "      <td>622700.0</td>\n",
       "      <td>100</td>\n",
       "      <td>623000.0</td>\n",
       "      <td>523</td>\n",
       "      <td>622600.0</td>\n",
       "      <td>300</td>\n",
       "      <td>623100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>249</td>\n",
       "      <td>623200.0</td>\n",
       "      <td>605</td>\n",
       "      <td>622400.0</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>620100.0</td>\n",
       "      <td>374</td>\n",
       "      <td>619900.0</td>\n",
       "      <td>110</td>\n",
       "      <td>620200.0</td>\n",
       "      <td>495</td>\n",
       "      <td>619800.0</td>\n",
       "      <td>210</td>\n",
       "      <td>620300.0</td>\n",
       "      <td>...</td>\n",
       "      <td>601</td>\n",
       "      <td>620400.0</td>\n",
       "      <td>310</td>\n",
       "      <td>619600.0</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199994</th>\n",
       "      <td>0</td>\n",
       "      <td>429000.0</td>\n",
       "      <td>400</td>\n",
       "      <td>428800.0</td>\n",
       "      <td>500</td>\n",
       "      <td>429100.0</td>\n",
       "      <td>300</td>\n",
       "      <td>428700.0</td>\n",
       "      <td>700</td>\n",
       "      <td>429200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>655</td>\n",
       "      <td>429300.0</td>\n",
       "      <td>300</td>\n",
       "      <td>428500.0</td>\n",
       "      <td>400</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>1</td>\n",
       "      <td>413600.0</td>\n",
       "      <td>100</td>\n",
       "      <td>413500.0</td>\n",
       "      <td>500</td>\n",
       "      <td>413700.0</td>\n",
       "      <td>611</td>\n",
       "      <td>413400.0</td>\n",
       "      <td>457</td>\n",
       "      <td>413800.0</td>\n",
       "      <td>...</td>\n",
       "      <td>400</td>\n",
       "      <td>413900.0</td>\n",
       "      <td>300</td>\n",
       "      <td>413200.0</td>\n",
       "      <td>800</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>0</td>\n",
       "      <td>381900.0</td>\n",
       "      <td>401</td>\n",
       "      <td>381800.0</td>\n",
       "      <td>100</td>\n",
       "      <td>382000.0</td>\n",
       "      <td>705</td>\n",
       "      <td>381700.0</td>\n",
       "      <td>800</td>\n",
       "      <td>382100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>400</td>\n",
       "      <td>382200.0</td>\n",
       "      <td>400</td>\n",
       "      <td>381500.0</td>\n",
       "      <td>400</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>1</td>\n",
       "      <td>443000.0</td>\n",
       "      <td>100</td>\n",
       "      <td>442900.0</td>\n",
       "      <td>29</td>\n",
       "      <td>443100.0</td>\n",
       "      <td>300</td>\n",
       "      <td>442800.0</td>\n",
       "      <td>100</td>\n",
       "      <td>443200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>400</td>\n",
       "      <td>443300.0</td>\n",
       "      <td>800</td>\n",
       "      <td>442600.0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>0</td>\n",
       "      <td>528200.0</td>\n",
       "      <td>202</td>\n",
       "      <td>528000.0</td>\n",
       "      <td>13</td>\n",
       "      <td>528500.0</td>\n",
       "      <td>100</td>\n",
       "      <td>527900.0</td>\n",
       "      <td>706</td>\n",
       "      <td>528600.0</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>528700.0</td>\n",
       "      <td>100</td>\n",
       "      <td>527600.0</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199999 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        target  ask_price1  ask_size1  bid_price1  bid_size1  ask_price2  \\\n",
       "0            0    650400.0        501    650200.0        106    650500.0   \n",
       "1            1    636200.0        153    635800.0        150    636300.0   \n",
       "2            0    724800.0          4    724500.0         14    724900.0   \n",
       "3            0    622900.0        110    622700.0        100    623000.0   \n",
       "4            1    620100.0        374    619900.0        110    620200.0   \n",
       "...        ...         ...        ...         ...        ...         ...   \n",
       "199994       0    429000.0        400    428800.0        500    429100.0   \n",
       "199995       1    413600.0        100    413500.0        500    413700.0   \n",
       "199996       0    381900.0        401    381800.0        100    382000.0   \n",
       "199997       1    443000.0        100    442900.0         29    443100.0   \n",
       "199998       0    528200.0        202    528000.0         13    528500.0   \n",
       "\n",
       "        ask_size2  bid_price2  bid_size2  ask_price3  ...  bid_size3  \\\n",
       "0             245    650100.0        259    650600.0  ...        328   \n",
       "1             100    635700.0         15    636400.0  ...        110   \n",
       "2              50    724300.0        312    725100.0  ...        100   \n",
       "3             523    622600.0        300    623100.0  ...        249   \n",
       "4             495    619800.0        210    620300.0  ...        601   \n",
       "...           ...         ...        ...         ...  ...        ...   \n",
       "199994        300    428700.0        700    429200.0  ...        655   \n",
       "199995        611    413400.0        457    413800.0  ...        400   \n",
       "199996        705    381700.0        800    382100.0  ...        400   \n",
       "199997        300    442800.0        100    443200.0  ...        400   \n",
       "199998        100    527900.0        706    528600.0  ...         17   \n",
       "\n",
       "        ask_price4  ask_size4  bid_price4  bid_size4  midprice_change1  \\\n",
       "0         650700.0        141    649900.0        277                 0   \n",
       "1         636500.0        105    635500.0        201                 0   \n",
       "2         725200.0        379    724100.0         56                 1   \n",
       "3         623200.0        605    622400.0        200                 1   \n",
       "4         620400.0        310    619600.0        100                 0   \n",
       "...            ...        ...         ...        ...               ...   \n",
       "199994    429300.0        300    428500.0        400                 1   \n",
       "199995    413900.0        300    413200.0        800                 0   \n",
       "199996    382200.0        400    381500.0        400                 1   \n",
       "199997    443300.0        800    442600.0        200                 0   \n",
       "199998    528700.0        100    527600.0         80                 1   \n",
       "\n",
       "        midprice_change2  midprice_change3  midprice_change4  midprice_change5  \n",
       "0                      0                 1                 1                 1  \n",
       "1                      1                 0                 1                 0  \n",
       "2                      0                 0                 1                 1  \n",
       "3                      0                 1                 1                 0  \n",
       "4                      1                 0                 1                 0  \n",
       "...                  ...               ...               ...               ...  \n",
       "199994                 0                 1                 1                 1  \n",
       "199995                 1                 0                 1                 0  \n",
       "199996                 0                 1                 0                 0  \n",
       "199997                 1                 0                 1                 0  \n",
       "199998                 1                 0                 0                 0  \n",
       "\n",
       "[199999 rows x 22 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_wap(df, bid_p, ask_p, bid_s, ask_s):\n",
    "    return (df[bid_p] * df[ask_s] + df[ask_p] * df[bid_s]) / (df[bid_s] + df[ask_s])\n",
    "\n",
    "def calc_wmp(df, bid_p, ask_p, bid_s, ask_s):\n",
    "    return (df[bid_p] * df[bid_s] + df[ask_p] * df[ask_s]) / (df[bid_s] + df[ask_s])\n",
    "\n",
    "def mid_price(df,  bid_p, ask_p):\n",
    "    return (df[bid_p] + df[ask_p])/2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(df):\n",
    "    # Step 1: Calculate price & volume features\n",
    "    for i in range(1,5):\n",
    "        df[f\"wap{i}\"] = calc_wap(df, f\"bid_price{i}\", f\"ask_price{i}\", f\"bid_size{i}\", f\"ask_size{i}\")\n",
    "        df[f\"wmp{i}\"] = calc_wmp(df, f\"bid_price{i}\", f\"ask_price{i}\", f\"bid_size{i}\", f\"ask_size{i}\")\n",
    "        df[f\"price_spread{i}\"] = (df[f\"ask_price{i}\"] - df[f\"bid_price{i}\"]) / ((df[f\"ask_price{i}\"] + df[f\"bid_price{i}\"]) / 2)\n",
    "        df[f'midprice{i}']=mid_price(df,f'bid_price{i}',f'ask_price{i}')\n",
    " \n",
    "        df[f\"order_imbalance{i}\"] = (df[f\"bid_size{i}\"] - df[f\"ask_size{i}\"]) / (df[f\"bid_size{i}\"] + df[f\"ask_size{i}\"])\n",
    "        df[f'bid_ask_ratio{i}'] = df[f'bid_size{i}'] / (df[f'bid_size{i}'] + df[f'ask_size{i}'])\n",
    "        \n",
    "    # Price features\n",
    "    df[\"wap_balance\"] = (df[\"wap1\"] - df[\"wap2\"]).abs()\n",
    "    df['bid_depth_ratio'] = df['bid_size1'] / (df[['bid_size1','bid_size2','bid_size3','bid_size4']].sum(axis=1))\n",
    "    df['ask_depth_ratio'] = df['ask_size1'] / (df[['ask_size1','ask_size2','ask_size3','ask_size4']].sum(axis=1))\n",
    "\n",
    "    # Volume features\n",
    "    df[\"total_volume\"] = df[[\"ask_size1\", \"ask_size2\", \"bid_size1\", \"bid_size2\"]].sum(axis=1)\n",
    "    df[\"volume_imbalance\"] = (\n",
    "        (df[\"ask_size1\"] + df[\"ask_size2\"]) - (df[\"bid_size1\"] + df[\"bid_size2\"])\n",
    "    ).abs()\n",
    "\n",
    "    # Order imbalances \n",
    "    df[\"order_imbalance_total\"] = (\n",
    "        (df[\"bid_size1\"] + df[\"bid_size2\"] - df[\"ask_size1\"] - df[\"ask_size2\"]) /\n",
    "        (df[\"bid_size1\"] + df[\"bid_size2\"] + df[\"ask_size1\"] + df[\"ask_size2\"])\n",
    "    )\n",
    "\n",
    "    df['bid_vol_ratio'] = df[['bid_size1','bid_size2']].sum(axis=1) / df[['bid_size1','bid_size2','ask_size1','ask_size2']].sum(axis=1)\n",
    "    df['ask_vol_ratio'] = 1 - df['bid_vol_ratio']\n",
    "\n",
    "    # Step 3: Drop raw order book columns\n",
    "    #df = df.drop(columns=level_cols)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nn = preprocessor(train_df)\n",
    "test_nn = preprocessor(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nn=train_df\n",
    "test_nn = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ask_price1</th>\n",
       "      <th>ask_size1</th>\n",
       "      <th>bid_price1</th>\n",
       "      <th>bid_size1</th>\n",
       "      <th>ask_price2</th>\n",
       "      <th>ask_size2</th>\n",
       "      <th>bid_price2</th>\n",
       "      <th>bid_size2</th>\n",
       "      <th>ask_price3</th>\n",
       "      <th>...</th>\n",
       "      <th>order_imbalance4</th>\n",
       "      <th>bid_ask_ratio4</th>\n",
       "      <th>wap_balance</th>\n",
       "      <th>bid_depth_ratio</th>\n",
       "      <th>ask_depth_ratio</th>\n",
       "      <th>total_volume</th>\n",
       "      <th>volume_imbalance</th>\n",
       "      <th>order_imbalance_total</th>\n",
       "      <th>bid_vol_ratio</th>\n",
       "      <th>ask_vol_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>650400.0</td>\n",
       "      <td>501</td>\n",
       "      <td>650200.0</td>\n",
       "      <td>106</td>\n",
       "      <td>650500.0</td>\n",
       "      <td>245</td>\n",
       "      <td>650100.0</td>\n",
       "      <td>259</td>\n",
       "      <td>650600.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.325359</td>\n",
       "      <td>0.662679</td>\n",
       "      <td>70.629691</td>\n",
       "      <td>0.109278</td>\n",
       "      <td>0.484058</td>\n",
       "      <td>1111</td>\n",
       "      <td>381</td>\n",
       "      <td>-0.342934</td>\n",
       "      <td>0.328533</td>\n",
       "      <td>0.671467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>636200.0</td>\n",
       "      <td>153</td>\n",
       "      <td>635800.0</td>\n",
       "      <td>150</td>\n",
       "      <td>636300.0</td>\n",
       "      <td>100</td>\n",
       "      <td>635700.0</td>\n",
       "      <td>15</td>\n",
       "      <td>636400.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.313725</td>\n",
       "      <td>0.656863</td>\n",
       "      <td>219.758932</td>\n",
       "      <td>0.315126</td>\n",
       "      <td>0.301181</td>\n",
       "      <td>418</td>\n",
       "      <td>88</td>\n",
       "      <td>-0.210526</td>\n",
       "      <td>0.394737</td>\n",
       "      <td>0.605263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>724800.0</td>\n",
       "      <td>4</td>\n",
       "      <td>724500.0</td>\n",
       "      <td>14</td>\n",
       "      <td>724900.0</td>\n",
       "      <td>50</td>\n",
       "      <td>724300.0</td>\n",
       "      <td>312</td>\n",
       "      <td>725100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.742529</td>\n",
       "      <td>0.128736</td>\n",
       "      <td>83.793738</td>\n",
       "      <td>0.029046</td>\n",
       "      <td>0.006861</td>\n",
       "      <td>380</td>\n",
       "      <td>272</td>\n",
       "      <td>0.715789</td>\n",
       "      <td>0.857895</td>\n",
       "      <td>0.142105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>622900.0</td>\n",
       "      <td>110</td>\n",
       "      <td>622700.0</td>\n",
       "      <td>100</td>\n",
       "      <td>623000.0</td>\n",
       "      <td>523</td>\n",
       "      <td>622600.0</td>\n",
       "      <td>300</td>\n",
       "      <td>623100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.503106</td>\n",
       "      <td>0.248447</td>\n",
       "      <td>49.430076</td>\n",
       "      <td>0.117786</td>\n",
       "      <td>0.065632</td>\n",
       "      <td>1033</td>\n",
       "      <td>233</td>\n",
       "      <td>-0.225557</td>\n",
       "      <td>0.387222</td>\n",
       "      <td>0.612778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>620100.0</td>\n",
       "      <td>374</td>\n",
       "      <td>619900.0</td>\n",
       "      <td>110</td>\n",
       "      <td>620200.0</td>\n",
       "      <td>495</td>\n",
       "      <td>619800.0</td>\n",
       "      <td>210</td>\n",
       "      <td>620300.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.512195</td>\n",
       "      <td>0.243902</td>\n",
       "      <td>26.305609</td>\n",
       "      <td>0.107738</td>\n",
       "      <td>0.269258</td>\n",
       "      <td>1189</td>\n",
       "      <td>549</td>\n",
       "      <td>-0.461733</td>\n",
       "      <td>0.269134</td>\n",
       "      <td>0.730866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199994</th>\n",
       "      <td>0</td>\n",
       "      <td>429000.0</td>\n",
       "      <td>400</td>\n",
       "      <td>428800.0</td>\n",
       "      <td>500</td>\n",
       "      <td>429100.0</td>\n",
       "      <td>300</td>\n",
       "      <td>428700.0</td>\n",
       "      <td>700</td>\n",
       "      <td>429200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>68.888889</td>\n",
       "      <td>0.221729</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>1900</td>\n",
       "      <td>500</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.368421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>1</td>\n",
       "      <td>413600.0</td>\n",
       "      <td>100</td>\n",
       "      <td>413500.0</td>\n",
       "      <td>500</td>\n",
       "      <td>413700.0</td>\n",
       "      <td>611</td>\n",
       "      <td>413400.0</td>\n",
       "      <td>457</td>\n",
       "      <td>413800.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>54.962547</td>\n",
       "      <td>0.231803</td>\n",
       "      <td>0.070872</td>\n",
       "      <td>1668</td>\n",
       "      <td>246</td>\n",
       "      <td>0.147482</td>\n",
       "      <td>0.573741</td>\n",
       "      <td>0.426259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>0</td>\n",
       "      <td>381900.0</td>\n",
       "      <td>401</td>\n",
       "      <td>381800.0</td>\n",
       "      <td>100</td>\n",
       "      <td>382000.0</td>\n",
       "      <td>705</td>\n",
       "      <td>381700.0</td>\n",
       "      <td>800</td>\n",
       "      <td>382100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>39.508359</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.179821</td>\n",
       "      <td>2006</td>\n",
       "      <td>206</td>\n",
       "      <td>-0.102692</td>\n",
       "      <td>0.448654</td>\n",
       "      <td>0.551346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>1</td>\n",
       "      <td>443000.0</td>\n",
       "      <td>100</td>\n",
       "      <td>442900.0</td>\n",
       "      <td>29</td>\n",
       "      <td>443100.0</td>\n",
       "      <td>300</td>\n",
       "      <td>442800.0</td>\n",
       "      <td>100</td>\n",
       "      <td>443200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>47.480620</td>\n",
       "      <td>0.039781</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>529</td>\n",
       "      <td>271</td>\n",
       "      <td>-0.512287</td>\n",
       "      <td>0.243856</td>\n",
       "      <td>0.756144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>0</td>\n",
       "      <td>528200.0</td>\n",
       "      <td>202</td>\n",
       "      <td>528000.0</td>\n",
       "      <td>13</td>\n",
       "      <td>528500.0</td>\n",
       "      <td>100</td>\n",
       "      <td>527900.0</td>\n",
       "      <td>706</td>\n",
       "      <td>528600.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111111</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>413.465289</td>\n",
       "      <td>0.015931</td>\n",
       "      <td>0.495098</td>\n",
       "      <td>1021</td>\n",
       "      <td>417</td>\n",
       "      <td>0.408423</td>\n",
       "      <td>0.704212</td>\n",
       "      <td>0.295788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199999 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        target  ask_price1  ask_size1  bid_price1  bid_size1  ask_price2  \\\n",
       "0            0    650400.0        501    650200.0        106    650500.0   \n",
       "1            1    636200.0        153    635800.0        150    636300.0   \n",
       "2            0    724800.0          4    724500.0         14    724900.0   \n",
       "3            0    622900.0        110    622700.0        100    623000.0   \n",
       "4            1    620100.0        374    619900.0        110    620200.0   \n",
       "...        ...         ...        ...         ...        ...         ...   \n",
       "199994       0    429000.0        400    428800.0        500    429100.0   \n",
       "199995       1    413600.0        100    413500.0        500    413700.0   \n",
       "199996       0    381900.0        401    381800.0        100    382000.0   \n",
       "199997       1    443000.0        100    442900.0         29    443100.0   \n",
       "199998       0    528200.0        202    528000.0         13    528500.0   \n",
       "\n",
       "        ask_size2  bid_price2  bid_size2  ask_price3  ...  order_imbalance4  \\\n",
       "0             245    650100.0        259    650600.0  ...          0.325359   \n",
       "1             100    635700.0         15    636400.0  ...          0.313725   \n",
       "2              50    724300.0        312    725100.0  ...         -0.742529   \n",
       "3             523    622600.0        300    623100.0  ...         -0.503106   \n",
       "4             495    619800.0        210    620300.0  ...         -0.512195   \n",
       "...           ...         ...        ...         ...  ...               ...   \n",
       "199994        300    428700.0        700    429200.0  ...          0.142857   \n",
       "199995        611    413400.0        457    413800.0  ...          0.454545   \n",
       "199996        705    381700.0        800    382100.0  ...          0.000000   \n",
       "199997        300    442800.0        100    443200.0  ...         -0.600000   \n",
       "199998        100    527900.0        706    528600.0  ...         -0.111111   \n",
       "\n",
       "        bid_ask_ratio4  wap_balance  bid_depth_ratio  ask_depth_ratio  \\\n",
       "0             0.662679    70.629691         0.109278         0.484058   \n",
       "1             0.656863   219.758932         0.315126         0.301181   \n",
       "2             0.128736    83.793738         0.029046         0.006861   \n",
       "3             0.248447    49.430076         0.117786         0.065632   \n",
       "4             0.243902    26.305609         0.107738         0.269258   \n",
       "...                ...          ...              ...              ...   \n",
       "199994        0.571429    68.888889         0.221729         0.210526   \n",
       "199995        0.727273    54.962547         0.231803         0.070872   \n",
       "199996        0.500000    39.508359         0.058824         0.179821   \n",
       "199997        0.200000    47.480620         0.039781         0.066667   \n",
       "199998        0.444444   413.465289         0.015931         0.495098   \n",
       "\n",
       "        total_volume  volume_imbalance  order_imbalance_total  bid_vol_ratio  \\\n",
       "0               1111               381              -0.342934       0.328533   \n",
       "1                418                88              -0.210526       0.394737   \n",
       "2                380               272               0.715789       0.857895   \n",
       "3               1033               233              -0.225557       0.387222   \n",
       "4               1189               549              -0.461733       0.269134   \n",
       "...              ...               ...                    ...            ...   \n",
       "199994          1900               500               0.263158       0.631579   \n",
       "199995          1668               246               0.147482       0.573741   \n",
       "199996          2006               206              -0.102692       0.448654   \n",
       "199997           529               271              -0.512287       0.243856   \n",
       "199998          1021               417               0.408423       0.704212   \n",
       "\n",
       "        ask_vol_ratio  \n",
       "0            0.671467  \n",
       "1            0.605263  \n",
       "2            0.142105  \n",
       "3            0.612778  \n",
       "4            0.730866  \n",
       "...               ...  \n",
       "199994       0.368421  \n",
       "199995       0.426259  \n",
       "199996       0.551346  \n",
       "199997       0.756144  \n",
       "199998       0.295788  \n",
       "\n",
       "[199999 rows x 62 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def swish(x, beta = 1):\n",
    "    return (x * K.sigmoid(beta * x))\n",
    "\n",
    "\n",
    "get_custom_objects().update({'swish': Activation(swish)})\n",
    "\n",
    "hidden_units = (128,64,32)\n",
    "feature_cols = [c for c in train_nn.columns if c != 'target']\n",
    "num_features = len(feature_cols)\n",
    "\n",
    "\n",
    "def base_model():\n",
    "  \n",
    "    num_input = keras.Input(shape=(num_features), name='num_data')\n",
    "\n",
    "    x = keras.layers.Concatenate()([num_input])\n",
    "   \n",
    "    for n_hidden in hidden_units:\n",
    "        x = keras.layers.Dense(n_hidden, activation='swish')(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "        x = keras.layers.Dropout(0.3)(x)\n",
    "    out = keras.layers.Dense(1, activation='sigmoid', name='prediction')(x)\n",
    "\n",
    "    model = keras.Model(\n",
    "        inputs=[num_input],\n",
    "        outputs=out\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5859 - accuracy: 0.6814 - val_loss: 0.5535 - val_accuracy: 0.6974 - lr: 0.0050\n",
      "Epoch 2/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5601 - accuracy: 0.6923 - val_loss: 0.5425 - val_accuracy: 0.7000 - lr: 0.0050\n",
      "Epoch 3/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5533 - accuracy: 0.6939 - val_loss: 0.5375 - val_accuracy: 0.7048 - lr: 0.0050\n",
      "Epoch 4/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5480 - accuracy: 0.6989 - val_loss: 0.5355 - val_accuracy: 0.7070 - lr: 0.0050\n",
      "Epoch 5/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5446 - accuracy: 0.7011 - val_loss: 0.5306 - val_accuracy: 0.7079 - lr: 0.0050\n",
      "Epoch 6/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5425 - accuracy: 0.7036 - val_loss: 0.5283 - val_accuracy: 0.7114 - lr: 0.0050\n",
      "Epoch 7/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5398 - accuracy: 0.7059 - val_loss: 0.5282 - val_accuracy: 0.7114 - lr: 0.0050\n",
      "Epoch 8/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5389 - accuracy: 0.7062 - val_loss: 0.5245 - val_accuracy: 0.7142 - lr: 0.0050\n",
      "Epoch 9/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5381 - accuracy: 0.7062 - val_loss: 0.5252 - val_accuracy: 0.7148 - lr: 0.0050\n",
      "Epoch 10/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5372 - accuracy: 0.7073 - val_loss: 0.5236 - val_accuracy: 0.7149 - lr: 0.0050\n",
      "Epoch 11/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5366 - accuracy: 0.7078 - val_loss: 0.5226 - val_accuracy: 0.7153 - lr: 0.0050\n",
      "Epoch 12/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5360 - accuracy: 0.7088 - val_loss: 0.5238 - val_accuracy: 0.7144 - lr: 0.0050\n",
      "Epoch 13/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5348 - accuracy: 0.7094 - val_loss: 0.5240 - val_accuracy: 0.7164 - lr: 0.0050\n",
      "Epoch 14/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5346 - accuracy: 0.7091 - val_loss: 0.5220 - val_accuracy: 0.7192 - lr: 0.0050\n",
      "Epoch 15/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5336 - accuracy: 0.7098 - val_loss: 0.5219 - val_accuracy: 0.7177 - lr: 0.0050\n",
      "Epoch 16/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5334 - accuracy: 0.7102 - val_loss: 0.5231 - val_accuracy: 0.7164 - lr: 0.0050\n",
      "Epoch 17/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5327 - accuracy: 0.7108 - val_loss: 0.5211 - val_accuracy: 0.7190 - lr: 0.0050\n",
      "Epoch 18/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5327 - accuracy: 0.7110 - val_loss: 0.5208 - val_accuracy: 0.7193 - lr: 0.0050\n",
      "Epoch 19/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5324 - accuracy: 0.7104 - val_loss: 0.5209 - val_accuracy: 0.7178 - lr: 0.0050\n",
      "Epoch 20/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5321 - accuracy: 0.7112 - val_loss: 0.5188 - val_accuracy: 0.7206 - lr: 0.0050\n",
      "Epoch 21/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5310 - accuracy: 0.7131 - val_loss: 0.5190 - val_accuracy: 0.7191 - lr: 0.0050\n",
      "Epoch 22/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5309 - accuracy: 0.7120 - val_loss: 0.5217 - val_accuracy: 0.7194 - lr: 0.0050\n",
      "Epoch 23/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5309 - accuracy: 0.7121 - val_loss: 0.5191 - val_accuracy: 0.7193 - lr: 0.0050\n",
      "Epoch 24/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5300 - accuracy: 0.7128 - val_loss: 0.5192 - val_accuracy: 0.7195 - lr: 0.0050\n",
      "Epoch 25/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5301 - accuracy: 0.7130 - val_loss: 0.5190 - val_accuracy: 0.7192 - lr: 0.0050\n",
      "Epoch 26/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5296 - accuracy: 0.7137 - val_loss: 0.5209 - val_accuracy: 0.7190 - lr: 0.0050\n",
      "Epoch 27/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5294 - accuracy: 0.7134 - val_loss: 0.5187 - val_accuracy: 0.7207 - lr: 0.0050\n",
      "Epoch 28/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5258 - accuracy: 0.7171 - val_loss: 0.5160 - val_accuracy: 0.7229 - lr: 1.0000e-03\n",
      "Epoch 29/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5247 - accuracy: 0.7181 - val_loss: 0.5162 - val_accuracy: 0.7248 - lr: 1.0000e-03\n",
      "Epoch 30/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5239 - accuracy: 0.7192 - val_loss: 0.5155 - val_accuracy: 0.7217 - lr: 1.0000e-03\n",
      "Epoch 31/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5235 - accuracy: 0.7179 - val_loss: 0.5157 - val_accuracy: 0.7218 - lr: 1.0000e-03\n",
      "Epoch 32/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5244 - accuracy: 0.7182 - val_loss: 0.5156 - val_accuracy: 0.7220 - lr: 1.0000e-03\n",
      "Epoch 33/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5234 - accuracy: 0.7183 - val_loss: 0.5156 - val_accuracy: 0.7240 - lr: 1.0000e-03\n",
      "Epoch 34/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5240 - accuracy: 0.7176 - val_loss: 0.5153 - val_accuracy: 0.7242 - lr: 1.0000e-03\n",
      "Epoch 35/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5236 - accuracy: 0.7177 - val_loss: 0.5155 - val_accuracy: 0.7243 - lr: 1.0000e-03\n",
      "Epoch 36/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5231 - accuracy: 0.7173 - val_loss: 0.5151 - val_accuracy: 0.7234 - lr: 1.0000e-03\n",
      "Epoch 37/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5241 - accuracy: 0.7168 - val_loss: 0.5155 - val_accuracy: 0.7236 - lr: 1.0000e-03\n",
      "Epoch 38/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5235 - accuracy: 0.7179 - val_loss: 0.5153 - val_accuracy: 0.7226 - lr: 1.0000e-03\n",
      "Epoch 39/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5236 - accuracy: 0.7192 - val_loss: 0.5145 - val_accuracy: 0.7227 - lr: 1.0000e-03\n",
      "Epoch 40/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5222 - accuracy: 0.7182 - val_loss: 0.5152 - val_accuracy: 0.7242 - lr: 1.0000e-03\n",
      "Epoch 41/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5231 - accuracy: 0.7186 - val_loss: 0.5147 - val_accuracy: 0.7240 - lr: 1.0000e-03\n",
      "Epoch 42/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5225 - accuracy: 0.7180 - val_loss: 0.5153 - val_accuracy: 0.7229 - lr: 1.0000e-03\n",
      "Epoch 43/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5224 - accuracy: 0.7184 - val_loss: 0.5154 - val_accuracy: 0.7224 - lr: 1.0000e-03\n",
      "Epoch 44/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5230 - accuracy: 0.7192 - val_loss: 0.5148 - val_accuracy: 0.7250 - lr: 1.0000e-03\n",
      "Epoch 45/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5220 - accuracy: 0.7184 - val_loss: 0.5148 - val_accuracy: 0.7239 - lr: 1.0000e-03\n",
      "Epoch 46/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5219 - accuracy: 0.7199 - val_loss: 0.5157 - val_accuracy: 0.7246 - lr: 1.0000e-03\n",
      "Epoch 47/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5221 - accuracy: 0.7194 - val_loss: 0.5143 - val_accuracy: 0.7235 - lr: 2.0000e-04\n",
      "Epoch 48/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5217 - accuracy: 0.7194 - val_loss: 0.5143 - val_accuracy: 0.7237 - lr: 2.0000e-04\n",
      "Epoch 49/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5215 - accuracy: 0.7203 - val_loss: 0.5144 - val_accuracy: 0.7240 - lr: 2.0000e-04\n",
      "Epoch 50/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5207 - accuracy: 0.7214 - val_loss: 0.5141 - val_accuracy: 0.7235 - lr: 2.0000e-04\n",
      "Epoch 51/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5207 - accuracy: 0.7210 - val_loss: 0.5141 - val_accuracy: 0.7241 - lr: 2.0000e-04\n",
      "Epoch 52/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5212 - accuracy: 0.7208 - val_loss: 0.5145 - val_accuracy: 0.7242 - lr: 2.0000e-04\n",
      "Epoch 53/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5208 - accuracy: 0.7197 - val_loss: 0.5141 - val_accuracy: 0.7243 - lr: 2.0000e-04\n",
      "Epoch 54/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5210 - accuracy: 0.7206 - val_loss: 0.5141 - val_accuracy: 0.7243 - lr: 2.0000e-04\n",
      "Epoch 55/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5216 - accuracy: 0.7204 - val_loss: 0.5143 - val_accuracy: 0.7237 - lr: 2.0000e-04\n",
      "Epoch 56/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5212 - accuracy: 0.7204 - val_loss: 0.5143 - val_accuracy: 0.7238 - lr: 2.0000e-04\n",
      "Epoch 57/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5208 - accuracy: 0.7204 - val_loss: 0.5144 - val_accuracy: 0.7243 - lr: 2.0000e-04\n",
      "Epoch 58/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5212 - accuracy: 0.7202 - val_loss: 0.5142 - val_accuracy: 0.7240 - lr: 4.0000e-05\n",
      "Epoch 59/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5205 - accuracy: 0.7204 - val_loss: 0.5142 - val_accuracy: 0.7242 - lr: 4.0000e-05\n",
      "Epoch 60/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5208 - accuracy: 0.7200 - val_loss: 0.5143 - val_accuracy: 0.7241 - lr: 4.0000e-05\n",
      "Epoch 61/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5207 - accuracy: 0.7199 - val_loss: 0.5142 - val_accuracy: 0.7242 - lr: 4.0000e-05\n",
      "Epoch 62/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5206 - accuracy: 0.7208 - val_loss: 0.5142 - val_accuracy: 0.7242 - lr: 4.0000e-05\n",
      "Epoch 63/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5211 - accuracy: 0.7192 - val_loss: 0.5142 - val_accuracy: 0.7239 - lr: 4.0000e-05\n",
      "Epoch 64/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5213 - accuracy: 0.7191 - val_loss: 0.5142 - val_accuracy: 0.7238 - lr: 4.0000e-05\n",
      "Epoch 65/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5203 - accuracy: 0.7208 - val_loss: 0.5142 - val_accuracy: 0.7241 - lr: 8.0000e-06\n",
      "Epoch 66/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5207 - accuracy: 0.7205 - val_loss: 0.5142 - val_accuracy: 0.7239 - lr: 8.0000e-06\n",
      "Epoch 67/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5204 - accuracy: 0.7198 - val_loss: 0.5142 - val_accuracy: 0.7240 - lr: 8.0000e-06\n",
      "Epoch 68/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5208 - accuracy: 0.7204 - val_loss: 0.5142 - val_accuracy: 0.7239 - lr: 8.0000e-06\n",
      "Epoch 69/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5203 - accuracy: 0.7206 - val_loss: 0.5143 - val_accuracy: 0.7239 - lr: 8.0000e-06\n",
      "Epoch 70/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5202 - accuracy: 0.7210 - val_loss: 0.5142 - val_accuracy: 0.7240 - lr: 8.0000e-06\n",
      "Epoch 71/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5206 - accuracy: 0.7216 - val_loss: 0.5141 - val_accuracy: 0.7242 - lr: 8.0000e-06\n",
      "Epoch 72/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5215 - accuracy: 0.7197 - val_loss: 0.5141 - val_accuracy: 0.7242 - lr: 1.6000e-06\n",
      "Epoch 73/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5198 - accuracy: 0.7214 - val_loss: 0.5142 - val_accuracy: 0.7239 - lr: 1.6000e-06\n",
      "Epoch 74/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5203 - accuracy: 0.7220 - val_loss: 0.5141 - val_accuracy: 0.7243 - lr: 1.6000e-06\n",
      "1250/1250 [==============================] - 0s 316us/step\n",
      "Fold 1: Accuracy = 0.72425\n",
      "625/625 [==============================] - 0s 321us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5850 - accuracy: 0.6830 - val_loss: 0.5556 - val_accuracy: 0.6983 - lr: 0.0050\n",
      "Epoch 2/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5604 - accuracy: 0.6924 - val_loss: 0.5465 - val_accuracy: 0.6995 - lr: 0.0050\n",
      "Epoch 3/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5523 - accuracy: 0.6963 - val_loss: 0.5360 - val_accuracy: 0.7030 - lr: 0.0050\n",
      "Epoch 4/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5473 - accuracy: 0.7003 - val_loss: 0.5311 - val_accuracy: 0.7074 - lr: 0.0050\n",
      "Epoch 5/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5436 - accuracy: 0.7036 - val_loss: 0.5285 - val_accuracy: 0.7111 - lr: 0.0050\n",
      "Epoch 6/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5427 - accuracy: 0.7043 - val_loss: 0.5280 - val_accuracy: 0.7125 - lr: 0.0050\n",
      "Epoch 7/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5396 - accuracy: 0.7062 - val_loss: 0.5250 - val_accuracy: 0.7146 - lr: 0.0050\n",
      "Epoch 8/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5387 - accuracy: 0.7072 - val_loss: 0.5238 - val_accuracy: 0.7151 - lr: 0.0050\n",
      "Epoch 9/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5379 - accuracy: 0.7067 - val_loss: 0.5239 - val_accuracy: 0.7147 - lr: 0.0050\n",
      "Epoch 10/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5363 - accuracy: 0.7080 - val_loss: 0.5222 - val_accuracy: 0.7153 - lr: 0.0050\n",
      "Epoch 11/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5357 - accuracy: 0.7092 - val_loss: 0.5221 - val_accuracy: 0.7178 - lr: 0.0050\n",
      "Epoch 12/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5345 - accuracy: 0.7100 - val_loss: 0.5208 - val_accuracy: 0.7176 - lr: 0.0050\n",
      "Epoch 13/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5335 - accuracy: 0.7108 - val_loss: 0.5208 - val_accuracy: 0.7164 - lr: 0.0050\n",
      "Epoch 14/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5333 - accuracy: 0.7116 - val_loss: 0.5221 - val_accuracy: 0.7170 - lr: 0.0050\n",
      "Epoch 15/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5334 - accuracy: 0.7120 - val_loss: 0.5211 - val_accuracy: 0.7166 - lr: 0.0050\n",
      "Epoch 16/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5335 - accuracy: 0.7118 - val_loss: 0.5202 - val_accuracy: 0.7166 - lr: 0.0050\n",
      "Epoch 17/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5318 - accuracy: 0.7107 - val_loss: 0.5202 - val_accuracy: 0.7175 - lr: 0.0050\n",
      "Epoch 18/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5317 - accuracy: 0.7120 - val_loss: 0.5216 - val_accuracy: 0.7186 - lr: 0.0050\n",
      "Epoch 19/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5320 - accuracy: 0.7119 - val_loss: 0.5191 - val_accuracy: 0.7198 - lr: 0.0050\n",
      "Epoch 20/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5313 - accuracy: 0.7131 - val_loss: 0.5177 - val_accuracy: 0.7204 - lr: 0.0050\n",
      "Epoch 21/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5305 - accuracy: 0.7131 - val_loss: 0.5179 - val_accuracy: 0.7211 - lr: 0.0050\n",
      "Epoch 22/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5300 - accuracy: 0.7128 - val_loss: 0.5179 - val_accuracy: 0.7214 - lr: 0.0050\n",
      "Epoch 23/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5300 - accuracy: 0.7124 - val_loss: 0.5189 - val_accuracy: 0.7187 - lr: 0.0050\n",
      "Epoch 24/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5299 - accuracy: 0.7123 - val_loss: 0.5177 - val_accuracy: 0.7212 - lr: 0.0050\n",
      "Epoch 25/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5297 - accuracy: 0.7142 - val_loss: 0.5172 - val_accuracy: 0.7212 - lr: 0.0050\n",
      "Epoch 26/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5287 - accuracy: 0.7140 - val_loss: 0.5173 - val_accuracy: 0.7201 - lr: 0.0050\n",
      "Epoch 27/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5286 - accuracy: 0.7152 - val_loss: 0.5168 - val_accuracy: 0.7211 - lr: 0.0050\n",
      "Epoch 28/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5285 - accuracy: 0.7140 - val_loss: 0.5167 - val_accuracy: 0.7203 - lr: 0.0050\n",
      "Epoch 29/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5282 - accuracy: 0.7151 - val_loss: 0.5181 - val_accuracy: 0.7214 - lr: 0.0050\n",
      "Epoch 30/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5285 - accuracy: 0.7155 - val_loss: 0.5156 - val_accuracy: 0.7228 - lr: 0.0050\n",
      "Epoch 31/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5284 - accuracy: 0.7137 - val_loss: 0.5181 - val_accuracy: 0.7194 - lr: 0.0050\n",
      "Epoch 32/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5278 - accuracy: 0.7151 - val_loss: 0.5191 - val_accuracy: 0.7194 - lr: 0.0050\n",
      "Epoch 33/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5275 - accuracy: 0.7155 - val_loss: 0.5153 - val_accuracy: 0.7217 - lr: 0.0050\n",
      "Epoch 34/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5272 - accuracy: 0.7140 - val_loss: 0.5167 - val_accuracy: 0.7196 - lr: 0.0050\n",
      "Epoch 35/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5268 - accuracy: 0.7160 - val_loss: 0.5165 - val_accuracy: 0.7217 - lr: 0.0050\n",
      "Epoch 36/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5269 - accuracy: 0.7164 - val_loss: 0.5171 - val_accuracy: 0.7224 - lr: 0.0050\n",
      "Epoch 37/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5266 - accuracy: 0.7175 - val_loss: 0.5155 - val_accuracy: 0.7227 - lr: 0.0050\n",
      "Epoch 38/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5262 - accuracy: 0.7161 - val_loss: 0.5155 - val_accuracy: 0.7234 - lr: 0.0050\n",
      "Epoch 39/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5266 - accuracy: 0.7162 - val_loss: 0.5164 - val_accuracy: 0.7223 - lr: 0.0050\n",
      "Epoch 40/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5261 - accuracy: 0.7166 - val_loss: 0.5165 - val_accuracy: 0.7194 - lr: 0.0050\n",
      "Epoch 41/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5242 - accuracy: 0.7183 - val_loss: 0.5139 - val_accuracy: 0.7231 - lr: 1.0000e-03\n",
      "Epoch 42/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5226 - accuracy: 0.7187 - val_loss: 0.5130 - val_accuracy: 0.7248 - lr: 1.0000e-03\n",
      "Epoch 43/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5222 - accuracy: 0.7193 - val_loss: 0.5133 - val_accuracy: 0.7242 - lr: 1.0000e-03\n",
      "Epoch 44/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5221 - accuracy: 0.7191 - val_loss: 0.5132 - val_accuracy: 0.7250 - lr: 1.0000e-03\n",
      "Epoch 45/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5219 - accuracy: 0.7206 - val_loss: 0.5134 - val_accuracy: 0.7253 - lr: 1.0000e-03\n",
      "Epoch 46/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5216 - accuracy: 0.7200 - val_loss: 0.5138 - val_accuracy: 0.7254 - lr: 1.0000e-03\n",
      "Epoch 47/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5214 - accuracy: 0.7199 - val_loss: 0.5129 - val_accuracy: 0.7245 - lr: 1.0000e-03\n",
      "Epoch 48/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5211 - accuracy: 0.7201 - val_loss: 0.5130 - val_accuracy: 0.7247 - lr: 1.0000e-03\n",
      "Epoch 49/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5215 - accuracy: 0.7199 - val_loss: 0.5132 - val_accuracy: 0.7255 - lr: 1.0000e-03\n",
      "Epoch 50/200\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.5212 - accuracy: 0.7207 - val_loss: 0.5131 - val_accuracy: 0.7239 - lr: 1.0000e-03\n",
      "Epoch 51/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5205 - accuracy: 0.7212 - val_loss: 0.5129 - val_accuracy: 0.7242 - lr: 1.0000e-03\n",
      "Epoch 52/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5211 - accuracy: 0.7208 - val_loss: 0.5134 - val_accuracy: 0.7250 - lr: 1.0000e-03\n",
      "Epoch 53/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5205 - accuracy: 0.7209 - val_loss: 0.5129 - val_accuracy: 0.7240 - lr: 1.0000e-03\n",
      "Epoch 54/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5210 - accuracy: 0.7204 - val_loss: 0.5128 - val_accuracy: 0.7245 - lr: 1.0000e-03\n",
      "Epoch 55/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5210 - accuracy: 0.7202 - val_loss: 0.5131 - val_accuracy: 0.7246 - lr: 1.0000e-03\n",
      "Epoch 56/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5208 - accuracy: 0.7208 - val_loss: 0.5130 - val_accuracy: 0.7253 - lr: 1.0000e-03\n",
      "Epoch 57/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5203 - accuracy: 0.7218 - val_loss: 0.5130 - val_accuracy: 0.7252 - lr: 1.0000e-03\n",
      "Epoch 58/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5202 - accuracy: 0.7215 - val_loss: 0.5125 - val_accuracy: 0.7252 - lr: 1.0000e-03\n",
      "Epoch 59/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5202 - accuracy: 0.7218 - val_loss: 0.5121 - val_accuracy: 0.7253 - lr: 1.0000e-03\n",
      "Epoch 60/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5202 - accuracy: 0.7215 - val_loss: 0.5129 - val_accuracy: 0.7233 - lr: 1.0000e-03\n",
      "Epoch 61/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5207 - accuracy: 0.7203 - val_loss: 0.5122 - val_accuracy: 0.7269 - lr: 1.0000e-03\n",
      "Epoch 62/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5199 - accuracy: 0.7215 - val_loss: 0.5129 - val_accuracy: 0.7271 - lr: 1.0000e-03\n",
      "Epoch 63/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5207 - accuracy: 0.7207 - val_loss: 0.5128 - val_accuracy: 0.7263 - lr: 1.0000e-03\n",
      "Epoch 64/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5204 - accuracy: 0.7204 - val_loss: 0.5130 - val_accuracy: 0.7246 - lr: 1.0000e-03\n",
      "Epoch 65/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5201 - accuracy: 0.7219 - val_loss: 0.5126 - val_accuracy: 0.7248 - lr: 1.0000e-03\n",
      "Epoch 66/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5200 - accuracy: 0.7209 - val_loss: 0.5127 - val_accuracy: 0.7261 - lr: 1.0000e-03\n",
      "Epoch 67/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5198 - accuracy: 0.7206 - val_loss: 0.5122 - val_accuracy: 0.7274 - lr: 2.0000e-04\n",
      "Epoch 68/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5191 - accuracy: 0.7218 - val_loss: 0.5122 - val_accuracy: 0.7270 - lr: 2.0000e-04\n",
      "Epoch 69/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5190 - accuracy: 0.7221 - val_loss: 0.5120 - val_accuracy: 0.7264 - lr: 2.0000e-04\n",
      "Epoch 70/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5190 - accuracy: 0.7211 - val_loss: 0.5120 - val_accuracy: 0.7272 - lr: 2.0000e-04\n",
      "Epoch 71/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5193 - accuracy: 0.7217 - val_loss: 0.5120 - val_accuracy: 0.7267 - lr: 2.0000e-04\n",
      "Epoch 72/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5189 - accuracy: 0.7233 - val_loss: 0.5120 - val_accuracy: 0.7261 - lr: 2.0000e-04\n",
      "Epoch 73/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5185 - accuracy: 0.7218 - val_loss: 0.5119 - val_accuracy: 0.7265 - lr: 2.0000e-04\n",
      "Epoch 74/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5196 - accuracy: 0.7210 - val_loss: 0.5119 - val_accuracy: 0.7268 - lr: 2.0000e-04\n",
      "Epoch 75/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5191 - accuracy: 0.7212 - val_loss: 0.5120 - val_accuracy: 0.7268 - lr: 2.0000e-04\n",
      "Epoch 76/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5187 - accuracy: 0.7223 - val_loss: 0.5119 - val_accuracy: 0.7266 - lr: 2.0000e-04\n",
      "Epoch 77/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5186 - accuracy: 0.7226 - val_loss: 0.5118 - val_accuracy: 0.7268 - lr: 2.0000e-04\n",
      "Epoch 78/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5187 - accuracy: 0.7216 - val_loss: 0.5118 - val_accuracy: 0.7257 - lr: 2.0000e-04\n",
      "Epoch 79/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5185 - accuracy: 0.7228 - val_loss: 0.5119 - val_accuracy: 0.7261 - lr: 2.0000e-04\n",
      "Epoch 80/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5190 - accuracy: 0.7218 - val_loss: 0.5119 - val_accuracy: 0.7266 - lr: 2.0000e-04\n",
      "Epoch 81/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5191 - accuracy: 0.7221 - val_loss: 0.5119 - val_accuracy: 0.7268 - lr: 2.0000e-04\n",
      "Epoch 82/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5190 - accuracy: 0.7216 - val_loss: 0.5120 - val_accuracy: 0.7269 - lr: 2.0000e-04\n",
      "Epoch 83/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5192 - accuracy: 0.7217 - val_loss: 0.5120 - val_accuracy: 0.7268 - lr: 2.0000e-04\n",
      "Epoch 84/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5184 - accuracy: 0.7220 - val_loss: 0.5121 - val_accuracy: 0.7262 - lr: 2.0000e-04\n",
      "Epoch 85/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5196 - accuracy: 0.7208 - val_loss: 0.5121 - val_accuracy: 0.7265 - lr: 4.0000e-05\n",
      "Epoch 86/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5193 - accuracy: 0.7222 - val_loss: 0.5121 - val_accuracy: 0.7261 - lr: 4.0000e-05\n",
      "Epoch 87/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5184 - accuracy: 0.7232 - val_loss: 0.5120 - val_accuracy: 0.7261 - lr: 4.0000e-05\n",
      "Epoch 88/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5182 - accuracy: 0.7226 - val_loss: 0.5120 - val_accuracy: 0.7263 - lr: 4.0000e-05\n",
      "Epoch 89/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5188 - accuracy: 0.7228 - val_loss: 0.5120 - val_accuracy: 0.7264 - lr: 4.0000e-05\n",
      "Epoch 90/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5187 - accuracy: 0.7226 - val_loss: 0.5119 - val_accuracy: 0.7261 - lr: 4.0000e-05\n",
      "Epoch 91/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5186 - accuracy: 0.7228 - val_loss: 0.5121 - val_accuracy: 0.7261 - lr: 4.0000e-05\n",
      "Epoch 92/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5188 - accuracy: 0.7230 - val_loss: 0.5119 - val_accuracy: 0.7264 - lr: 8.0000e-06\n",
      "Epoch 93/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5184 - accuracy: 0.7230 - val_loss: 0.5119 - val_accuracy: 0.7261 - lr: 8.0000e-06\n",
      "Epoch 94/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5187 - accuracy: 0.7225 - val_loss: 0.5119 - val_accuracy: 0.7260 - lr: 8.0000e-06\n",
      "Epoch 95/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5181 - accuracy: 0.7233 - val_loss: 0.5119 - val_accuracy: 0.7261 - lr: 8.0000e-06\n",
      "Epoch 96/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5181 - accuracy: 0.7234 - val_loss: 0.5119 - val_accuracy: 0.7264 - lr: 8.0000e-06\n",
      "Epoch 97/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5185 - accuracy: 0.7211 - val_loss: 0.5120 - val_accuracy: 0.7261 - lr: 8.0000e-06\n",
      "Epoch 98/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5192 - accuracy: 0.7229 - val_loss: 0.5120 - val_accuracy: 0.7263 - lr: 8.0000e-06\n",
      "1250/1250 [==============================] - 0s 341us/step\n",
      "Fold 2: Accuracy = 0.72565\n",
      "625/625 [==============================] - 0s 499us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "313/313 [==============================] - 2s 3ms/step - loss: 0.5845 - accuracy: 0.6816 - val_loss: 0.5591 - val_accuracy: 0.6941 - lr: 0.0050\n",
      "Epoch 2/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5601 - accuracy: 0.6928 - val_loss: 0.5469 - val_accuracy: 0.6975 - lr: 0.0050\n",
      "Epoch 3/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5521 - accuracy: 0.6953 - val_loss: 0.5423 - val_accuracy: 0.6983 - lr: 0.0050\n",
      "Epoch 4/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5477 - accuracy: 0.6987 - val_loss: 0.5364 - val_accuracy: 0.7017 - lr: 0.0050\n",
      "Epoch 5/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5441 - accuracy: 0.7007 - val_loss: 0.5362 - val_accuracy: 0.7033 - lr: 0.0050\n",
      "Epoch 6/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5419 - accuracy: 0.7037 - val_loss: 0.5337 - val_accuracy: 0.7068 - lr: 0.0050\n",
      "Epoch 7/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5399 - accuracy: 0.7046 - val_loss: 0.5317 - val_accuracy: 0.7078 - lr: 0.0050\n",
      "Epoch 8/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5389 - accuracy: 0.7048 - val_loss: 0.5292 - val_accuracy: 0.7085 - lr: 0.0050\n",
      "Epoch 9/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5372 - accuracy: 0.7064 - val_loss: 0.5283 - val_accuracy: 0.7098 - lr: 0.0050\n",
      "Epoch 10/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5359 - accuracy: 0.7078 - val_loss: 0.5263 - val_accuracy: 0.7107 - lr: 0.0050\n",
      "Epoch 11/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5355 - accuracy: 0.7079 - val_loss: 0.5268 - val_accuracy: 0.7113 - lr: 0.0050\n",
      "Epoch 12/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5353 - accuracy: 0.7075 - val_loss: 0.5270 - val_accuracy: 0.7127 - lr: 0.0050\n",
      "Epoch 13/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5334 - accuracy: 0.7100 - val_loss: 0.5255 - val_accuracy: 0.7129 - lr: 0.0050\n",
      "Epoch 14/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5331 - accuracy: 0.7087 - val_loss: 0.5256 - val_accuracy: 0.7137 - lr: 0.0050\n",
      "Epoch 15/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5322 - accuracy: 0.7103 - val_loss: 0.5251 - val_accuracy: 0.7139 - lr: 0.0050\n",
      "Epoch 16/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5321 - accuracy: 0.7098 - val_loss: 0.5268 - val_accuracy: 0.7132 - lr: 0.0050\n",
      "Epoch 17/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5317 - accuracy: 0.7103 - val_loss: 0.5242 - val_accuracy: 0.7137 - lr: 0.0050\n",
      "Epoch 18/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5311 - accuracy: 0.7114 - val_loss: 0.5227 - val_accuracy: 0.7143 - lr: 0.0050\n",
      "Epoch 19/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5306 - accuracy: 0.7128 - val_loss: 0.5226 - val_accuracy: 0.7163 - lr: 0.0050\n",
      "Epoch 20/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5294 - accuracy: 0.7125 - val_loss: 0.5229 - val_accuracy: 0.7168 - lr: 0.0050\n",
      "Epoch 21/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5297 - accuracy: 0.7115 - val_loss: 0.5252 - val_accuracy: 0.7152 - lr: 0.0050\n",
      "Epoch 22/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5294 - accuracy: 0.7136 - val_loss: 0.5218 - val_accuracy: 0.7167 - lr: 0.0050\n",
      "Epoch 23/200\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.5280 - accuracy: 0.7144 - val_loss: 0.5223 - val_accuracy: 0.7167 - lr: 0.0050\n",
      "Epoch 24/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5289 - accuracy: 0.7132 - val_loss: 0.5223 - val_accuracy: 0.7162 - lr: 0.0050\n",
      "Epoch 25/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5284 - accuracy: 0.7133 - val_loss: 0.5224 - val_accuracy: 0.7174 - lr: 0.0050\n",
      "Epoch 26/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5284 - accuracy: 0.7147 - val_loss: 0.5221 - val_accuracy: 0.7174 - lr: 0.0050\n",
      "Epoch 27/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5278 - accuracy: 0.7137 - val_loss: 0.5223 - val_accuracy: 0.7175 - lr: 0.0050\n",
      "Epoch 28/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5280 - accuracy: 0.7141 - val_loss: 0.5219 - val_accuracy: 0.7152 - lr: 0.0050\n",
      "Epoch 29/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5270 - accuracy: 0.7154 - val_loss: 0.5210 - val_accuracy: 0.7189 - lr: 0.0050\n",
      "Epoch 30/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5272 - accuracy: 0.7138 - val_loss: 0.5215 - val_accuracy: 0.7173 - lr: 0.0050\n",
      "Epoch 31/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5267 - accuracy: 0.7147 - val_loss: 0.5230 - val_accuracy: 0.7173 - lr: 0.0050\n",
      "Epoch 32/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5262 - accuracy: 0.7157 - val_loss: 0.5221 - val_accuracy: 0.7187 - lr: 0.0050\n",
      "Epoch 33/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5269 - accuracy: 0.7151 - val_loss: 0.5194 - val_accuracy: 0.7185 - lr: 0.0050\n",
      "Epoch 34/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5261 - accuracy: 0.7148 - val_loss: 0.5192 - val_accuracy: 0.7207 - lr: 0.0050\n",
      "Epoch 35/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5268 - accuracy: 0.7149 - val_loss: 0.5192 - val_accuracy: 0.7193 - lr: 0.0050\n",
      "Epoch 36/200\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.5263 - accuracy: 0.7153 - val_loss: 0.5187 - val_accuracy: 0.7193 - lr: 0.0050\n",
      "Epoch 37/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5251 - accuracy: 0.7159 - val_loss: 0.5208 - val_accuracy: 0.7193 - lr: 0.0050\n",
      "Epoch 38/200\n",
      "313/313 [==============================] - 1s 5ms/step - loss: 0.5257 - accuracy: 0.7149 - val_loss: 0.5198 - val_accuracy: 0.7199 - lr: 0.0050\n",
      "Epoch 39/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5251 - accuracy: 0.7171 - val_loss: 0.5198 - val_accuracy: 0.7189 - lr: 0.0050\n",
      "Epoch 40/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5254 - accuracy: 0.7166 - val_loss: 0.5188 - val_accuracy: 0.7196 - lr: 0.0050\n",
      "Epoch 41/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5250 - accuracy: 0.7163 - val_loss: 0.5181 - val_accuracy: 0.7206 - lr: 0.0050\n",
      "Epoch 42/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5249 - accuracy: 0.7160 - val_loss: 0.5183 - val_accuracy: 0.7202 - lr: 0.0050\n",
      "Epoch 43/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5251 - accuracy: 0.7152 - val_loss: 0.5193 - val_accuracy: 0.7200 - lr: 0.0050\n",
      "Epoch 44/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5247 - accuracy: 0.7173 - val_loss: 0.5188 - val_accuracy: 0.7208 - lr: 0.0050\n",
      "Epoch 45/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5246 - accuracy: 0.7159 - val_loss: 0.5187 - val_accuracy: 0.7210 - lr: 0.0050\n",
      "Epoch 46/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5240 - accuracy: 0.7163 - val_loss: 0.5190 - val_accuracy: 0.7196 - lr: 0.0050\n",
      "Epoch 47/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5238 - accuracy: 0.7171 - val_loss: 0.5177 - val_accuracy: 0.7209 - lr: 0.0050\n",
      "Epoch 48/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5233 - accuracy: 0.7171 - val_loss: 0.5198 - val_accuracy: 0.7217 - lr: 0.0050\n",
      "Epoch 49/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5245 - accuracy: 0.7181 - val_loss: 0.5186 - val_accuracy: 0.7200 - lr: 0.0050\n",
      "Epoch 50/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5237 - accuracy: 0.7186 - val_loss: 0.5194 - val_accuracy: 0.7209 - lr: 0.0050\n",
      "Epoch 51/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5237 - accuracy: 0.7189 - val_loss: 0.5208 - val_accuracy: 0.7175 - lr: 0.0050\n",
      "Epoch 52/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5235 - accuracy: 0.7182 - val_loss: 0.5185 - val_accuracy: 0.7211 - lr: 0.0050\n",
      "Epoch 53/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5229 - accuracy: 0.7181 - val_loss: 0.5186 - val_accuracy: 0.7213 - lr: 0.0050\n",
      "Epoch 54/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5225 - accuracy: 0.7184 - val_loss: 0.5176 - val_accuracy: 0.7222 - lr: 0.0050\n",
      "Epoch 55/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5199 - accuracy: 0.7211 - val_loss: 0.5158 - val_accuracy: 0.7218 - lr: 1.0000e-03\n",
      "Epoch 56/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5193 - accuracy: 0.7211 - val_loss: 0.5151 - val_accuracy: 0.7223 - lr: 1.0000e-03\n",
      "Epoch 57/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5196 - accuracy: 0.7207 - val_loss: 0.5156 - val_accuracy: 0.7221 - lr: 1.0000e-03\n",
      "Epoch 58/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5194 - accuracy: 0.7211 - val_loss: 0.5147 - val_accuracy: 0.7236 - lr: 1.0000e-03\n",
      "Epoch 59/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5196 - accuracy: 0.7210 - val_loss: 0.5151 - val_accuracy: 0.7228 - lr: 1.0000e-03\n",
      "Epoch 60/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5186 - accuracy: 0.7221 - val_loss: 0.5152 - val_accuracy: 0.7222 - lr: 1.0000e-03\n",
      "Epoch 61/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5185 - accuracy: 0.7215 - val_loss: 0.5149 - val_accuracy: 0.7225 - lr: 1.0000e-03\n",
      "Epoch 62/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5195 - accuracy: 0.7216 - val_loss: 0.5148 - val_accuracy: 0.7240 - lr: 1.0000e-03\n",
      "Epoch 63/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5182 - accuracy: 0.7223 - val_loss: 0.5154 - val_accuracy: 0.7227 - lr: 1.0000e-03\n",
      "Epoch 64/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5181 - accuracy: 0.7220 - val_loss: 0.5154 - val_accuracy: 0.7235 - lr: 1.0000e-03\n",
      "Epoch 65/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5181 - accuracy: 0.7222 - val_loss: 0.5147 - val_accuracy: 0.7226 - lr: 1.0000e-03\n",
      "Epoch 66/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5176 - accuracy: 0.7220 - val_loss: 0.5147 - val_accuracy: 0.7227 - lr: 2.0000e-04\n",
      "Epoch 67/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5174 - accuracy: 0.7231 - val_loss: 0.5147 - val_accuracy: 0.7226 - lr: 2.0000e-04\n",
      "Epoch 68/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5178 - accuracy: 0.7226 - val_loss: 0.5147 - val_accuracy: 0.7230 - lr: 2.0000e-04\n",
      "Epoch 69/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5170 - accuracy: 0.7232 - val_loss: 0.5147 - val_accuracy: 0.7227 - lr: 2.0000e-04\n",
      "Epoch 70/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5175 - accuracy: 0.7223 - val_loss: 0.5145 - val_accuracy: 0.7237 - lr: 2.0000e-04\n",
      "Epoch 71/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5179 - accuracy: 0.7223 - val_loss: 0.5144 - val_accuracy: 0.7236 - lr: 2.0000e-04\n",
      "Epoch 72/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5169 - accuracy: 0.7226 - val_loss: 0.5144 - val_accuracy: 0.7232 - lr: 2.0000e-04\n",
      "Epoch 73/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5174 - accuracy: 0.7233 - val_loss: 0.5144 - val_accuracy: 0.7233 - lr: 2.0000e-04\n",
      "Epoch 74/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5174 - accuracy: 0.7223 - val_loss: 0.5144 - val_accuracy: 0.7228 - lr: 2.0000e-04\n",
      "Epoch 75/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5169 - accuracy: 0.7231 - val_loss: 0.5143 - val_accuracy: 0.7233 - lr: 2.0000e-04\n",
      "Epoch 76/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5172 - accuracy: 0.7238 - val_loss: 0.5145 - val_accuracy: 0.7233 - lr: 2.0000e-04\n",
      "Epoch 77/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5167 - accuracy: 0.7229 - val_loss: 0.5145 - val_accuracy: 0.7232 - lr: 2.0000e-04\n",
      "Epoch 78/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5176 - accuracy: 0.7225 - val_loss: 0.5149 - val_accuracy: 0.7230 - lr: 2.0000e-04\n",
      "Epoch 79/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5172 - accuracy: 0.7224 - val_loss: 0.5143 - val_accuracy: 0.7233 - lr: 4.0000e-05\n",
      "Epoch 80/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5169 - accuracy: 0.7237 - val_loss: 0.5142 - val_accuracy: 0.7230 - lr: 4.0000e-05\n",
      "Epoch 81/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5169 - accuracy: 0.7242 - val_loss: 0.5144 - val_accuracy: 0.7232 - lr: 4.0000e-05\n",
      "Epoch 82/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5171 - accuracy: 0.7226 - val_loss: 0.5145 - val_accuracy: 0.7228 - lr: 4.0000e-05\n",
      "Epoch 83/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5168 - accuracy: 0.7234 - val_loss: 0.5142 - val_accuracy: 0.7230 - lr: 4.0000e-05\n",
      "Epoch 84/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5163 - accuracy: 0.7237 - val_loss: 0.5142 - val_accuracy: 0.7233 - lr: 4.0000e-05\n",
      "Epoch 85/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5162 - accuracy: 0.7246 - val_loss: 0.5145 - val_accuracy: 0.7231 - lr: 4.0000e-05\n",
      "Epoch 86/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5170 - accuracy: 0.7240 - val_loss: 0.5143 - val_accuracy: 0.7232 - lr: 4.0000e-05\n",
      "Epoch 87/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5169 - accuracy: 0.7237 - val_loss: 0.5142 - val_accuracy: 0.7233 - lr: 4.0000e-05\n",
      "Epoch 88/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5166 - accuracy: 0.7235 - val_loss: 0.5144 - val_accuracy: 0.7228 - lr: 8.0000e-06\n",
      "Epoch 89/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5177 - accuracy: 0.7221 - val_loss: 0.5142 - val_accuracy: 0.7231 - lr: 8.0000e-06\n",
      "Epoch 90/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5172 - accuracy: 0.7234 - val_loss: 0.5144 - val_accuracy: 0.7234 - lr: 8.0000e-06\n",
      "Epoch 91/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5169 - accuracy: 0.7226 - val_loss: 0.5143 - val_accuracy: 0.7231 - lr: 8.0000e-06\n",
      "Epoch 92/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5173 - accuracy: 0.7219 - val_loss: 0.5142 - val_accuracy: 0.7235 - lr: 8.0000e-06\n",
      "Epoch 93/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5164 - accuracy: 0.7229 - val_loss: 0.5143 - val_accuracy: 0.7232 - lr: 8.0000e-06\n",
      "Epoch 94/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5165 - accuracy: 0.7229 - val_loss: 0.5142 - val_accuracy: 0.7234 - lr: 8.0000e-06\n",
      "Epoch 95/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5171 - accuracy: 0.7226 - val_loss: 0.5143 - val_accuracy: 0.7234 - lr: 1.6000e-06\n",
      "Epoch 96/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5167 - accuracy: 0.7242 - val_loss: 0.5142 - val_accuracy: 0.7235 - lr: 1.6000e-06\n",
      "Epoch 97/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5171 - accuracy: 0.7237 - val_loss: 0.5143 - val_accuracy: 0.7232 - lr: 1.6000e-06\n",
      "Epoch 98/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5166 - accuracy: 0.7230 - val_loss: 0.5143 - val_accuracy: 0.7230 - lr: 1.6000e-06\n",
      "Epoch 99/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5176 - accuracy: 0.7232 - val_loss: 0.5143 - val_accuracy: 0.7233 - lr: 1.6000e-06\n",
      "Epoch 100/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5167 - accuracy: 0.7238 - val_loss: 0.5143 - val_accuracy: 0.7233 - lr: 1.6000e-06\n",
      "Epoch 101/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5162 - accuracy: 0.7230 - val_loss: 0.5143 - val_accuracy: 0.7233 - lr: 1.6000e-06\n",
      "Epoch 102/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5172 - accuracy: 0.7222 - val_loss: 0.5143 - val_accuracy: 0.7231 - lr: 3.2000e-07\n",
      "Epoch 103/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5163 - accuracy: 0.7239 - val_loss: 0.5145 - val_accuracy: 0.7232 - lr: 3.2000e-07\n",
      "Epoch 104/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5170 - accuracy: 0.7231 - val_loss: 0.5142 - val_accuracy: 0.7232 - lr: 3.2000e-07\n",
      "Epoch 105/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5171 - accuracy: 0.7218 - val_loss: 0.5142 - val_accuracy: 0.7235 - lr: 3.2000e-07\n",
      "Epoch 106/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5174 - accuracy: 0.7225 - val_loss: 0.5144 - val_accuracy: 0.7231 - lr: 3.2000e-07\n",
      "Epoch 107/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5166 - accuracy: 0.7233 - val_loss: 0.5144 - val_accuracy: 0.7229 - lr: 3.2000e-07\n",
      "Epoch 108/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5161 - accuracy: 0.7241 - val_loss: 0.5145 - val_accuracy: 0.7228 - lr: 3.2000e-07\n",
      "Epoch 109/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5169 - accuracy: 0.7229 - val_loss: 0.5141 - val_accuracy: 0.7235 - lr: 6.4000e-08\n",
      "Epoch 110/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5171 - accuracy: 0.7230 - val_loss: 0.5142 - val_accuracy: 0.7232 - lr: 6.4000e-08\n",
      "Epoch 111/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5161 - accuracy: 0.7241 - val_loss: 0.5143 - val_accuracy: 0.7233 - lr: 6.4000e-08\n",
      "Epoch 112/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5172 - accuracy: 0.7227 - val_loss: 0.5143 - val_accuracy: 0.7231 - lr: 6.4000e-08\n",
      "Epoch 113/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5168 - accuracy: 0.7227 - val_loss: 0.5142 - val_accuracy: 0.7236 - lr: 6.4000e-08\n",
      "Epoch 114/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5172 - accuracy: 0.7221 - val_loss: 0.5142 - val_accuracy: 0.7235 - lr: 6.4000e-08\n",
      "Epoch 115/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5169 - accuracy: 0.7222 - val_loss: 0.5143 - val_accuracy: 0.7232 - lr: 6.4000e-08\n",
      "Epoch 116/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5170 - accuracy: 0.7235 - val_loss: 0.5142 - val_accuracy: 0.7236 - lr: 1.2800e-08\n",
      "Epoch 117/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5171 - accuracy: 0.7231 - val_loss: 0.5142 - val_accuracy: 0.7233 - lr: 1.2800e-08\n",
      "Epoch 118/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5168 - accuracy: 0.7223 - val_loss: 0.5142 - val_accuracy: 0.7232 - lr: 1.2800e-08\n",
      "Epoch 119/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5169 - accuracy: 0.7237 - val_loss: 0.5144 - val_accuracy: 0.7236 - lr: 1.2800e-08\n",
      "Epoch 120/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5168 - accuracy: 0.7231 - val_loss: 0.5143 - val_accuracy: 0.7232 - lr: 1.2800e-08\n",
      "Epoch 121/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5171 - accuracy: 0.7238 - val_loss: 0.5143 - val_accuracy: 0.7234 - lr: 1.2800e-08\n",
      "Epoch 122/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5168 - accuracy: 0.7224 - val_loss: 0.5142 - val_accuracy: 0.7236 - lr: 1.2800e-08\n",
      "Epoch 123/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5169 - accuracy: 0.7230 - val_loss: 0.5142 - val_accuracy: 0.7234 - lr: 2.5600e-09\n",
      "Epoch 124/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5166 - accuracy: 0.7232 - val_loss: 0.5142 - val_accuracy: 0.7233 - lr: 2.5600e-09\n",
      "Epoch 125/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5167 - accuracy: 0.7233 - val_loss: 0.5144 - val_accuracy: 0.7230 - lr: 2.5600e-09\n",
      "Epoch 126/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5173 - accuracy: 0.7231 - val_loss: 0.5142 - val_accuracy: 0.7235 - lr: 2.5600e-09\n",
      "Epoch 127/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5178 - accuracy: 0.7225 - val_loss: 0.5142 - val_accuracy: 0.7232 - lr: 2.5600e-09\n",
      "Epoch 128/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5168 - accuracy: 0.7234 - val_loss: 0.5145 - val_accuracy: 0.7232 - lr: 2.5600e-09\n",
      "Epoch 129/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5170 - accuracy: 0.7232 - val_loss: 0.5142 - val_accuracy: 0.7236 - lr: 2.5600e-09\n",
      "1250/1250 [==============================] - 2s 1ms/step\n",
      "Fold 3: Accuracy = 0.72347\n",
      "625/625 [==============================] - 0s 538us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5856 - accuracy: 0.6825 - val_loss: 0.5516 - val_accuracy: 0.6967 - lr: 0.0050\n",
      "Epoch 2/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5571 - accuracy: 0.6933 - val_loss: 0.5446 - val_accuracy: 0.6977 - lr: 0.0050\n",
      "Epoch 3/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5509 - accuracy: 0.6964 - val_loss: 0.5411 - val_accuracy: 0.6996 - lr: 0.0050\n",
      "Epoch 4/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5469 - accuracy: 0.6990 - val_loss: 0.5394 - val_accuracy: 0.7008 - lr: 0.0050\n",
      "Epoch 5/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5437 - accuracy: 0.7001 - val_loss: 0.5344 - val_accuracy: 0.7059 - lr: 0.0050\n",
      "Epoch 6/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5415 - accuracy: 0.7032 - val_loss: 0.5320 - val_accuracy: 0.7053 - lr: 0.0050\n",
      "Epoch 7/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5395 - accuracy: 0.7050 - val_loss: 0.5342 - val_accuracy: 0.7069 - lr: 0.0050\n",
      "Epoch 8/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5389 - accuracy: 0.7046 - val_loss: 0.5317 - val_accuracy: 0.7077 - lr: 0.0050\n",
      "Epoch 9/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5361 - accuracy: 0.7072 - val_loss: 0.5277 - val_accuracy: 0.7111 - lr: 0.0050\n",
      "Epoch 10/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5356 - accuracy: 0.7077 - val_loss: 0.5276 - val_accuracy: 0.7103 - lr: 0.0050\n",
      "Epoch 11/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5347 - accuracy: 0.7080 - val_loss: 0.5290 - val_accuracy: 0.7114 - lr: 0.0050\n",
      "Epoch 12/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5335 - accuracy: 0.7093 - val_loss: 0.5284 - val_accuracy: 0.7119 - lr: 0.0050\n",
      "Epoch 13/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5335 - accuracy: 0.7097 - val_loss: 0.5279 - val_accuracy: 0.7122 - lr: 0.0050\n",
      "Epoch 14/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5322 - accuracy: 0.7106 - val_loss: 0.5247 - val_accuracy: 0.7136 - lr: 0.0050\n",
      "Epoch 15/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5319 - accuracy: 0.7104 - val_loss: 0.5260 - val_accuracy: 0.7137 - lr: 0.0050\n",
      "Epoch 16/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5310 - accuracy: 0.7117 - val_loss: 0.5263 - val_accuracy: 0.7132 - lr: 0.0050\n",
      "Epoch 17/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5314 - accuracy: 0.7103 - val_loss: 0.5239 - val_accuracy: 0.7139 - lr: 0.0050\n",
      "Epoch 18/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5306 - accuracy: 0.7120 - val_loss: 0.5241 - val_accuracy: 0.7154 - lr: 0.0050\n",
      "Epoch 19/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5300 - accuracy: 0.7132 - val_loss: 0.5247 - val_accuracy: 0.7146 - lr: 0.0050\n",
      "Epoch 20/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5291 - accuracy: 0.7139 - val_loss: 0.5245 - val_accuracy: 0.7137 - lr: 0.0050\n",
      "Epoch 21/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5285 - accuracy: 0.7139 - val_loss: 0.5249 - val_accuracy: 0.7139 - lr: 0.0050\n",
      "Epoch 22/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5284 - accuracy: 0.7127 - val_loss: 0.5237 - val_accuracy: 0.7159 - lr: 0.0050\n",
      "Epoch 23/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5280 - accuracy: 0.7134 - val_loss: 0.5230 - val_accuracy: 0.7176 - lr: 0.0050\n",
      "Epoch 24/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5279 - accuracy: 0.7138 - val_loss: 0.5239 - val_accuracy: 0.7162 - lr: 0.0050\n",
      "Epoch 25/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5277 - accuracy: 0.7133 - val_loss: 0.5216 - val_accuracy: 0.7179 - lr: 0.0050\n",
      "Epoch 26/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5278 - accuracy: 0.7141 - val_loss: 0.5219 - val_accuracy: 0.7175 - lr: 0.0050\n",
      "Epoch 27/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5267 - accuracy: 0.7141 - val_loss: 0.5240 - val_accuracy: 0.7169 - lr: 0.0050\n",
      "Epoch 28/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5272 - accuracy: 0.7148 - val_loss: 0.5205 - val_accuracy: 0.7182 - lr: 0.0050\n",
      "Epoch 29/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5268 - accuracy: 0.7138 - val_loss: 0.5271 - val_accuracy: 0.7120 - lr: 0.0050\n",
      "Epoch 30/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5270 - accuracy: 0.7161 - val_loss: 0.5211 - val_accuracy: 0.7173 - lr: 0.0050\n",
      "Epoch 31/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5270 - accuracy: 0.7146 - val_loss: 0.5213 - val_accuracy: 0.7182 - lr: 0.0050\n",
      "Epoch 32/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5258 - accuracy: 0.7159 - val_loss: 0.5229 - val_accuracy: 0.7154 - lr: 0.0050\n",
      "Epoch 33/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5259 - accuracy: 0.7154 - val_loss: 0.5223 - val_accuracy: 0.7165 - lr: 0.0050\n",
      "Epoch 34/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5255 - accuracy: 0.7153 - val_loss: 0.5211 - val_accuracy: 0.7173 - lr: 0.0050\n",
      "Epoch 35/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5260 - accuracy: 0.7158 - val_loss: 0.5214 - val_accuracy: 0.7181 - lr: 0.0050\n",
      "Epoch 36/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5222 - accuracy: 0.7177 - val_loss: 0.5198 - val_accuracy: 0.7194 - lr: 1.0000e-03\n",
      "Epoch 37/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5211 - accuracy: 0.7199 - val_loss: 0.5189 - val_accuracy: 0.7202 - lr: 1.0000e-03\n",
      "Epoch 38/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5210 - accuracy: 0.7204 - val_loss: 0.5189 - val_accuracy: 0.7199 - lr: 1.0000e-03\n",
      "Epoch 39/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5212 - accuracy: 0.7196 - val_loss: 0.5187 - val_accuracy: 0.7193 - lr: 1.0000e-03\n",
      "Epoch 40/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5209 - accuracy: 0.7199 - val_loss: 0.5189 - val_accuracy: 0.7214 - lr: 1.0000e-03\n",
      "Epoch 41/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5210 - accuracy: 0.7198 - val_loss: 0.5188 - val_accuracy: 0.7216 - lr: 1.0000e-03\n",
      "Epoch 42/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5202 - accuracy: 0.7206 - val_loss: 0.5186 - val_accuracy: 0.7205 - lr: 1.0000e-03\n",
      "Epoch 43/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5208 - accuracy: 0.7198 - val_loss: 0.5183 - val_accuracy: 0.7212 - lr: 1.0000e-03\n",
      "Epoch 44/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5206 - accuracy: 0.7195 - val_loss: 0.5183 - val_accuracy: 0.7211 - lr: 1.0000e-03\n",
      "Epoch 45/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5203 - accuracy: 0.7199 - val_loss: 0.5187 - val_accuracy: 0.7209 - lr: 1.0000e-03\n",
      "Epoch 46/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5196 - accuracy: 0.7202 - val_loss: 0.5179 - val_accuracy: 0.7205 - lr: 1.0000e-03\n",
      "Epoch 47/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5201 - accuracy: 0.7197 - val_loss: 0.5183 - val_accuracy: 0.7214 - lr: 1.0000e-03\n",
      "Epoch 48/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5196 - accuracy: 0.7214 - val_loss: 0.5182 - val_accuracy: 0.7215 - lr: 1.0000e-03\n",
      "Epoch 49/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5203 - accuracy: 0.7204 - val_loss: 0.5179 - val_accuracy: 0.7219 - lr: 1.0000e-03\n",
      "Epoch 50/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5197 - accuracy: 0.7210 - val_loss: 0.5177 - val_accuracy: 0.7218 - lr: 1.0000e-03\n",
      "Epoch 51/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5197 - accuracy: 0.7210 - val_loss: 0.5178 - val_accuracy: 0.7212 - lr: 1.0000e-03\n",
      "Epoch 52/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5200 - accuracy: 0.7206 - val_loss: 0.5182 - val_accuracy: 0.7216 - lr: 1.0000e-03\n",
      "Epoch 53/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5190 - accuracy: 0.7208 - val_loss: 0.5183 - val_accuracy: 0.7211 - lr: 1.0000e-03\n",
      "Epoch 54/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5196 - accuracy: 0.7201 - val_loss: 0.5176 - val_accuracy: 0.7223 - lr: 1.0000e-03\n",
      "Epoch 55/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5190 - accuracy: 0.7218 - val_loss: 0.5181 - val_accuracy: 0.7214 - lr: 1.0000e-03\n",
      "Epoch 56/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5197 - accuracy: 0.7214 - val_loss: 0.5178 - val_accuracy: 0.7207 - lr: 1.0000e-03\n",
      "Epoch 57/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5187 - accuracy: 0.7217 - val_loss: 0.5177 - val_accuracy: 0.7219 - lr: 1.0000e-03\n",
      "Epoch 58/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5191 - accuracy: 0.7217 - val_loss: 0.5177 - val_accuracy: 0.7217 - lr: 1.0000e-03\n",
      "Epoch 59/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5195 - accuracy: 0.7208 - val_loss: 0.5181 - val_accuracy: 0.7211 - lr: 1.0000e-03\n",
      "Epoch 60/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5186 - accuracy: 0.7213 - val_loss: 0.5175 - val_accuracy: 0.7217 - lr: 1.0000e-03\n",
      "Epoch 61/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5187 - accuracy: 0.7221 - val_loss: 0.5178 - val_accuracy: 0.7216 - lr: 1.0000e-03\n",
      "Epoch 62/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5186 - accuracy: 0.7216 - val_loss: 0.5174 - val_accuracy: 0.7224 - lr: 2.0000e-04\n",
      "Epoch 63/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5187 - accuracy: 0.7215 - val_loss: 0.5175 - val_accuracy: 0.7229 - lr: 2.0000e-04\n",
      "Epoch 64/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5177 - accuracy: 0.7225 - val_loss: 0.5173 - val_accuracy: 0.7224 - lr: 2.0000e-04\n",
      "Epoch 65/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5181 - accuracy: 0.7220 - val_loss: 0.5176 - val_accuracy: 0.7222 - lr: 2.0000e-04\n",
      "Epoch 66/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5184 - accuracy: 0.7223 - val_loss: 0.5175 - val_accuracy: 0.7224 - lr: 2.0000e-04\n",
      "Epoch 67/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5176 - accuracy: 0.7224 - val_loss: 0.5174 - val_accuracy: 0.7215 - lr: 2.0000e-04\n",
      "Epoch 68/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5177 - accuracy: 0.7228 - val_loss: 0.5173 - val_accuracy: 0.7219 - lr: 2.0000e-04\n",
      "Epoch 69/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5181 - accuracy: 0.7219 - val_loss: 0.5174 - val_accuracy: 0.7220 - lr: 2.0000e-04\n",
      "Epoch 70/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5173 - accuracy: 0.7231 - val_loss: 0.5174 - val_accuracy: 0.7223 - lr: 2.0000e-04\n",
      "Epoch 71/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5175 - accuracy: 0.7224 - val_loss: 0.5174 - val_accuracy: 0.7229 - lr: 2.0000e-04\n",
      "Epoch 72/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5180 - accuracy: 0.7223 - val_loss: 0.5173 - val_accuracy: 0.7228 - lr: 4.0000e-05\n",
      "Epoch 73/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5175 - accuracy: 0.7229 - val_loss: 0.5173 - val_accuracy: 0.7228 - lr: 4.0000e-05\n",
      "Epoch 74/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5176 - accuracy: 0.7218 - val_loss: 0.5173 - val_accuracy: 0.7224 - lr: 4.0000e-05\n",
      "Epoch 75/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5167 - accuracy: 0.7227 - val_loss: 0.5172 - val_accuracy: 0.7224 - lr: 4.0000e-05\n",
      "Epoch 76/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5175 - accuracy: 0.7229 - val_loss: 0.5173 - val_accuracy: 0.7229 - lr: 4.0000e-05\n",
      "Epoch 77/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5177 - accuracy: 0.7224 - val_loss: 0.5174 - val_accuracy: 0.7229 - lr: 4.0000e-05\n",
      "Epoch 78/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5183 - accuracy: 0.7219 - val_loss: 0.5173 - val_accuracy: 0.7225 - lr: 4.0000e-05\n",
      "Epoch 79/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5179 - accuracy: 0.7234 - val_loss: 0.5173 - val_accuracy: 0.7225 - lr: 8.0000e-06\n",
      "Epoch 80/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5185 - accuracy: 0.7220 - val_loss: 0.5173 - val_accuracy: 0.7223 - lr: 8.0000e-06\n",
      "Epoch 81/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5177 - accuracy: 0.7217 - val_loss: 0.5173 - val_accuracy: 0.7224 - lr: 8.0000e-06\n",
      "Epoch 82/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5181 - accuracy: 0.7216 - val_loss: 0.5173 - val_accuracy: 0.7222 - lr: 8.0000e-06\n",
      "Epoch 83/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5173 - accuracy: 0.7225 - val_loss: 0.5173 - val_accuracy: 0.7228 - lr: 8.0000e-06\n",
      "Epoch 84/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5175 - accuracy: 0.7218 - val_loss: 0.5174 - val_accuracy: 0.7228 - lr: 8.0000e-06\n",
      "Epoch 85/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5181 - accuracy: 0.7228 - val_loss: 0.5173 - val_accuracy: 0.7230 - lr: 8.0000e-06\n",
      "Epoch 86/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5175 - accuracy: 0.7220 - val_loss: 0.5172 - val_accuracy: 0.7224 - lr: 1.6000e-06\n",
      "Epoch 87/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5170 - accuracy: 0.7227 - val_loss: 0.5173 - val_accuracy: 0.7226 - lr: 1.6000e-06\n",
      "Epoch 88/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5175 - accuracy: 0.7237 - val_loss: 0.5174 - val_accuracy: 0.7227 - lr: 1.6000e-06\n",
      "Epoch 89/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5173 - accuracy: 0.7232 - val_loss: 0.5173 - val_accuracy: 0.7226 - lr: 1.6000e-06\n",
      "Epoch 90/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5176 - accuracy: 0.7230 - val_loss: 0.5173 - val_accuracy: 0.7227 - lr: 1.6000e-06\n",
      "Epoch 91/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5179 - accuracy: 0.7230 - val_loss: 0.5173 - val_accuracy: 0.7228 - lr: 1.6000e-06\n",
      "Epoch 92/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5174 - accuracy: 0.7217 - val_loss: 0.5174 - val_accuracy: 0.7229 - lr: 1.6000e-06\n",
      "Epoch 93/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5166 - accuracy: 0.7233 - val_loss: 0.5173 - val_accuracy: 0.7225 - lr: 3.2000e-07\n",
      "Epoch 94/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5177 - accuracy: 0.7214 - val_loss: 0.5173 - val_accuracy: 0.7228 - lr: 3.2000e-07\n",
      "Epoch 95/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5169 - accuracy: 0.7230 - val_loss: 0.5173 - val_accuracy: 0.7228 - lr: 3.2000e-07\n",
      "Epoch 96/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5182 - accuracy: 0.7212 - val_loss: 0.5173 - val_accuracy: 0.7232 - lr: 3.2000e-07\n",
      "Epoch 97/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5180 - accuracy: 0.7221 - val_loss: 0.5174 - val_accuracy: 0.7227 - lr: 3.2000e-07\n",
      "Epoch 98/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5173 - accuracy: 0.7224 - val_loss: 0.5173 - val_accuracy: 0.7224 - lr: 3.2000e-07\n",
      "Epoch 99/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5175 - accuracy: 0.7227 - val_loss: 0.5173 - val_accuracy: 0.7229 - lr: 3.2000e-07\n",
      "Epoch 100/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5171 - accuracy: 0.7228 - val_loss: 0.5173 - val_accuracy: 0.7230 - lr: 6.4000e-08\n",
      "Epoch 101/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5175 - accuracy: 0.7236 - val_loss: 0.5173 - val_accuracy: 0.7227 - lr: 6.4000e-08\n",
      "Epoch 102/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5179 - accuracy: 0.7220 - val_loss: 0.5173 - val_accuracy: 0.7228 - lr: 6.4000e-08\n",
      "Epoch 103/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5176 - accuracy: 0.7219 - val_loss: 0.5172 - val_accuracy: 0.7225 - lr: 6.4000e-08\n",
      "Epoch 104/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5179 - accuracy: 0.7214 - val_loss: 0.5174 - val_accuracy: 0.7230 - lr: 6.4000e-08\n",
      "Epoch 105/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5179 - accuracy: 0.7228 - val_loss: 0.5174 - val_accuracy: 0.7229 - lr: 6.4000e-08\n",
      "Epoch 106/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5177 - accuracy: 0.7220 - val_loss: 0.5173 - val_accuracy: 0.7227 - lr: 6.4000e-08\n",
      "1250/1250 [==============================] - 0s 305us/step\n",
      "Fold 4: Accuracy = 0.72240\n",
      "625/625 [==============================] - 0s 312us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5866 - accuracy: 0.6823 - val_loss: 0.5596 - val_accuracy: 0.6946 - lr: 0.0050\n",
      "Epoch 2/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5584 - accuracy: 0.6927 - val_loss: 0.5496 - val_accuracy: 0.6998 - lr: 0.0050\n",
      "Epoch 3/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5511 - accuracy: 0.6968 - val_loss: 0.5434 - val_accuracy: 0.7044 - lr: 0.0050\n",
      "Epoch 4/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5460 - accuracy: 0.6995 - val_loss: 0.5378 - val_accuracy: 0.7080 - lr: 0.0050\n",
      "Epoch 5/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5434 - accuracy: 0.7021 - val_loss: 0.5383 - val_accuracy: 0.7067 - lr: 0.0050\n",
      "Epoch 6/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5409 - accuracy: 0.7034 - val_loss: 0.5354 - val_accuracy: 0.7091 - lr: 0.0050\n",
      "Epoch 7/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5391 - accuracy: 0.7055 - val_loss: 0.5348 - val_accuracy: 0.7093 - lr: 0.0050\n",
      "Epoch 8/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5382 - accuracy: 0.7055 - val_loss: 0.5326 - val_accuracy: 0.7103 - lr: 0.0050\n",
      "Epoch 9/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5367 - accuracy: 0.7065 - val_loss: 0.5306 - val_accuracy: 0.7107 - lr: 0.0050\n",
      "Epoch 10/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5363 - accuracy: 0.7087 - val_loss: 0.5313 - val_accuracy: 0.7109 - lr: 0.0050\n",
      "Epoch 11/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5353 - accuracy: 0.7090 - val_loss: 0.5291 - val_accuracy: 0.7127 - lr: 0.0050\n",
      "Epoch 12/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5341 - accuracy: 0.7093 - val_loss: 0.5308 - val_accuracy: 0.7120 - lr: 0.0050\n",
      "Epoch 13/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5332 - accuracy: 0.7105 - val_loss: 0.5303 - val_accuracy: 0.7105 - lr: 0.0050\n",
      "Epoch 14/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5330 - accuracy: 0.7103 - val_loss: 0.5295 - val_accuracy: 0.7139 - lr: 0.0050\n",
      "Epoch 15/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5325 - accuracy: 0.7109 - val_loss: 0.5273 - val_accuracy: 0.7147 - lr: 0.0050\n",
      "Epoch 16/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5309 - accuracy: 0.7108 - val_loss: 0.5275 - val_accuracy: 0.7146 - lr: 0.0050\n",
      "Epoch 17/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5314 - accuracy: 0.7113 - val_loss: 0.5271 - val_accuracy: 0.7155 - lr: 0.0050\n",
      "Epoch 18/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5313 - accuracy: 0.7122 - val_loss: 0.5279 - val_accuracy: 0.7135 - lr: 0.0050\n",
      "Epoch 19/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5301 - accuracy: 0.7113 - val_loss: 0.5254 - val_accuracy: 0.7131 - lr: 0.0050\n",
      "Epoch 20/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5308 - accuracy: 0.7110 - val_loss: 0.5285 - val_accuracy: 0.7142 - lr: 0.0050\n",
      "Epoch 21/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5296 - accuracy: 0.7142 - val_loss: 0.5259 - val_accuracy: 0.7147 - lr: 0.0050\n",
      "Epoch 22/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5297 - accuracy: 0.7127 - val_loss: 0.5269 - val_accuracy: 0.7131 - lr: 0.0050\n",
      "Epoch 23/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5291 - accuracy: 0.7137 - val_loss: 0.5263 - val_accuracy: 0.7156 - lr: 0.0050\n",
      "Epoch 24/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5284 - accuracy: 0.7148 - val_loss: 0.5254 - val_accuracy: 0.7166 - lr: 0.0050\n",
      "Epoch 25/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5286 - accuracy: 0.7132 - val_loss: 0.5246 - val_accuracy: 0.7165 - lr: 0.0050\n",
      "Epoch 26/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5278 - accuracy: 0.7140 - val_loss: 0.5245 - val_accuracy: 0.7162 - lr: 0.0050\n",
      "Epoch 27/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5277 - accuracy: 0.7147 - val_loss: 0.5246 - val_accuracy: 0.7163 - lr: 0.0050\n",
      "Epoch 28/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5270 - accuracy: 0.7144 - val_loss: 0.5250 - val_accuracy: 0.7175 - lr: 0.0050\n",
      "Epoch 29/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5278 - accuracy: 0.7146 - val_loss: 0.5249 - val_accuracy: 0.7170 - lr: 0.0050\n",
      "Epoch 30/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5261 - accuracy: 0.7148 - val_loss: 0.5254 - val_accuracy: 0.7146 - lr: 0.0050\n",
      "Epoch 31/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5265 - accuracy: 0.7142 - val_loss: 0.5243 - val_accuracy: 0.7162 - lr: 0.0050\n",
      "Epoch 32/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5255 - accuracy: 0.7163 - val_loss: 0.5238 - val_accuracy: 0.7171 - lr: 0.0050\n",
      "Epoch 33/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5263 - accuracy: 0.7136 - val_loss: 0.5236 - val_accuracy: 0.7168 - lr: 0.0050\n",
      "Epoch 34/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5248 - accuracy: 0.7162 - val_loss: 0.5245 - val_accuracy: 0.7177 - lr: 0.0050\n",
      "Epoch 35/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5257 - accuracy: 0.7151 - val_loss: 0.5240 - val_accuracy: 0.7177 - lr: 0.0050\n",
      "Epoch 36/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5256 - accuracy: 0.7158 - val_loss: 0.5238 - val_accuracy: 0.7194 - lr: 0.0050\n",
      "Epoch 37/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5262 - accuracy: 0.7162 - val_loss: 0.5259 - val_accuracy: 0.7137 - lr: 0.0050\n",
      "Epoch 38/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5254 - accuracy: 0.7163 - val_loss: 0.5251 - val_accuracy: 0.7162 - lr: 0.0050\n",
      "Epoch 39/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5243 - accuracy: 0.7165 - val_loss: 0.5239 - val_accuracy: 0.7159 - lr: 0.0050\n",
      "Epoch 40/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5253 - accuracy: 0.7166 - val_loss: 0.5236 - val_accuracy: 0.7199 - lr: 0.0050\n",
      "Epoch 41/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5220 - accuracy: 0.7182 - val_loss: 0.5210 - val_accuracy: 0.7190 - lr: 1.0000e-03\n",
      "Epoch 42/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5212 - accuracy: 0.7189 - val_loss: 0.5208 - val_accuracy: 0.7200 - lr: 1.0000e-03\n",
      "Epoch 43/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5207 - accuracy: 0.7196 - val_loss: 0.5206 - val_accuracy: 0.7184 - lr: 1.0000e-03\n",
      "Epoch 44/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5208 - accuracy: 0.7204 - val_loss: 0.5204 - val_accuracy: 0.7202 - lr: 1.0000e-03\n",
      "Epoch 45/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5208 - accuracy: 0.7208 - val_loss: 0.5204 - val_accuracy: 0.7189 - lr: 1.0000e-03\n",
      "Epoch 46/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5201 - accuracy: 0.7213 - val_loss: 0.5204 - val_accuracy: 0.7203 - lr: 1.0000e-03\n",
      "Epoch 47/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5203 - accuracy: 0.7200 - val_loss: 0.5205 - val_accuracy: 0.7187 - lr: 1.0000e-03\n",
      "Epoch 48/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5198 - accuracy: 0.7205 - val_loss: 0.5203 - val_accuracy: 0.7179 - lr: 1.0000e-03\n",
      "Epoch 49/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5201 - accuracy: 0.7204 - val_loss: 0.5203 - val_accuracy: 0.7183 - lr: 1.0000e-03\n",
      "Epoch 50/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5200 - accuracy: 0.7197 - val_loss: 0.5202 - val_accuracy: 0.7193 - lr: 1.0000e-03\n",
      "Epoch 51/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5197 - accuracy: 0.7206 - val_loss: 0.5206 - val_accuracy: 0.7189 - lr: 1.0000e-03\n",
      "Epoch 52/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5198 - accuracy: 0.7207 - val_loss: 0.5206 - val_accuracy: 0.7194 - lr: 1.0000e-03\n",
      "Epoch 53/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5200 - accuracy: 0.7201 - val_loss: 0.5203 - val_accuracy: 0.7180 - lr: 1.0000e-03\n",
      "Epoch 54/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5196 - accuracy: 0.7201 - val_loss: 0.5203 - val_accuracy: 0.7181 - lr: 1.0000e-03\n",
      "Epoch 55/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5197 - accuracy: 0.7205 - val_loss: 0.5203 - val_accuracy: 0.7189 - lr: 1.0000e-03\n",
      "Epoch 56/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5192 - accuracy: 0.7211 - val_loss: 0.5200 - val_accuracy: 0.7202 - lr: 1.0000e-03\n",
      "Epoch 57/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5203 - accuracy: 0.7213 - val_loss: 0.5199 - val_accuracy: 0.7200 - lr: 1.0000e-03\n",
      "Epoch 58/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5193 - accuracy: 0.7205 - val_loss: 0.5196 - val_accuracy: 0.7206 - lr: 1.0000e-03\n",
      "Epoch 59/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5190 - accuracy: 0.7214 - val_loss: 0.5195 - val_accuracy: 0.7205 - lr: 1.0000e-03\n",
      "Epoch 60/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5189 - accuracy: 0.7209 - val_loss: 0.5201 - val_accuracy: 0.7202 - lr: 1.0000e-03\n",
      "Epoch 61/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5194 - accuracy: 0.7218 - val_loss: 0.5199 - val_accuracy: 0.7200 - lr: 1.0000e-03\n",
      "Epoch 62/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5190 - accuracy: 0.7209 - val_loss: 0.5200 - val_accuracy: 0.7197 - lr: 1.0000e-03\n",
      "Epoch 63/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5185 - accuracy: 0.7219 - val_loss: 0.5201 - val_accuracy: 0.7199 - lr: 1.0000e-03\n",
      "Epoch 64/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5191 - accuracy: 0.7213 - val_loss: 0.5198 - val_accuracy: 0.7186 - lr: 1.0000e-03\n",
      "Epoch 65/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5185 - accuracy: 0.7218 - val_loss: 0.5199 - val_accuracy: 0.7206 - lr: 1.0000e-03\n",
      "Epoch 66/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5178 - accuracy: 0.7225 - val_loss: 0.5196 - val_accuracy: 0.7203 - lr: 2.0000e-04\n",
      "Epoch 67/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5178 - accuracy: 0.7227 - val_loss: 0.5195 - val_accuracy: 0.7200 - lr: 2.0000e-04\n",
      "Epoch 68/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5174 - accuracy: 0.7227 - val_loss: 0.5196 - val_accuracy: 0.7198 - lr: 2.0000e-04\n",
      "Epoch 69/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5176 - accuracy: 0.7215 - val_loss: 0.5195 - val_accuracy: 0.7201 - lr: 2.0000e-04\n",
      "Epoch 70/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5179 - accuracy: 0.7221 - val_loss: 0.5194 - val_accuracy: 0.7194 - lr: 2.0000e-04\n",
      "Epoch 71/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5173 - accuracy: 0.7225 - val_loss: 0.5193 - val_accuracy: 0.7201 - lr: 2.0000e-04\n",
      "Epoch 72/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5177 - accuracy: 0.7220 - val_loss: 0.5195 - val_accuracy: 0.7202 - lr: 2.0000e-04\n",
      "Epoch 73/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5178 - accuracy: 0.7214 - val_loss: 0.5195 - val_accuracy: 0.7204 - lr: 2.0000e-04\n",
      "Epoch 74/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5169 - accuracy: 0.7231 - val_loss: 0.5194 - val_accuracy: 0.7198 - lr: 2.0000e-04\n",
      "Epoch 75/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5176 - accuracy: 0.7226 - val_loss: 0.5194 - val_accuracy: 0.7206 - lr: 2.0000e-04\n",
      "Epoch 76/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5179 - accuracy: 0.7228 - val_loss: 0.5194 - val_accuracy: 0.7199 - lr: 2.0000e-04\n",
      "Epoch 77/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5172 - accuracy: 0.7223 - val_loss: 0.5192 - val_accuracy: 0.7200 - lr: 2.0000e-04\n",
      "Epoch 78/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5175 - accuracy: 0.7232 - val_loss: 0.5193 - val_accuracy: 0.7209 - lr: 2.0000e-04\n",
      "Epoch 79/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5168 - accuracy: 0.7234 - val_loss: 0.5193 - val_accuracy: 0.7203 - lr: 4.0000e-05\n",
      "Epoch 80/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5170 - accuracy: 0.7229 - val_loss: 0.5193 - val_accuracy: 0.7200 - lr: 4.0000e-05\n",
      "Epoch 81/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5174 - accuracy: 0.7226 - val_loss: 0.5193 - val_accuracy: 0.7200 - lr: 4.0000e-05\n",
      "Epoch 82/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5175 - accuracy: 0.7243 - val_loss: 0.5193 - val_accuracy: 0.7201 - lr: 4.0000e-05\n",
      "Epoch 83/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5177 - accuracy: 0.7221 - val_loss: 0.5193 - val_accuracy: 0.7197 - lr: 4.0000e-05\n",
      "Epoch 84/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5172 - accuracy: 0.7230 - val_loss: 0.5193 - val_accuracy: 0.7204 - lr: 4.0000e-05\n",
      "Epoch 85/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5168 - accuracy: 0.7234 - val_loss: 0.5193 - val_accuracy: 0.7202 - lr: 4.0000e-05\n",
      "Epoch 86/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5163 - accuracy: 0.7235 - val_loss: 0.5193 - val_accuracy: 0.7201 - lr: 8.0000e-06\n",
      "Epoch 87/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5169 - accuracy: 0.7214 - val_loss: 0.5193 - val_accuracy: 0.7200 - lr: 8.0000e-06\n",
      "Epoch 88/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5168 - accuracy: 0.7215 - val_loss: 0.5193 - val_accuracy: 0.7203 - lr: 8.0000e-06\n",
      "Epoch 89/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5170 - accuracy: 0.7232 - val_loss: 0.5194 - val_accuracy: 0.7199 - lr: 8.0000e-06\n",
      "Epoch 90/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5165 - accuracy: 0.7227 - val_loss: 0.5193 - val_accuracy: 0.7202 - lr: 8.0000e-06\n",
      "Epoch 91/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5169 - accuracy: 0.7235 - val_loss: 0.5193 - val_accuracy: 0.7201 - lr: 8.0000e-06\n",
      "Epoch 92/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5172 - accuracy: 0.7228 - val_loss: 0.5193 - val_accuracy: 0.7204 - lr: 8.0000e-06\n",
      "Epoch 93/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5172 - accuracy: 0.7224 - val_loss: 0.5193 - val_accuracy: 0.7199 - lr: 1.6000e-06\n",
      "Epoch 94/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5172 - accuracy: 0.7225 - val_loss: 0.5193 - val_accuracy: 0.7202 - lr: 1.6000e-06\n",
      "Epoch 95/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5166 - accuracy: 0.7227 - val_loss: 0.5193 - val_accuracy: 0.7200 - lr: 1.6000e-06\n",
      "Epoch 96/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5180 - accuracy: 0.7227 - val_loss: 0.5193 - val_accuracy: 0.7204 - lr: 1.6000e-06\n",
      "Epoch 97/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5176 - accuracy: 0.7221 - val_loss: 0.5193 - val_accuracy: 0.7199 - lr: 1.6000e-06\n",
      "1250/1250 [==============================] - 0s 309us/step\n",
      "Fold 5: Accuracy = 0.72004\n",
      "625/625 [==============================] - 0s 311us/step\n",
      "CV folds scores: [0.7242500000000001, 0.72565, 0.723475, 0.7223999999999999, 0.720043001075027]\n"
     ]
    }
   ],
   "source": [
    "train_nn = train_nn.fillna(0)\n",
    "test_nn = test_nn.fillna(0)\n",
    "\n",
    "X = train_nn[feature_cols]\n",
    "y = train_nn['target']\n",
    "\n",
    "test_predictions_nn = np.zeros(test_nn.shape[0])\n",
    "scores_folds = {'NN_model': []}\n",
    "\n",
    "# KFold \n",
    "kfolds = 5\n",
    "kf = KFold(n_splits=kfolds, shuffle=True, random_state=42)\n",
    "\n",
    "counter = 1\n",
    "\n",
    "for train_index, val_index in kf.split(X):\n",
    "\n",
    "    X_train = X.iloc[train_index]\n",
    "    y_train = y.iloc[train_index]\n",
    "    X_val = X.iloc[val_index]\n",
    "    y_val = y.iloc[val_index]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "    model = base_model()\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.005),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # ---- callbacks ----\n",
    "    # Early stopping callback\n",
    "    es = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=20, verbose=0,\n",
    "        mode='min', restore_best_weights=True)\n",
    "\n",
    "    # Learning rate reduction callback \n",
    "    plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.2, patience=7, verbose=0,\n",
    "        mode='min')\n",
    "    model.fit(\n",
    "        [X_train_scaled],\n",
    "        y_train,\n",
    "        batch_size=512,\n",
    "        epochs=200,\n",
    "        validation_data=([X_val_scaled], y_val),\n",
    "        callbacks=[es, plateau],\n",
    "        shuffle=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    preds = model.predict([X_val_scaled]).reshape(-1)\n",
    "    score = np.mean((preds.round() != y_val.values).astype(float)) \n",
    "    print(f'Fold {counter}: Accuracy = {1 - score:.5f}')\n",
    "    scores_folds['NN_model'].append(1 - score)\n",
    "    \n",
    "    X_test = test_nn[feature_cols]\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "    test_predictions_nn += model.predict([X_test_scaled]).reshape(-1) / kfolds\n",
    "    \n",
    "    counter += 1\n",
    "\n",
    "\n",
    "print(\"CV folds scores:\", scores_folds['NN_model'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_evn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
