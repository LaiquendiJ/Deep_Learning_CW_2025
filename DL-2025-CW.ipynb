{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATH7016 Deep Learning\n",
    "### Coursework 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.layers import Activation\n",
    "from keras.utils import get_custom_objects\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ask_price1</th>\n",
       "      <th>ask_size1</th>\n",
       "      <th>bid_price1</th>\n",
       "      <th>bid_size1</th>\n",
       "      <th>ask_price2</th>\n",
       "      <th>ask_size2</th>\n",
       "      <th>bid_price2</th>\n",
       "      <th>bid_size2</th>\n",
       "      <th>ask_price3</th>\n",
       "      <th>ask_size3</th>\n",
       "      <th>...</th>\n",
       "      <th>bid_size3</th>\n",
       "      <th>ask_price4</th>\n",
       "      <th>ask_size4</th>\n",
       "      <th>bid_price4</th>\n",
       "      <th>bid_size4</th>\n",
       "      <th>midprice_change1</th>\n",
       "      <th>midprice_change2</th>\n",
       "      <th>midprice_change3</th>\n",
       "      <th>midprice_change4</th>\n",
       "      <th>midprice_change5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>696400.0</td>\n",
       "      <td>16</td>\n",
       "      <td>696000.0</td>\n",
       "      <td>12</td>\n",
       "      <td>696500.0</td>\n",
       "      <td>57</td>\n",
       "      <td>695900.0</td>\n",
       "      <td>118</td>\n",
       "      <td>696600.0</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>262</td>\n",
       "      <td>696700.0</td>\n",
       "      <td>150</td>\n",
       "      <td>695700.0</td>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>740800.0</td>\n",
       "      <td>2</td>\n",
       "      <td>740400.0</td>\n",
       "      <td>20</td>\n",
       "      <td>741000.0</td>\n",
       "      <td>60</td>\n",
       "      <td>740200.0</td>\n",
       "      <td>27</td>\n",
       "      <td>741200.0</td>\n",
       "      <td>156</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>741300.0</td>\n",
       "      <td>200</td>\n",
       "      <td>740000.0</td>\n",
       "      <td>170</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>730900.0</td>\n",
       "      <td>1</td>\n",
       "      <td>730200.0</td>\n",
       "      <td>230</td>\n",
       "      <td>731000.0</td>\n",
       "      <td>111</td>\n",
       "      <td>730100.0</td>\n",
       "      <td>86</td>\n",
       "      <td>731100.0</td>\n",
       "      <td>42</td>\n",
       "      <td>...</td>\n",
       "      <td>136</td>\n",
       "      <td>731200.0</td>\n",
       "      <td>100</td>\n",
       "      <td>729900.0</td>\n",
       "      <td>132</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>630600.0</td>\n",
       "      <td>100</td>\n",
       "      <td>630300.0</td>\n",
       "      <td>69</td>\n",
       "      <td>630700.0</td>\n",
       "      <td>110</td>\n",
       "      <td>630200.0</td>\n",
       "      <td>2</td>\n",
       "      <td>630800.0</td>\n",
       "      <td>219</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>630900.0</td>\n",
       "      <td>101</td>\n",
       "      <td>630000.0</td>\n",
       "      <td>104</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>851100.0</td>\n",
       "      <td>579</td>\n",
       "      <td>850300.0</td>\n",
       "      <td>25</td>\n",
       "      <td>851200.0</td>\n",
       "      <td>17</td>\n",
       "      <td>850100.0</td>\n",
       "      <td>287</td>\n",
       "      <td>851400.0</td>\n",
       "      <td>307</td>\n",
       "      <td>...</td>\n",
       "      <td>270</td>\n",
       "      <td>851500.0</td>\n",
       "      <td>223</td>\n",
       "      <td>849900.0</td>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ask_price1  ask_size1  bid_price1  bid_size1  ask_price2  ask_size2  \\\n",
       "0    696400.0         16    696000.0         12    696500.0         57   \n",
       "1    740800.0          2    740400.0         20    741000.0         60   \n",
       "2    730900.0          1    730200.0        230    731000.0        111   \n",
       "3    630600.0        100    630300.0         69    630700.0        110   \n",
       "4    851100.0        579    850300.0         25    851200.0         17   \n",
       "\n",
       "   bid_price2  bid_size2  ask_price3  ask_size3  ...  bid_size3  ask_price4  \\\n",
       "0    695900.0        118    696600.0        100  ...        262    696700.0   \n",
       "1    740200.0         27    741200.0        156  ...         31    741300.0   \n",
       "2    730100.0         86    731100.0         42  ...        136    731200.0   \n",
       "3    630200.0          2    630800.0        219  ...          1    630900.0   \n",
       "4    850100.0        287    851400.0        307  ...        270    851500.0   \n",
       "\n",
       "   ask_size4  bid_price4  bid_size4  midprice_change1  midprice_change2  \\\n",
       "0        150    695700.0        104                 1                 0   \n",
       "1        200    740000.0        170                 0                 1   \n",
       "2        100    729900.0        132                 1                 1   \n",
       "3        101    630000.0        104                 0                 1   \n",
       "4        223    849900.0         72                 1                 0   \n",
       "\n",
       "   midprice_change3  midprice_change4  midprice_change5  \n",
       "0                 1                 0                 1  \n",
       "1                 0                 0                 1  \n",
       "2                 0                 0                 1  \n",
       "3                 0                 0                 0  \n",
       "4                 1                 0                 1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read training and test datasets\n",
    "train_df = pd.read_csv('DL-2025-CW-data/Data_A.csv')\n",
    "test_df = pd.read_csv('DL-2025-CW-data/Data_B_nolabels.csv')\n",
    "\n",
    "# Define column names\n",
    "columns_names = ['target']\n",
    "level_cols=[]\n",
    "for i in range(4):\n",
    "    level_cols +=[f'ask_price{i+1}',f'ask_size{i+1}',f'bid_price{i+1}',f'bid_size{i+1}']\n",
    "change_cols =  []\n",
    "for i in range(5):\n",
    "    change_cols+=[f'midprice_change{i+1}']\n",
    "columns_names+=level_cols+change_cols\n",
    "train_df.columns = columns_names\n",
    "test_df.columns = columns_names[1:]\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mid_price(df,  bid_p, ask_p):\n",
    "    return (df[bid_p] + df[ask_p])/2\n",
    "\n",
    "def calc_wap(df, bid_p, ask_p, bid_s, ask_s):\n",
    "    return (df[bid_p] * df[ask_s] + df[ask_p] * df[bid_s]) / (df[bid_s] + df[ask_s])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(df):\n",
    "    # Step 1: Calculate price & volume features\n",
    "    for i in range(1,5):\n",
    "        df[f\"wap{i}\"] = calc_wap(df, f\"bid_price{i}\", f\"ask_price{i}\", f\"bid_size{i}\", f\"ask_size{i}\")\n",
    "        #df[f\"wmp{i}\"] = calc_wmp(df, f\"bid_price{i}\", f\"ask_price{i}\", f\"bid_size{i}\", f\"ask_size{i}\")\n",
    "        df[f\"price_spread{i}\"] = (df[f\"ask_price{i}\"] - df[f\"bid_price{i}\"]) / ((df[f\"ask_price{i}\"] + df[f\"bid_price{i}\"]) / 2)\n",
    "        df[f'midprice{i}']=mid_price(df,f'bid_price{i}',f'ask_price{i}')\n",
    " \n",
    "        df[f\"order_imbalance{i}\"] = (df[f\"bid_size{i}\"] - df[f\"ask_size{i}\"]) / (df[f\"bid_size{i}\"] + df[f\"ask_size{i}\"])\n",
    "        df[f'bid_ask_ratio{i}'] = df[f'bid_size{i}'] / (df[f'bid_size{i}'] + df[f'ask_size{i}'])\n",
    "        \n",
    "    # Price features\n",
    "\n",
    "    df['bid_depth_ratio'] = df['bid_size1'] / (df[['bid_size1','bid_size2','bid_size3','bid_size4']].sum(axis=1))\n",
    "    df['ask_depth_ratio'] = df['ask_size1'] / (df[['ask_size1','ask_size2','ask_size3','ask_size4']].sum(axis=1))\n",
    "\n",
    "    # Volume features\n",
    "    df[\"total_volume\"] = df[[\"ask_size1\", \"ask_size2\", \"bid_size1\", \"bid_size2\"]].sum(axis=1)\n",
    "    df[\"volume_imbalance\"] = (\n",
    "        (df[\"ask_size1\"] + df[\"ask_size2\"]) - (df[\"bid_size1\"] + df[\"bid_size2\"])\n",
    "    ).abs()\n",
    "\n",
    "    # Order imbalances \n",
    "    df['bid_vol_ratio'] = df[['bid_size1','bid_size2']].sum(axis=1) / df[['bid_size1','bid_size2','ask_size1','ask_size2']].sum(axis=1)\n",
    "    df['ask_vol_ratio'] = 1 - df['bid_vol_ratio']\n",
    "    \n",
    "    # Step 3: Drop raw order book columns\n",
    "    #df = df.drop(columns=level_cols)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nn = preprocessor(train_df)\n",
    "test_nn = preprocessor(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swish(x, beta = 1):\n",
    "    return (x * K.sigmoid(beta * x))\n",
    "\n",
    "get_custom_objects().update({'swish': Activation(swish)})\n",
    "\n",
    "hidden_units = (128,64,32)\n",
    "feature_cols = [c for c in train_nn.columns if c != 'target']\n",
    "num_features = len(feature_cols)\n",
    "\n",
    "def base_model():\n",
    "  \n",
    "    num_input = keras.Input(shape=(num_features), name='num_data')\n",
    "    x = keras.layers.Concatenate()([num_input])\n",
    "   \n",
    "    for n_hidden in hidden_units:\n",
    "        x = keras.layers.Dense(n_hidden, activation='swish')(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "        x = keras.layers.Dropout(0.3)(x)\n",
    "    out = keras.layers.Dense(1, activation='sigmoid', name='prediction')(x)\n",
    "\n",
    "    model = keras.Model(\n",
    "        inputs=[num_input],\n",
    "        outputs=out\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5850 - accuracy: 0.6820 - val_loss: 0.5525 - val_accuracy: 0.6985 - lr: 0.0050\n",
      "Epoch 2/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5594 - accuracy: 0.6930 - val_loss: 0.5441 - val_accuracy: 0.6995 - lr: 0.0050\n",
      "Epoch 3/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5520 - accuracy: 0.6957 - val_loss: 0.5355 - val_accuracy: 0.7047 - lr: 0.0050\n",
      "Epoch 4/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5476 - accuracy: 0.6993 - val_loss: 0.5365 - val_accuracy: 0.7063 - lr: 0.0050\n",
      "Epoch 5/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5437 - accuracy: 0.7016 - val_loss: 0.5297 - val_accuracy: 0.7085 - lr: 0.0050\n",
      "Epoch 6/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5420 - accuracy: 0.7042 - val_loss: 0.5279 - val_accuracy: 0.7114 - lr: 0.0050\n",
      "Epoch 7/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5407 - accuracy: 0.7041 - val_loss: 0.5297 - val_accuracy: 0.7085 - lr: 0.0050\n",
      "Epoch 8/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5391 - accuracy: 0.7053 - val_loss: 0.5250 - val_accuracy: 0.7145 - lr: 0.0050\n",
      "Epoch 9/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5385 - accuracy: 0.7050 - val_loss: 0.5263 - val_accuracy: 0.7128 - lr: 0.0050\n",
      "Epoch 10/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5373 - accuracy: 0.7068 - val_loss: 0.5245 - val_accuracy: 0.7165 - lr: 0.0050\n",
      "Epoch 11/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5356 - accuracy: 0.7087 - val_loss: 0.5224 - val_accuracy: 0.7143 - lr: 0.0050\n",
      "Epoch 12/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5350 - accuracy: 0.7077 - val_loss: 0.5238 - val_accuracy: 0.7136 - lr: 0.0050\n",
      "Epoch 13/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5342 - accuracy: 0.7100 - val_loss: 0.5225 - val_accuracy: 0.7168 - lr: 0.0050\n",
      "Epoch 14/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5341 - accuracy: 0.7096 - val_loss: 0.5237 - val_accuracy: 0.7159 - lr: 0.0050\n",
      "Epoch 15/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5327 - accuracy: 0.7101 - val_loss: 0.5206 - val_accuracy: 0.7141 - lr: 0.0050\n",
      "Epoch 16/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5333 - accuracy: 0.7096 - val_loss: 0.5223 - val_accuracy: 0.7161 - lr: 0.0050\n",
      "Epoch 17/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5313 - accuracy: 0.7115 - val_loss: 0.5219 - val_accuracy: 0.7175 - lr: 0.0050\n",
      "Epoch 18/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5320 - accuracy: 0.7102 - val_loss: 0.5196 - val_accuracy: 0.7177 - lr: 0.0050\n",
      "Epoch 19/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5306 - accuracy: 0.7118 - val_loss: 0.5207 - val_accuracy: 0.7194 - lr: 0.0050\n",
      "Epoch 20/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5307 - accuracy: 0.7112 - val_loss: 0.5194 - val_accuracy: 0.7207 - lr: 0.0050\n",
      "Epoch 21/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5303 - accuracy: 0.7119 - val_loss: 0.5180 - val_accuracy: 0.7187 - lr: 0.0050\n",
      "Epoch 22/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5298 - accuracy: 0.7120 - val_loss: 0.5202 - val_accuracy: 0.7182 - lr: 0.0050\n",
      "Epoch 23/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5297 - accuracy: 0.7124 - val_loss: 0.5180 - val_accuracy: 0.7183 - lr: 0.0050\n",
      "Epoch 24/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5290 - accuracy: 0.7133 - val_loss: 0.5185 - val_accuracy: 0.7196 - lr: 0.0050\n",
      "Epoch 25/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5289 - accuracy: 0.7137 - val_loss: 0.5179 - val_accuracy: 0.7200 - lr: 0.0050\n",
      "Epoch 26/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5284 - accuracy: 0.7138 - val_loss: 0.5212 - val_accuracy: 0.7176 - lr: 0.0050\n",
      "Epoch 27/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5280 - accuracy: 0.7148 - val_loss: 0.5188 - val_accuracy: 0.7200 - lr: 0.0050\n",
      "Epoch 28/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5280 - accuracy: 0.7141 - val_loss: 0.5175 - val_accuracy: 0.7188 - lr: 0.0050\n",
      "Epoch 29/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5279 - accuracy: 0.7138 - val_loss: 0.5173 - val_accuracy: 0.7215 - lr: 0.0050\n",
      "Epoch 30/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5271 - accuracy: 0.7143 - val_loss: 0.5162 - val_accuracy: 0.7230 - lr: 0.0050\n",
      "Epoch 31/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5269 - accuracy: 0.7141 - val_loss: 0.5179 - val_accuracy: 0.7194 - lr: 0.0050\n",
      "Epoch 32/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5275 - accuracy: 0.7142 - val_loss: 0.5172 - val_accuracy: 0.7208 - lr: 0.0050\n",
      "Epoch 33/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5270 - accuracy: 0.7142 - val_loss: 0.5168 - val_accuracy: 0.7218 - lr: 0.0050\n",
      "Epoch 34/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5268 - accuracy: 0.7146 - val_loss: 0.5171 - val_accuracy: 0.7228 - lr: 0.0050\n",
      "Epoch 35/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5266 - accuracy: 0.7154 - val_loss: 0.5168 - val_accuracy: 0.7211 - lr: 0.0050\n",
      "Epoch 36/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5262 - accuracy: 0.7157 - val_loss: 0.5174 - val_accuracy: 0.7225 - lr: 0.0050\n",
      "Epoch 37/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5260 - accuracy: 0.7169 - val_loss: 0.5162 - val_accuracy: 0.7217 - lr: 0.0050\n",
      "Epoch 38/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5235 - accuracy: 0.7182 - val_loss: 0.5143 - val_accuracy: 0.7244 - lr: 1.0000e-03\n",
      "Epoch 39/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5227 - accuracy: 0.7191 - val_loss: 0.5138 - val_accuracy: 0.7233 - lr: 1.0000e-03\n",
      "Epoch 40/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5216 - accuracy: 0.7188 - val_loss: 0.5137 - val_accuracy: 0.7249 - lr: 1.0000e-03\n",
      "Epoch 41/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5223 - accuracy: 0.7193 - val_loss: 0.5138 - val_accuracy: 0.7247 - lr: 1.0000e-03\n",
      "Epoch 42/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5217 - accuracy: 0.7183 - val_loss: 0.5138 - val_accuracy: 0.7241 - lr: 1.0000e-03\n",
      "Epoch 43/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5212 - accuracy: 0.7191 - val_loss: 0.5143 - val_accuracy: 0.7236 - lr: 1.0000e-03\n",
      "Epoch 44/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5221 - accuracy: 0.7186 - val_loss: 0.5136 - val_accuracy: 0.7251 - lr: 1.0000e-03\n",
      "Epoch 45/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5214 - accuracy: 0.7190 - val_loss: 0.5138 - val_accuracy: 0.7251 - lr: 1.0000e-03\n",
      "Epoch 46/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5210 - accuracy: 0.7196 - val_loss: 0.5144 - val_accuracy: 0.7250 - lr: 1.0000e-03\n",
      "Epoch 47/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5206 - accuracy: 0.7212 - val_loss: 0.5136 - val_accuracy: 0.7248 - lr: 1.0000e-03\n",
      "Epoch 48/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5207 - accuracy: 0.7211 - val_loss: 0.5131 - val_accuracy: 0.7253 - lr: 1.0000e-03\n",
      "Epoch 49/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5207 - accuracy: 0.7189 - val_loss: 0.5138 - val_accuracy: 0.7254 - lr: 1.0000e-03\n",
      "Epoch 50/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5208 - accuracy: 0.7201 - val_loss: 0.5131 - val_accuracy: 0.7260 - lr: 1.0000e-03\n",
      "Epoch 51/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5200 - accuracy: 0.7204 - val_loss: 0.5130 - val_accuracy: 0.7251 - lr: 1.0000e-03\n",
      "Epoch 52/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5202 - accuracy: 0.7202 - val_loss: 0.5135 - val_accuracy: 0.7247 - lr: 1.0000e-03\n",
      "Epoch 53/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5197 - accuracy: 0.7201 - val_loss: 0.5130 - val_accuracy: 0.7250 - lr: 1.0000e-03\n",
      "Epoch 54/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5209 - accuracy: 0.7190 - val_loss: 0.5132 - val_accuracy: 0.7248 - lr: 1.0000e-03\n",
      "Epoch 55/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5206 - accuracy: 0.7198 - val_loss: 0.5143 - val_accuracy: 0.7243 - lr: 1.0000e-03\n",
      "Epoch 56/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5190 - accuracy: 0.7213 - val_loss: 0.5131 - val_accuracy: 0.7250 - lr: 2.0000e-04\n",
      "Epoch 57/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5197 - accuracy: 0.7204 - val_loss: 0.5133 - val_accuracy: 0.7253 - lr: 2.0000e-04\n",
      "Epoch 58/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5197 - accuracy: 0.7212 - val_loss: 0.5131 - val_accuracy: 0.7250 - lr: 2.0000e-04\n",
      "Epoch 59/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5195 - accuracy: 0.7208 - val_loss: 0.5130 - val_accuracy: 0.7258 - lr: 2.0000e-04\n",
      "Epoch 60/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5193 - accuracy: 0.7209 - val_loss: 0.5130 - val_accuracy: 0.7261 - lr: 2.0000e-04\n",
      "Epoch 61/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5193 - accuracy: 0.7205 - val_loss: 0.5130 - val_accuracy: 0.7261 - lr: 2.0000e-04\n",
      "Epoch 62/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5198 - accuracy: 0.7199 - val_loss: 0.5131 - val_accuracy: 0.7261 - lr: 2.0000e-04\n",
      "Epoch 63/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5188 - accuracy: 0.7210 - val_loss: 0.5130 - val_accuracy: 0.7254 - lr: 4.0000e-05\n",
      "Epoch 64/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5195 - accuracy: 0.7213 - val_loss: 0.5130 - val_accuracy: 0.7255 - lr: 4.0000e-05\n",
      "Epoch 65/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5196 - accuracy: 0.7210 - val_loss: 0.5130 - val_accuracy: 0.7259 - lr: 4.0000e-05\n",
      "Epoch 66/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5198 - accuracy: 0.7208 - val_loss: 0.5130 - val_accuracy: 0.7254 - lr: 4.0000e-05\n",
      "Epoch 67/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5186 - accuracy: 0.7214 - val_loss: 0.5130 - val_accuracy: 0.7257 - lr: 4.0000e-05\n",
      "Epoch 68/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5191 - accuracy: 0.7211 - val_loss: 0.5129 - val_accuracy: 0.7259 - lr: 4.0000e-05\n",
      "Epoch 69/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5193 - accuracy: 0.7201 - val_loss: 0.5131 - val_accuracy: 0.7261 - lr: 4.0000e-05\n",
      "Epoch 70/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5189 - accuracy: 0.7217 - val_loss: 0.5129 - val_accuracy: 0.7257 - lr: 4.0000e-05\n",
      "Epoch 71/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5189 - accuracy: 0.7215 - val_loss: 0.5128 - val_accuracy: 0.7256 - lr: 4.0000e-05\n",
      "Epoch 72/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5196 - accuracy: 0.7216 - val_loss: 0.5129 - val_accuracy: 0.7258 - lr: 4.0000e-05\n",
      "Epoch 73/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5194 - accuracy: 0.7211 - val_loss: 0.5129 - val_accuracy: 0.7255 - lr: 4.0000e-05\n",
      "Epoch 74/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5189 - accuracy: 0.7215 - val_loss: 0.5129 - val_accuracy: 0.7260 - lr: 4.0000e-05\n",
      "Epoch 75/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5186 - accuracy: 0.7208 - val_loss: 0.5129 - val_accuracy: 0.7257 - lr: 4.0000e-05\n",
      "Epoch 76/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5196 - accuracy: 0.7209 - val_loss: 0.5129 - val_accuracy: 0.7261 - lr: 4.0000e-05\n",
      "Epoch 77/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5189 - accuracy: 0.7212 - val_loss: 0.5129 - val_accuracy: 0.7258 - lr: 4.0000e-05\n",
      "Epoch 78/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5197 - accuracy: 0.7201 - val_loss: 0.5129 - val_accuracy: 0.7257 - lr: 4.0000e-05\n",
      "Epoch 79/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5189 - accuracy: 0.7220 - val_loss: 0.5129 - val_accuracy: 0.7263 - lr: 8.0000e-06\n",
      "Epoch 80/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5181 - accuracy: 0.7215 - val_loss: 0.5128 - val_accuracy: 0.7258 - lr: 8.0000e-06\n",
      "Epoch 81/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5189 - accuracy: 0.7209 - val_loss: 0.5128 - val_accuracy: 0.7258 - lr: 8.0000e-06\n",
      "Epoch 82/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5189 - accuracy: 0.7208 - val_loss: 0.5130 - val_accuracy: 0.7261 - lr: 8.0000e-06\n",
      "Epoch 83/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5185 - accuracy: 0.7216 - val_loss: 0.5129 - val_accuracy: 0.7261 - lr: 8.0000e-06\n",
      "Epoch 84/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5182 - accuracy: 0.7225 - val_loss: 0.5129 - val_accuracy: 0.7258 - lr: 8.0000e-06\n",
      "Epoch 85/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5191 - accuracy: 0.7209 - val_loss: 0.5130 - val_accuracy: 0.7261 - lr: 8.0000e-06\n",
      "Epoch 86/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5197 - accuracy: 0.7208 - val_loss: 0.5129 - val_accuracy: 0.7261 - lr: 1.6000e-06\n",
      "Epoch 87/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5184 - accuracy: 0.7217 - val_loss: 0.5128 - val_accuracy: 0.7258 - lr: 1.6000e-06\n",
      "Epoch 88/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5191 - accuracy: 0.7211 - val_loss: 0.5129 - val_accuracy: 0.7260 - lr: 1.6000e-06\n",
      "Epoch 89/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5189 - accuracy: 0.7210 - val_loss: 0.5129 - val_accuracy: 0.7261 - lr: 1.6000e-06\n",
      "Epoch 90/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5191 - accuracy: 0.7212 - val_loss: 0.5129 - val_accuracy: 0.7258 - lr: 1.6000e-06\n",
      "Epoch 91/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5188 - accuracy: 0.7214 - val_loss: 0.5128 - val_accuracy: 0.7258 - lr: 1.6000e-06\n",
      "1250/1250 [==============================] - 1s 329us/step\n",
      "Fold 1: Accuracy = 0.72560\n",
      "625/625 [==============================] - 0s 322us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5834 - accuracy: 0.6834 - val_loss: 0.5574 - val_accuracy: 0.6981 - lr: 0.0050\n",
      "Epoch 2/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5606 - accuracy: 0.6930 - val_loss: 0.5445 - val_accuracy: 0.6998 - lr: 0.0050\n",
      "Epoch 3/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5536 - accuracy: 0.6949 - val_loss: 0.5366 - val_accuracy: 0.7053 - lr: 0.0050\n",
      "Epoch 4/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5485 - accuracy: 0.6994 - val_loss: 0.5318 - val_accuracy: 0.7084 - lr: 0.0050\n",
      "Epoch 5/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5449 - accuracy: 0.7022 - val_loss: 0.5289 - val_accuracy: 0.7097 - lr: 0.0050\n",
      "Epoch 6/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5424 - accuracy: 0.7028 - val_loss: 0.5292 - val_accuracy: 0.7102 - lr: 0.0050\n",
      "Epoch 7/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5407 - accuracy: 0.7053 - val_loss: 0.5266 - val_accuracy: 0.7118 - lr: 0.0050\n",
      "Epoch 8/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5397 - accuracy: 0.7053 - val_loss: 0.5253 - val_accuracy: 0.7133 - lr: 0.0050\n",
      "Epoch 9/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5379 - accuracy: 0.7065 - val_loss: 0.5249 - val_accuracy: 0.7124 - lr: 0.0050\n",
      "Epoch 10/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5366 - accuracy: 0.7064 - val_loss: 0.5227 - val_accuracy: 0.7154 - lr: 0.0050\n",
      "Epoch 11/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5362 - accuracy: 0.7078 - val_loss: 0.5220 - val_accuracy: 0.7151 - lr: 0.0050\n",
      "Epoch 12/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5356 - accuracy: 0.7077 - val_loss: 0.5218 - val_accuracy: 0.7158 - lr: 0.0050\n",
      "Epoch 13/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5346 - accuracy: 0.7103 - val_loss: 0.5215 - val_accuracy: 0.7151 - lr: 0.0050\n",
      "Epoch 14/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5337 - accuracy: 0.7092 - val_loss: 0.5214 - val_accuracy: 0.7176 - lr: 0.0050\n",
      "Epoch 15/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5335 - accuracy: 0.7109 - val_loss: 0.5201 - val_accuracy: 0.7180 - lr: 0.0050\n",
      "Epoch 16/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5333 - accuracy: 0.7107 - val_loss: 0.5205 - val_accuracy: 0.7168 - lr: 0.0050\n",
      "Epoch 17/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5322 - accuracy: 0.7101 - val_loss: 0.5191 - val_accuracy: 0.7160 - lr: 0.0050\n",
      "Epoch 18/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5311 - accuracy: 0.7112 - val_loss: 0.5193 - val_accuracy: 0.7186 - lr: 0.0050\n",
      "Epoch 19/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5304 - accuracy: 0.7127 - val_loss: 0.5183 - val_accuracy: 0.7164 - lr: 0.0050\n",
      "Epoch 20/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5313 - accuracy: 0.7120 - val_loss: 0.5182 - val_accuracy: 0.7197 - lr: 0.0050\n",
      "Epoch 21/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5305 - accuracy: 0.7126 - val_loss: 0.5190 - val_accuracy: 0.7206 - lr: 0.0050\n",
      "Epoch 22/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5299 - accuracy: 0.7137 - val_loss: 0.5180 - val_accuracy: 0.7198 - lr: 0.0050\n",
      "Epoch 23/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5301 - accuracy: 0.7128 - val_loss: 0.5176 - val_accuracy: 0.7181 - lr: 0.0050\n",
      "Epoch 24/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5292 - accuracy: 0.7130 - val_loss: 0.5178 - val_accuracy: 0.7203 - lr: 0.0050\n",
      "Epoch 25/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5296 - accuracy: 0.7138 - val_loss: 0.5172 - val_accuracy: 0.7208 - lr: 0.0050\n",
      "Epoch 26/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5291 - accuracy: 0.7124 - val_loss: 0.5170 - val_accuracy: 0.7212 - lr: 0.0050\n",
      "Epoch 27/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5283 - accuracy: 0.7140 - val_loss: 0.5179 - val_accuracy: 0.7206 - lr: 0.0050\n",
      "Epoch 28/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5281 - accuracy: 0.7140 - val_loss: 0.5169 - val_accuracy: 0.7200 - lr: 0.0050\n",
      "Epoch 29/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5280 - accuracy: 0.7137 - val_loss: 0.5182 - val_accuracy: 0.7211 - lr: 0.0050\n",
      "Epoch 30/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5280 - accuracy: 0.7154 - val_loss: 0.5160 - val_accuracy: 0.7217 - lr: 0.0050\n",
      "Epoch 31/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5279 - accuracy: 0.7140 - val_loss: 0.5191 - val_accuracy: 0.7186 - lr: 0.0050\n",
      "Epoch 32/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5275 - accuracy: 0.7146 - val_loss: 0.5173 - val_accuracy: 0.7198 - lr: 0.0050\n",
      "Epoch 33/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5263 - accuracy: 0.7153 - val_loss: 0.5148 - val_accuracy: 0.7219 - lr: 0.0050\n",
      "Epoch 34/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5260 - accuracy: 0.7149 - val_loss: 0.5160 - val_accuracy: 0.7189 - lr: 0.0050\n",
      "Epoch 35/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5263 - accuracy: 0.7154 - val_loss: 0.5155 - val_accuracy: 0.7225 - lr: 0.0050\n",
      "Epoch 36/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5261 - accuracy: 0.7162 - val_loss: 0.5159 - val_accuracy: 0.7222 - lr: 0.0050\n",
      "Epoch 37/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5256 - accuracy: 0.7163 - val_loss: 0.5146 - val_accuracy: 0.7221 - lr: 0.0050\n",
      "Epoch 38/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5259 - accuracy: 0.7154 - val_loss: 0.5146 - val_accuracy: 0.7224 - lr: 0.0050\n",
      "Epoch 39/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5256 - accuracy: 0.7160 - val_loss: 0.5164 - val_accuracy: 0.7211 - lr: 0.0050\n",
      "Epoch 40/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5257 - accuracy: 0.7175 - val_loss: 0.5164 - val_accuracy: 0.7221 - lr: 0.0050\n",
      "Epoch 41/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5257 - accuracy: 0.7177 - val_loss: 0.5166 - val_accuracy: 0.7198 - lr: 0.0050\n",
      "Epoch 42/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5249 - accuracy: 0.7162 - val_loss: 0.5152 - val_accuracy: 0.7218 - lr: 0.0050\n",
      "Epoch 43/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5248 - accuracy: 0.7177 - val_loss: 0.5148 - val_accuracy: 0.7209 - lr: 0.0050\n",
      "Epoch 44/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5254 - accuracy: 0.7170 - val_loss: 0.5142 - val_accuracy: 0.7232 - lr: 0.0050\n",
      "Epoch 45/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5253 - accuracy: 0.7160 - val_loss: 0.5136 - val_accuracy: 0.7218 - lr: 0.0050\n",
      "Epoch 46/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5250 - accuracy: 0.7168 - val_loss: 0.5170 - val_accuracy: 0.7231 - lr: 0.0050\n",
      "Epoch 47/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5245 - accuracy: 0.7164 - val_loss: 0.5150 - val_accuracy: 0.7209 - lr: 0.0050\n",
      "Epoch 48/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5250 - accuracy: 0.7170 - val_loss: 0.5145 - val_accuracy: 0.7223 - lr: 0.0050\n",
      "Epoch 49/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5246 - accuracy: 0.7178 - val_loss: 0.5150 - val_accuracy: 0.7234 - lr: 0.0050\n",
      "Epoch 50/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5249 - accuracy: 0.7182 - val_loss: 0.5145 - val_accuracy: 0.7225 - lr: 0.0050\n",
      "Epoch 51/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5242 - accuracy: 0.7175 - val_loss: 0.5148 - val_accuracy: 0.7219 - lr: 0.0050\n",
      "Epoch 52/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5242 - accuracy: 0.7188 - val_loss: 0.5152 - val_accuracy: 0.7240 - lr: 0.0050\n",
      "Epoch 53/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5216 - accuracy: 0.7200 - val_loss: 0.5121 - val_accuracy: 0.7253 - lr: 1.0000e-03\n",
      "Epoch 54/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5209 - accuracy: 0.7206 - val_loss: 0.5120 - val_accuracy: 0.7243 - lr: 1.0000e-03\n",
      "Epoch 55/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5206 - accuracy: 0.7208 - val_loss: 0.5120 - val_accuracy: 0.7249 - lr: 1.0000e-03\n",
      "Epoch 56/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5200 - accuracy: 0.7217 - val_loss: 0.5120 - val_accuracy: 0.7249 - lr: 1.0000e-03\n",
      "Epoch 57/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5199 - accuracy: 0.7209 - val_loss: 0.5119 - val_accuracy: 0.7247 - lr: 1.0000e-03\n",
      "Epoch 58/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5194 - accuracy: 0.7216 - val_loss: 0.5116 - val_accuracy: 0.7257 - lr: 1.0000e-03\n",
      "Epoch 59/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5202 - accuracy: 0.7217 - val_loss: 0.5113 - val_accuracy: 0.7260 - lr: 1.0000e-03\n",
      "Epoch 60/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5192 - accuracy: 0.7213 - val_loss: 0.5119 - val_accuracy: 0.7232 - lr: 1.0000e-03\n",
      "Epoch 61/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5191 - accuracy: 0.7226 - val_loss: 0.5118 - val_accuracy: 0.7253 - lr: 1.0000e-03\n",
      "Epoch 62/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5195 - accuracy: 0.7206 - val_loss: 0.5121 - val_accuracy: 0.7267 - lr: 1.0000e-03\n",
      "Epoch 63/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5202 - accuracy: 0.7198 - val_loss: 0.5120 - val_accuracy: 0.7240 - lr: 1.0000e-03\n",
      "Epoch 64/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5192 - accuracy: 0.7211 - val_loss: 0.5116 - val_accuracy: 0.7256 - lr: 1.0000e-03\n",
      "Epoch 65/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5192 - accuracy: 0.7220 - val_loss: 0.5114 - val_accuracy: 0.7253 - lr: 1.0000e-03\n",
      "Epoch 66/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5194 - accuracy: 0.7215 - val_loss: 0.5118 - val_accuracy: 0.7256 - lr: 1.0000e-03\n",
      "Epoch 67/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5188 - accuracy: 0.7222 - val_loss: 0.5113 - val_accuracy: 0.7259 - lr: 2.0000e-04\n",
      "Epoch 68/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5178 - accuracy: 0.7230 - val_loss: 0.5112 - val_accuracy: 0.7254 - lr: 2.0000e-04\n",
      "Epoch 69/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5179 - accuracy: 0.7233 - val_loss: 0.5111 - val_accuracy: 0.7254 - lr: 2.0000e-04\n",
      "Epoch 70/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5182 - accuracy: 0.7232 - val_loss: 0.5110 - val_accuracy: 0.7253 - lr: 2.0000e-04\n",
      "Epoch 71/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5182 - accuracy: 0.7231 - val_loss: 0.5111 - val_accuracy: 0.7250 - lr: 2.0000e-04\n",
      "Epoch 72/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5181 - accuracy: 0.7227 - val_loss: 0.5110 - val_accuracy: 0.7254 - lr: 2.0000e-04\n",
      "Epoch 73/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5180 - accuracy: 0.7224 - val_loss: 0.5109 - val_accuracy: 0.7255 - lr: 2.0000e-04\n",
      "Epoch 74/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5174 - accuracy: 0.7241 - val_loss: 0.5109 - val_accuracy: 0.7253 - lr: 2.0000e-04\n",
      "Epoch 75/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5181 - accuracy: 0.7230 - val_loss: 0.5110 - val_accuracy: 0.7261 - lr: 2.0000e-04\n",
      "Epoch 76/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5181 - accuracy: 0.7230 - val_loss: 0.5109 - val_accuracy: 0.7254 - lr: 2.0000e-04\n",
      "Epoch 77/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5181 - accuracy: 0.7224 - val_loss: 0.5109 - val_accuracy: 0.7262 - lr: 2.0000e-04\n",
      "Epoch 78/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5179 - accuracy: 0.7233 - val_loss: 0.5110 - val_accuracy: 0.7256 - lr: 2.0000e-04\n",
      "Epoch 79/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5177 - accuracy: 0.7224 - val_loss: 0.5109 - val_accuracy: 0.7259 - lr: 2.0000e-04\n",
      "Epoch 80/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5184 - accuracy: 0.7228 - val_loss: 0.5109 - val_accuracy: 0.7257 - lr: 2.0000e-04\n",
      "Epoch 81/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5174 - accuracy: 0.7219 - val_loss: 0.5109 - val_accuracy: 0.7257 - lr: 4.0000e-05\n",
      "Epoch 82/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5179 - accuracy: 0.7227 - val_loss: 0.5109 - val_accuracy: 0.7254 - lr: 4.0000e-05\n",
      "Epoch 83/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5178 - accuracy: 0.7229 - val_loss: 0.5109 - val_accuracy: 0.7255 - lr: 4.0000e-05\n",
      "Epoch 84/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5182 - accuracy: 0.7235 - val_loss: 0.5109 - val_accuracy: 0.7257 - lr: 4.0000e-05\n",
      "Epoch 85/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5183 - accuracy: 0.7231 - val_loss: 0.5109 - val_accuracy: 0.7257 - lr: 4.0000e-05\n",
      "Epoch 86/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5179 - accuracy: 0.7228 - val_loss: 0.5109 - val_accuracy: 0.7259 - lr: 4.0000e-05\n",
      "Epoch 87/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5176 - accuracy: 0.7230 - val_loss: 0.5109 - val_accuracy: 0.7260 - lr: 4.0000e-05\n",
      "Epoch 88/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5183 - accuracy: 0.7225 - val_loss: 0.5109 - val_accuracy: 0.7258 - lr: 8.0000e-06\n",
      "Epoch 89/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5176 - accuracy: 0.7235 - val_loss: 0.5109 - val_accuracy: 0.7257 - lr: 8.0000e-06\n",
      "Epoch 90/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5175 - accuracy: 0.7231 - val_loss: 0.5109 - val_accuracy: 0.7258 - lr: 8.0000e-06\n",
      "Epoch 91/200\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.5174 - accuracy: 0.7225 - val_loss: 0.5109 - val_accuracy: 0.7258 - lr: 8.0000e-06\n",
      "Epoch 92/200\n",
      "313/313 [==============================] - 1s 5ms/step - loss: 0.5176 - accuracy: 0.7223 - val_loss: 0.5108 - val_accuracy: 0.7258 - lr: 8.0000e-06\n",
      "Epoch 93/200\n",
      "313/313 [==============================] - 1s 5ms/step - loss: 0.5176 - accuracy: 0.7228 - val_loss: 0.5108 - val_accuracy: 0.7257 - lr: 8.0000e-06\n",
      "Epoch 94/200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.5181 - accuracy: 0.7223 - val_loss: 0.5109 - val_accuracy: 0.7258 - lr: 8.0000e-06\n",
      "Epoch 95/200\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.5181 - accuracy: 0.7221 - val_loss: 0.5109 - val_accuracy: 0.7258 - lr: 1.6000e-06\n",
      "Epoch 96/200\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.5182 - accuracy: 0.7226 - val_loss: 0.5109 - val_accuracy: 0.7258 - lr: 1.6000e-06\n",
      "Epoch 97/200\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.5181 - accuracy: 0.7210 - val_loss: 0.5109 - val_accuracy: 0.7257 - lr: 1.6000e-06\n",
      "Epoch 98/200\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.5175 - accuracy: 0.7235 - val_loss: 0.5109 - val_accuracy: 0.7261 - lr: 1.6000e-06\n",
      "Epoch 99/200\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.5175 - accuracy: 0.7235 - val_loss: 0.5109 - val_accuracy: 0.7255 - lr: 1.6000e-06\n",
      "Epoch 100/200\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.5179 - accuracy: 0.7223 - val_loss: 0.5109 - val_accuracy: 0.7255 - lr: 1.6000e-06\n",
      "Epoch 101/200\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.5174 - accuracy: 0.7229 - val_loss: 0.5109 - val_accuracy: 0.7258 - lr: 1.6000e-06\n",
      "Epoch 102/200\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.5178 - accuracy: 0.7237 - val_loss: 0.5109 - val_accuracy: 0.7257 - lr: 3.2000e-07\n",
      "Epoch 103/200\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.5170 - accuracy: 0.7227 - val_loss: 0.5109 - val_accuracy: 0.7255 - lr: 3.2000e-07\n",
      "Epoch 104/200\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.5177 - accuracy: 0.7235 - val_loss: 0.5109 - val_accuracy: 0.7257 - lr: 3.2000e-07\n",
      "Epoch 105/200\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.5180 - accuracy: 0.7231 - val_loss: 0.5109 - val_accuracy: 0.7258 - lr: 3.2000e-07\n",
      "Epoch 106/200\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.5175 - accuracy: 0.7233 - val_loss: 0.5109 - val_accuracy: 0.7257 - lr: 3.2000e-07\n",
      "Epoch 107/200\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.5171 - accuracy: 0.7237 - val_loss: 0.5109 - val_accuracy: 0.7255 - lr: 3.2000e-07\n",
      "Epoch 108/200\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.5167 - accuracy: 0.7235 - val_loss: 0.5109 - val_accuracy: 0.7257 - lr: 3.2000e-07\n",
      "Epoch 109/200\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.5184 - accuracy: 0.7227 - val_loss: 0.5108 - val_accuracy: 0.7257 - lr: 6.4000e-08\n",
      "Epoch 110/200\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.5172 - accuracy: 0.7228 - val_loss: 0.5108 - val_accuracy: 0.7257 - lr: 6.4000e-08\n",
      "Epoch 111/200\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.5176 - accuracy: 0.7227 - val_loss: 0.5109 - val_accuracy: 0.7256 - lr: 6.4000e-08\n",
      "Epoch 112/200\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.5177 - accuracy: 0.7230 - val_loss: 0.5109 - val_accuracy: 0.7260 - lr: 6.4000e-08\n",
      "Epoch 113/200\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.5178 - accuracy: 0.7228 - val_loss: 0.5109 - val_accuracy: 0.7256 - lr: 6.4000e-08\n",
      "Epoch 114/200\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.5188 - accuracy: 0.7227 - val_loss: 0.5108 - val_accuracy: 0.7257 - lr: 6.4000e-08\n",
      "Epoch 115/200\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.5179 - accuracy: 0.7224 - val_loss: 0.5108 - val_accuracy: 0.7259 - lr: 6.4000e-08\n",
      "Epoch 116/200\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.5181 - accuracy: 0.7229 - val_loss: 0.5108 - val_accuracy: 0.7259 - lr: 1.2800e-08\n",
      "Epoch 117/200\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.5176 - accuracy: 0.7220 - val_loss: 0.5109 - val_accuracy: 0.7258 - lr: 1.2800e-08\n",
      "Epoch 118/200\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.5180 - accuracy: 0.7225 - val_loss: 0.5109 - val_accuracy: 0.7257 - lr: 1.2800e-08\n",
      "Epoch 119/200\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.5183 - accuracy: 0.7219 - val_loss: 0.5109 - val_accuracy: 0.7260 - lr: 1.2800e-08\n",
      "Epoch 120/200\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.5185 - accuracy: 0.7218 - val_loss: 0.5109 - val_accuracy: 0.7257 - lr: 1.2800e-08\n",
      "Epoch 121/200\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.5179 - accuracy: 0.7222 - val_loss: 0.5108 - val_accuracy: 0.7258 - lr: 1.2800e-08\n",
      "Epoch 122/200\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.5176 - accuracy: 0.7226 - val_loss: 0.5109 - val_accuracy: 0.7257 - lr: 1.2800e-08\n",
      "Epoch 123/200\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.5177 - accuracy: 0.7226 - val_loss: 0.5109 - val_accuracy: 0.7255 - lr: 2.5600e-09\n",
      "Epoch 124/200\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.5174 - accuracy: 0.7237 - val_loss: 0.5109 - val_accuracy: 0.7258 - lr: 2.5600e-09\n",
      "Epoch 125/200\n",
      "313/313 [==============================] - 3s 9ms/step - loss: 0.5176 - accuracy: 0.7227 - val_loss: 0.5109 - val_accuracy: 0.7261 - lr: 2.5600e-09\n",
      "Epoch 126/200\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.5171 - accuracy: 0.7236 - val_loss: 0.5109 - val_accuracy: 0.7258 - lr: 2.5600e-09\n",
      "Epoch 127/200\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.5179 - accuracy: 0.7217 - val_loss: 0.5109 - val_accuracy: 0.7259 - lr: 2.5600e-09\n",
      "Epoch 128/200\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.5174 - accuracy: 0.7220 - val_loss: 0.5108 - val_accuracy: 0.7258 - lr: 2.5600e-09\n",
      "Epoch 129/200\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.5168 - accuracy: 0.7237 - val_loss: 0.5109 - val_accuracy: 0.7259 - lr: 2.5600e-09\n",
      "Epoch 130/200\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.5178 - accuracy: 0.7230 - val_loss: 0.5109 - val_accuracy: 0.7255 - lr: 5.1200e-10\n",
      "1250/1250 [==============================] - 1s 625us/step\n",
      "Fold 2: Accuracy = 0.72565\n",
      "625/625 [==============================] - 0s 608us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "313/313 [==============================] - 3s 7ms/step - loss: 0.5836 - accuracy: 0.6828 - val_loss: 0.5566 - val_accuracy: 0.6938 - lr: 0.0050\n",
      "Epoch 2/200\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.5581 - accuracy: 0.6931 - val_loss: 0.5450 - val_accuracy: 0.7000 - lr: 0.0050\n",
      "Epoch 3/200\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.5503 - accuracy: 0.6963 - val_loss: 0.5385 - val_accuracy: 0.7028 - lr: 0.0050\n",
      "Epoch 4/200\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.5456 - accuracy: 0.7012 - val_loss: 0.5352 - val_accuracy: 0.7063 - lr: 0.0050\n",
      "Epoch 5/200\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.5423 - accuracy: 0.7036 - val_loss: 0.5349 - val_accuracy: 0.7053 - lr: 0.0050\n",
      "Epoch 6/200\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.5403 - accuracy: 0.7048 - val_loss: 0.5304 - val_accuracy: 0.7099 - lr: 0.0050\n",
      "Epoch 7/200\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.5384 - accuracy: 0.7060 - val_loss: 0.5310 - val_accuracy: 0.7083 - lr: 0.0050\n",
      "Epoch 8/200\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.5372 - accuracy: 0.7070 - val_loss: 0.5274 - val_accuracy: 0.7138 - lr: 0.0050\n",
      "Epoch 9/200\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.5359 - accuracy: 0.7081 - val_loss: 0.5262 - val_accuracy: 0.7119 - lr: 0.0050\n",
      "Epoch 10/200\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.5348 - accuracy: 0.7093 - val_loss: 0.5250 - val_accuracy: 0.7134 - lr: 0.0050\n",
      "Epoch 11/200\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.5340 - accuracy: 0.7097 - val_loss: 0.5245 - val_accuracy: 0.7151 - lr: 0.0050\n",
      "Epoch 12/200\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.5334 - accuracy: 0.7098 - val_loss: 0.5265 - val_accuracy: 0.7156 - lr: 0.0050\n",
      "Epoch 13/200\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.5327 - accuracy: 0.7105 - val_loss: 0.5235 - val_accuracy: 0.7150 - lr: 0.0050\n",
      "Epoch 14/200\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.5319 - accuracy: 0.7118 - val_loss: 0.5247 - val_accuracy: 0.7143 - lr: 0.0050\n",
      "Epoch 15/200\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.5315 - accuracy: 0.7114 - val_loss: 0.5244 - val_accuracy: 0.7155 - lr: 0.0050\n",
      "Epoch 16/200\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.5312 - accuracy: 0.7119 - val_loss: 0.5255 - val_accuracy: 0.7153 - lr: 0.0050\n",
      "Epoch 17/200\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.5311 - accuracy: 0.7112 - val_loss: 0.5231 - val_accuracy: 0.7168 - lr: 0.0050\n",
      "Epoch 18/200\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.5298 - accuracy: 0.7136 - val_loss: 0.5225 - val_accuracy: 0.7161 - lr: 0.0050\n",
      "Epoch 19/200\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.5305 - accuracy: 0.7133 - val_loss: 0.5221 - val_accuracy: 0.7185 - lr: 0.0050\n",
      "Epoch 20/200\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.5293 - accuracy: 0.7130 - val_loss: 0.5230 - val_accuracy: 0.7179 - lr: 0.0050\n",
      "Epoch 21/200\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.5287 - accuracy: 0.7141 - val_loss: 0.5242 - val_accuracy: 0.7176 - lr: 0.0050\n",
      "Epoch 22/200\n",
      "313/313 [==============================] - 3s 10ms/step - loss: 0.5289 - accuracy: 0.7132 - val_loss: 0.5221 - val_accuracy: 0.7176 - lr: 0.0050\n",
      "Epoch 23/200\n",
      "295/313 [===========================>..] - ETA: 0s - loss: 0.5281 - accuracy: 0.7138"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/laiquendi/Documents/GitHub/Deep_Learning_CW_2025/DL-2025-CW.ipynb Cell 8\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laiquendi/Documents/GitHub/Deep_Learning_CW_2025/DL-2025-CW.ipynb#X10sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m# Learning rate reduction callback \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laiquendi/Documents/GitHub/Deep_Learning_CW_2025/DL-2025-CW.ipynb#X10sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m plateau \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mReduceLROnPlateau(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laiquendi/Documents/GitHub/Deep_Learning_CW_2025/DL-2025-CW.ipynb#X10sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, factor\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m7\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laiquendi/Documents/GitHub/Deep_Learning_CW_2025/DL-2025-CW.ipynb#X10sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/laiquendi/Documents/GitHub/Deep_Learning_CW_2025/DL-2025-CW.ipynb#X10sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laiquendi/Documents/GitHub/Deep_Learning_CW_2025/DL-2025-CW.ipynb#X10sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     [X_train_scaled],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laiquendi/Documents/GitHub/Deep_Learning_CW_2025/DL-2025-CW.ipynb#X10sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     y_train,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laiquendi/Documents/GitHub/Deep_Learning_CW_2025/DL-2025-CW.ipynb#X10sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laiquendi/Documents/GitHub/Deep_Learning_CW_2025/DL-2025-CW.ipynb#X10sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laiquendi/Documents/GitHub/Deep_Learning_CW_2025/DL-2025-CW.ipynb#X10sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49m([X_val_scaled], y_val),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laiquendi/Documents/GitHub/Deep_Learning_CW_2025/DL-2025-CW.ipynb#X10sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[es, plateau],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laiquendi/Documents/GitHub/Deep_Learning_CW_2025/DL-2025-CW.ipynb#X10sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laiquendi/Documents/GitHub/Deep_Learning_CW_2025/DL-2025-CW.ipynb#X10sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laiquendi/Documents/GitHub/Deep_Learning_CW_2025/DL-2025-CW.ipynb#X10sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laiquendi/Documents/GitHub/Deep_Learning_CW_2025/DL-2025-CW.ipynb#X10sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m preds \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict([X_val_scaled])\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/laiquendi/Documents/GitHub/Deep_Learning_CW_2025/DL-2025-CW.ipynb#X10sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m score \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean((preds\u001b[39m.\u001b[39mround() \u001b[39m!=\u001b[39m y_val\u001b[39m.\u001b[39mvalues)\u001b[39m.\u001b[39mastype(\u001b[39mfloat\u001b[39m)) \n",
      "File \u001b[0;32m~/miniconda3/envs/tf_evn/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_evn/lib/python3.10/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_evn/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_evn/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_evn/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_evn/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_evn/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_evn/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[1;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_evn/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[1;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[1;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1463\u001b[0m   )\n\u001b[1;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_evn/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_nn = train_nn.fillna(0)\n",
    "test_nn = test_nn.fillna(0)\n",
    "\n",
    "X = train_nn[feature_cols]\n",
    "y = train_nn['target']\n",
    "\n",
    "test_predictions_nn = np.zeros(test_nn.shape[0])\n",
    "scores_folds = {'NN_model': []}\n",
    "\n",
    "# KFold \n",
    "kfolds = 5\n",
    "kf = KFold(n_splits=kfolds, shuffle=True, random_state=42)\n",
    "\n",
    "counter = 1\n",
    "\n",
    "for train_index, val_index in kf.split(X):\n",
    "\n",
    "    X_train = X.iloc[train_index]\n",
    "    y_train = y.iloc[train_index]\n",
    "    X_val = X.iloc[val_index]\n",
    "    y_val = y.iloc[val_index]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "    model = base_model()\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.005),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # ---- callbacks ----\n",
    "    # Early stopping callback\n",
    "    es = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=20, verbose=0,\n",
    "        mode='min', restore_best_weights=True)\n",
    "\n",
    "    # Learning rate reduction callback \n",
    "    plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.2, patience=7, verbose=0,\n",
    "        mode='min')\n",
    "    model.fit(\n",
    "        [X_train_scaled],\n",
    "        y_train,\n",
    "        batch_size=512,\n",
    "        epochs=200,\n",
    "        validation_data=([X_val_scaled], y_val),\n",
    "        callbacks=[es, plateau],\n",
    "        shuffle=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    preds = model.predict([X_val_scaled]).reshape(-1)\n",
    "    score = np.mean((preds.round() != y_val.values).astype(float)) \n",
    "    print(f'Fold {counter}: Accuracy = {1 - score:.5f}')\n",
    "    scores_folds['NN_model'].append(1 - score)\n",
    "    \n",
    "    X_test = test_nn[feature_cols]\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    test_predictions_nn += model.predict([X_test_scaled]).reshape(-1) / kfolds\n",
    "   \n",
    "    counter += 1\n",
    "\n",
    "\n",
    "print(\"CV folds scores:\", scores_folds['NN_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = (test_predictions_nn > 0.5).astype(int)\n",
    "#np.savetxt(\"02205857_wang.txt\", final_pred, fmt=\"%d\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_evn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
